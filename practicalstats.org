#+TITLE: Practicalstats
#+PROPERTY: header-args:jupyter-python :session py
#+STARTUP: inlineimages
#+ATTR_HTML: :width 300px #org-image-actual-width

#+BEGIN_SRC jupyter-python
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-white')
import pandas as pd
import numpy as np
import datetime
sns.set()
import scipy.stats as stats
import statsmodels.api as sm
from numpy.random import RandomState
from permute.core import two_sample
from permute.core import one_sample
from scipy.stats import theilslopes
from scipy.stats import kendalltau
from scipy.stats import linregress
from scipy.stats import ttest_ind, shapiro
from scipy.stats import mannwhitneyu
from statsmodels.graphics.gofplots import ProbPlot
import statsmodels.stats
import statsmodels.formula.api as smf
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
matplotlib.rcParams['figure.dpi'] = 200

#+END_SRC

#+RESULTS:


https://pubs.er.usgs.gov/publication/tm4A3
* Section 1: Descriptive Statistics and Graphs
** Mean or Median?

** interested in totals, mass, volume?
Then use the mean, std
(e.g. long term chronic effects; total P into estuary over 1 year)

Mean is not resistant to outliers. It is the center of mass.
Parametric tests are tests on means. Affected by outliers.

** Interested in typical values, how often something occurs?
Then use the median, other percentiles
(e.g. short term acute exposure, P concentrations in 10 different streams)

Median is resistant to outliers. It is the center of frequency.
Nonparametric tests are tests on percentiles. Resistant to outliers.

** Transformations and Dealing with Outliers
Transformations used for three purposes
1) To make data more like a normal distribution
2) To make data more linear, and
3) To make data more constant in variance

*BUT there are consequences.*
Resampling methods have as one goal to avoid transformations and its
consequences.

** Geometric Means
mean of logs transformed back to original units
Geometric mean = exp(mean(logs)
\( = e^{\left[ \frac{1}{N}\sum_{i=1}^{N} \log(x_i)} \right] \)
\( \left( x_i * x_2 * ..* x_N\right)^{1/N} \)

if logs of data are normally distributed, the geometric mean is an estimate of
the MEDIAN of the data.
See the transformation bias below.

|          x |                log x |
|------------+----------------------|
|          1 |                    0 |
|         10 |                    1 |
|        100 |                    2 |
|       1000 |                    3 |
|      10000 |                    4 |
|------------+----------------------|
|  mean=2222 |      mean(log)=2=100 |
| median=100 | geometric mean = 100 |


Choose transformations using the Ladder of Powers (Box-Cox Transforms)
In increasing power of transformation, and from negative to positive skew:

(cubic, square, original units, square root, cube root, logarithm, reciprocal
root, reciprocal)

i.e. power (3, 2, 1, 1/2, 1/3, 0 -1/2 -1)

| power | equation       |
|-------+----------------|
|     3 | cube           |
|     2 | square         |
|     1 | original units |
|   1/2 | sqrt           |
|   1/3 | cube rt        |
|     0 | logarithm      |
|  -1/2 | reciprocal rt  |
|    -1 | reciprocal     |

transformations of interest with PARAMETRIC METHODS
trasnform data to more normal distribution

plot of x transforms to y, where \(y= x^{\theta}\)


Flint Michigan Lead data.
https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2017.01016.x
Lead levels in water samples collected by Flint city officials, covering the
period January to June 2015. As per the Lead and Copper Rule, if more than 10%
of samples, or the 90th percentile value, are above 15 ppb, officials are
required to take action. In total, 71 samples were collected, with a 90th
percentile value of almost 19 ppb. However, two samples (shown in red) were
excluded – one with a lead concentration of 20 ppb, the other 104 ppb. The
remaining 69 samples had a 90th percentile value of 12 ppb.

*** Outlier tests no not tell you which data are wrong
They critique the assumption that the data come from a normal distribution

*** Causes of outliers
1) error in measurement
2) "contamination" from another population (i.e. data from clean site and data
from contaminated site. It represents a condition you do not wish to describe)
3) skewed distributions-- most data in the natural world are skewed and not
normal. Instead of tests requiring data from a normal distribution, keep the
outlier and use nonparametric or permutation tests.

*** Outliers are often the most valuable observation
** Python
# https://orgmode.org/manual/Using-Header-Arguments.html#Using-Header-Arguments
:PROPERTIES:
:header-args:jupyter-python: :session py :async yes
:END:
#+BEGIN_SRC jupyter-python :results silent :exports none
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-white') #fivethirtyeight, ggplot
import pandas as pd
import numpy as np
import datetime
sns.set()
import scipy.stats as stats
matplotlib.rcParams['figure.dpi'] = 200
#+END_SRC

#+BEGIN_SRC jupyter-python :exports none
df = pd.read_excel(r'/home/jj/PracticalStats/AES1 Online 2_7/AES1 Online 2_7/AES1 Data/TP1.xlsx')
print(df.sample(5))
df.describe()
#+END_SRC

#+RESULTS:
:RESULTS:
:            Sample.Date Site.Name  TotalP
: 17 2012-07-04 06:16:00      WBU1     6.0
: 24 2012-09-12 11:33:00      WBU1     7.3
: 8  2011-07-28 05:31:00      WBU1     8.5
: 3  2010-09-16 10:57:00      WBU1     6.6
: 25 2012-09-20 08:13:00      WBU1     5.6
|       |   TotalP |
|-------+----------|
| count | 29       |
| mean  |  6.38966 |
| std   |  1.17545 |
| min   |  4.4     |
| 25%   |  5.6     |
| 50%   |  6       |
| 75%   |  7       |
| max   |  9       |
:END:
#+BEGIN_SRC jupyter-python :results image/png
#df.boxplot(column=['TotalP'])
data = df['TotalP']
fig, ax = plt.subplots()
sns.boxplot(data=data, color='aquamarine', width=0.5)
sns.stripplot(data=data, jitter=True, color='red', marker='o', alpha=0.5)
plt.title('Total P')
ax.set_xlabel('WBU1')

plt.style.use('seaborn-white')
matplotlib.rcParams['figure.dpi'] = 100
plt.draw()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/90f63e0c42d51bece6cc6b702d296ece708141a9.png]]


https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/
Emulating R plots in Python

#+BEGIN_SRC jupyter-python
import statsmodels.formula.api as smf
from statsmodels.graphics.gofplots import ProbPlot

plot_lm_2 = ProbPlot.qqplot(df['TotalP'].to_numpy(), line='45', alpha=0.5, color='#4C72B0', lw=1)

plot_lm_2.set_figheight(8)
plot_lm_2.set_figwidth(12)

plot_lm_2.axes[0].set_title('Normal Q-Q')
plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')
plot_lm_2.axes[0].set_ylabel('Standardized Residuals');

# annotations
abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)
abs_norm_resid_top_3 = abs_norm_resid[:3]

for r, i in enumerate(abs_norm_resid_top_3):
    plot_lm_2.axes[0].annotate(i,
                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],
                                   model_norm_residuals[i]));

#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example

AttributeErrorTraceback (most recent call last)
<ipython-input-10-cde6c8d52fdb> in <module>
    2 from statsmodels.graphics.gofplots import ProbPlot
    3
----> 4 plot_lm_2 = ProbPlot.qqplot(df['TotalP'].to_numpy(), line='45', alpha=0.5, color='#4C72B0', lw=1)
    5
    6 plot_lm_2.set_figheight(8)

~/anaconda3/lib/python3.7/site-packages/statsmodels/graphics/gofplots.py in qqplot(self, xlabel, ylabel, line, other, ax, **plotkwargs)
400
401         else:
--> 402             fig, ax = _do_plot(self.theoretical_quantiles,
403                                self.sample_quantiles,
404                                self.dist, ax=ax, line=line,

AttributeError: 'numpy.ndarray' object has no attribute 'theoretical_quantiles'
#+end_example
:END:

#+BEGIN_SRC jupyter-python
import numpy as np
import statsmodels.api as sm

data = df.TotalP.to_numpy()
sm.qqplot(data, line='q')
#+END_SRC

#+RESULTS:
:RESULTS:
: /home/jj/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
:   return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval
[[file:./.ob-jupyter/921972158960197a5cdf92e01c1e7765378664ad.png]]
:END:

#+BEGIN_SRC jupyter-python
x = df.TotalP.to_numpy()
f = lambda x: np.log(x)
logdata = f(x)
fig, ax = plt.subplots()
sns.boxplot(data=logdata, color='aquamarine', width=0.5)
sns.stripplot(data=logdata, jitter=True, color='red', marker='o', alpha=0.5)
plt.title('Log Total P')
ax.set_xlabel('WBU1')
ax.set_ylabel('log concentration')

plt.style.use('seaborn-white')
matplotlib.rcParams['figure.dpi'] = 100
plt.draw()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/93627601f8bb1f96f608056be33f85408cc5eac6.png]]


#+BEGIN_SRC jupyter-python :results image/png
from scipy.stats import t
fig, ax = plt.subplots()
sm.qqplot(logdata, line='q')
plt.title('QQ Plot, logtransformed Total P')
#+END_SRC

#+RESULTS:
:RESULTS:
: /home/jj/anaconda3/lib/python3.7/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.
:   return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval
: Text(0.5, 1.0, 'QQ Plot, logtransformed Total P')
[[file:./.ob-jupyter/1d88d45fdfdc7afa40964542e104e5da408c5040.png]]
:END:

#+BEGIN_SRC jupyter-python :results image/png
import seaborn as sns
from seaborn_qqplot import qqplot
from scipy.stats import norm
fig, ax = plt.subplots()
qqplot(df, y = norm, x = 'TotalP', aspect=1.5, display_kws={"identity":False, "fit":True, "reg":True, "ci":0.05})
plt.title('QQ Plot, Total P')
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1, 'QQ Plot, Total P')
[[file:./.ob-jupyter/248b8a7ee63e0ee1707c73eecc995fc72089b127.png]]
:END:
#+BEGIN_SRC jupyter-python
df['logP'] = np.log(df.TotalP)
df.sample(5)
fig, ax = plt.subplots()
qqplot(df, x = 'logP', y = norm, aspect=1.5, display_kws={"identity":False, "fit":True, "reg":True, "ci":0.05})
plt.title('QQ Plot, Log Total P')
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1, 'QQ Plot, Log Total P')
[[file:./.ob-jupyter/40c7be91a2c1919a1a7647ee9dea73426469b124.png]]
:END:

*** TODO learn how to add 95% CI on both theoretical distribution for this QQ plot, and other
https://stackoverflow.com/questions/28242593/correct-way-to-obtain-confidence-interval-with-scipy
https://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data
https://stackoverflow.com/questions/47414842/confidence-interval-of-probability-prediction-from-logistic-regression-statsmode
https://emredjan.github.io/blog/2017/07/11/emulating-r-plots-in-python/
It looks like digging into the code for seaborn-qqplot will make this clearer:
https://seaborn-qqplot.readthedocs.io/en/latest/
The two methods have different axes though. quantiles vs theoretical normal.
I'm not sure which is canonical.
*
* Section 2: Hypothesis Tests
** How Hypothesis tests work
- decide what parameter/test is of interest
- establish null and alternate hypothesis
- decide on alpha
- compute test statistic and p-value
- compare p-value to alpha
- reject H_0 if p-value < alpha
** What parameter and test is of interest?
what is the objective?

mass, totals, cumulative exposure -> mean; parametric or permutation tests

typical patterns -> percentiles (frequency); nonparametric tests

** Null and Alternate Hypotheses
Null Hypothesis: no signal. No difference between groups, no trend.

Alternate Hypothesis: a signal is present.
- two-sided
  *General. Change can be high or low* (Does arsenic differ between location? Is
  there a trend?)

- One-sided
  *Change is directional* (Has arsenic /increased/ on site? Is there a /decrease/ over
  time?)
** Decide on alpha level
Alpha is a management tool. A measure of risk. What percent error can I live
with?
Alpha is the probability of false positive, of rejecting the null hypothesis
when it is actually true. Tradition is $\alpha$ = 0.05.

** Compute test statistic
Ex. wilcoxon rank-sum test (nonparametric)
test statistic W = sum of the ranks for the group with the samller sample size
(n)
** compute the p-value (probability of getting a certain test statistic)
The three classes of hypothesis tests have three different methods for obtaining
a p-value from the test statistic:
1) assume the data follow a distribution (e.g. normal). The distribution test
   statistic is then used to compute the p-value, but is only valid if the data
   actually have the assumed distributional shape (parametric tests)
2) Compute all possible test statistics (nonparametric tests)
3) Compute a large number of (or all) possible test statistics (permutation
   tests)
** PYTHON Examples
#+BEGIN_SRC jupyter-python :results silent :exports none
%matplotlib inline
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
plt.style.use('seaborn-white') #fivethirtyeight, ggplot
import pandas as pd
import numpy as np
import datetime
sns.set()
import scipy.stats as stats
matplotlib.rcParams['figure.dpi'] = 200
#+END_SRC

#+TBLNAME: abv_blw
| Conc | ABV_BLW |
|------+---------|
|    6 | ABOVE   |
|    5 | ABOVE   |
|   10 | ABOVE   |
|   16 | BELOW   |
|    8 | BELOW   |
|   22 | BELOW   |
|   18 | BELOW   |
#+BEGIN_SRC jupyter-python :var data=abv_blw
from scipy.stats import wilcoxon
x = [x[0] for x in data[0:3]]
y = [y[0] for y in data[3:]]
wilcoxon(x,y, alternative='less')
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example

ValueErrorTraceback (most recent call last)
<ipython-input-3-3a1e02731e4e> in <module>
      3 x = [x[0] for x in data[0:3]]
      4 y = [y[0] for y in data[3:]]
----> 5 wilcoxon(x,y, alternative='less')

~/.local/lib/python3.7/site-packages/scipy/stats/morestats.py in wilcoxon(x, y, zero_method, correction, alternative)
   2862             raise ValueError('Samples x and y must be one-dimensional.')
   2863         if len(x) != len(y):
-> 2864             raise ValueError('The samples x and y must have the same length.')
   2865         d = x - y
   2866

ValueError: The samples x and y must have the same length.
#+end_example
:END:

When applied to two vectors, may be the same as Mann-Whitney
https://stackoverflow.com/questions/33890367/python-wilcoxon-unequal-n
https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test
#+BEGIN_SRC jupyter-python :var data=abv_blw
from scipy.stats import mannwhitneyu
x = [x[0] for x in data[0:3]]
y = [y[0] for y in data[3:]]
mannwhitneyu(x,y, alternative='less')
#+END_SRC

#+RESULTS:
: MannwhitneyuResult(statistic=1.0, pvalue=0.05580588414914612)

#+BEGIN_SRC jupyter-python
import scipy
scipy.__version__

#+END_SRC

#+RESULTS:
: 1.4.1

#+TBLNAME: moly
|    mo | locat    |
|-------+----------|
| 0.850 | DOWNGRAD |
| 0.390 | DOWNGRAD |
| 0.320 | DOWNGRAD |
| 0.300 | DOWNGRAD |
| 0.300 | DOWNGRAD |
| 0.205 | DOWNGRAD |
| 0.200 | DOWNGRAD |
| 0.200 | DOWNGRAD |
| 0.140 | DOWNGRAD |
| 0.140 | DOWNGRAD |
| 0.090 | DOWNGRAD |
| 0.046 | DOWNGRAD |
| 0.035 | DOWNGRAD |
| 6.900 | upgrad   |
| 3.200 | upgrad   |
| 1.700 | upgrad   |

#+BEGIN_SRC jupyter-python :var data = moly
x = [x[0] for x in data if "DOWNGRAD" in x[1]]
y = [y[0] for y in data if "upgrad" in y[1]]
mannwhitneyu(x,y,alternative='less')
#+END_SRC

#+RESULTS:
: MannwhitneyuResult(statistic=0.0, pvalue=0.00520635450787597)
#+BEGIN_SRC jupyter-python :var data = moly
from scipy.stats import shapiro
x = [x[0] for x in data if "DOWNGRAD" in x[1]]
y = [y[0] for y in data if "upgrad" in y[1]]
W, pvalue = shapiro(x)
print(f"W: {W}, p-value: {pvalue}")
#+END_SRC

#+RESULTS:
: W: 0.7953206896781921, p-value: 0.00600065290927887

First results is W test statistic, second is p-value. This tells us only 6 in
1000 times would we get a result this extreme if this came from a normal
distribution. This is not very likely. Our null hypothesis is that this was a
normal distribution. We do not accept the null hypothesis. This also means we
probably shouldn't used parametric tests going forward.

#+BEGIN_SRC jupyter-python
import statsmodels.api as sm
import numpy as np
plt.style.use('seaborn-white')
matplotlib.rcParams['figure.dpi'] = 100
sm.qqplot(np.array(x), line='q')
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/79a4ebf15c2dd7bf01e9c6685003fc59f57ae68a.png]]
:END:

** Three Classes of Hypothesis Tests
1) Parametric Tests
   tests on means or other distributional parameter. They assume the data follow some specific distribution, often normal, to get accurate p-value. Strong affect by outliers. Skewness will often push p-values higher and we won't see differences we should. P-values that are too high are an indication of low power, and inability to distinguish groups.
2) Nonparametric tests.
   Tests on ranks (percentiles), like wilcoxon rank sum. They compute all possible outcomes to get a p-value. No distribution assumed. Resistant to outliers.
3) Permutation Tests
   tests on any statistic. You can test means here without assuming a distribution. They determine the likelihood of getting the observed test statistic out of thousands of possible rearrangements of the data. Used as an alternative to parametric test, to test means while limiting effect of outliers.
** Power: ability of tests to find a signal when present
Parametric tests have low power whenever data have outliers, or are skewed, or groups have different variability. Environmental data usually have all three characteristics. So ANOVA, t-tests, and t confidence intervals don't work well for the type of data we usually encounter.

Alternatives:
Permutation tests can still test for differences in meands.

Nonparametric tests: if we don't need to know means, and just which group is higher or lower, we can test for differences in percentiles (e.g. test for typical patterns)

** Don't start from an assumption of Normal
It's just a bad start. Nonparametric tests have roughly equivalent or greater power than parametric tests whether they are normal or not. And field data is almost never normal.

Old-fashioned guidance start by assuming data follow a normal distribution or that it doesn't matter. This is bad guidance.
If you use a log transform, parametric tests will then test for geometric means and not mean of data. This is a measure of the median. You have to keep track of what you are actually testing. But you can just do this directly with nonparametric tests.

** STATISTICAL FLOWCHART

What is your objective?

1) Mass/Totals (chronic effects) --> Test means using permutation tests
2) Frequency/Typical patterns (acute effects) --> Test percentiles using nonparametric (rank-based) tests
** Permutation Tests
Make no distributional assumptions about sample populations

Does not rely on the Central Limit Theorem

Use only observed data and all possible rearrangements or permutations of the data

Is still affected by unequal variance, but in the same way that the null hypothesis is itself affected by this.

A plot of all permutations is a picture of the null hypothesis. If groups are the same, randomly permuting them gives all possible outcomes as a distribution. If so, how extreme is the observed difference in means between groups compared to all possible values? What is the probability of this occuring?

with two groups, x, y, there are $\frac{(x+y)!}{x! \, y!}$ combinations possible.
** PYTHON
1) H_0 is that there is no difference between nitrogen in precipitation over
   industrial and residential land.
   H_a is that there is a difference (2-sided), or that industrial has higher
   concentrations (1-sided)
#+BEGIN_SRC jupyter-python :results silent
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
plt.style.use('seaborn-white')
#+END_SRC


#+TBLNAME: nh4
|  NH4 | landuse |
|------+---------|
| 0.59 | indust  |
| 0.87 | indust  |
| 1.10 | indust  |
| 1.10 | indust  |
| 1.20 | indust  |
| 1.30 | indust  |
| 1.60 | indust  |
| 1.70 | indust  |
| 3.20 | indust  |
| 4.00 | indust  |
| 0.30 | residen |
| 0.36 | residen |
| 0.50 | residen |
| 0.70 | residen |
| 0.70 | residen |
| 0.90 | residen |
| 0.92 | residen |
| 1.00 | residen |
| 1.30 | residen |
| 9.70 | residen |
#+BEGIN_SRC jupyter-python :var data = nh4
from scipy.stats import probplot
ind = [x for x,_ in data if _=='indust']
res = [x for x,_ in data if _=='residen']

fig, (ax1,ax2) = plt.subplots(1,2, sharey=True)
probplot(ind, dist='norm', plot=ax1)
probplot(res, dist='norm', plot=ax2)
ax1.set_aspect(0.2)
ax2.set_aspect(0.2)
ax1.set_title(r'Industrial Land Use $NH_4$')
ax2.set_title(r'Residential Land Use $NH_4$')
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1.0, 'Residential Land Use $NH_4$')
[[file:./.ob-jupyter/8698f796c27960b14e8355b97b0a85ea632ab2cc.png]]
:END:
#+BEGIN_SRC jupyter-python
from scipy.stats import shapiro
data = {'Industrial':ind, 'Residential': res}
for key,value in data.items():
    W, pvalue = shapiro(value)
    print(f"{key} W: {W}, p-value: {pvalue}")

#+END_SRC

#+RESULTS:
: Industrial W: 0.8034603595733643, p-value: 0.015972202643752098
: Residential W: 0.46754151582717896, p-value: 1.5174567806752748e-06
3) These are very unlikely to be normally distributed based on these p-values.
* Section 3: Confidence, Prediction, and Tolerance Intervals
** Intervals: UCL, UPL, UTL
Based on n observations, the interval strates where the population statistic is
probably located.

1) confidence interval
   contains an unknown parameter (mean, median) of the population with a
   specific probability.
2) prediction interval
   contains one or more future observations with a specified probability.
3) tolerance interval
   contains a proportion(percentile) of future observations with a specified
   probability.

** UCL95
The 95% one-sided upper confidence bound (limit) on the mean.
The true population mean can be expected to lie below this bound with 95%
confidence, if computer correctly (e.g. if the data follow the normal
distribution if using the t-interval). It is used as a protective estimate of long
term or accumulative exposure.

This assumes the distribution is parametric, however.

** Confidence intervals on skewed data.
Parametric t-intervals assume either data follow a normal distribution, or there
is sufficient data that the sample mean does (central limit theorem)--USEPA did
a study in 2000 that suggests in may require up to 100 samples before CLT is valid.

T-intervals will not include the true value as often as the confidence level
states if these assumptions are not true. The interval bounds will not be wide
(high) enough.


** Methods for computing intervals on the mean
1) assume normality and compute t-intervals
2) transform the data and computer t-intervals (this will produce confidence
   intervals on the median, NOT the population mean)
3) Assume a distribution that fits data better than a normal distribution (PPCC,
   Shapiro-Wilk Test)
4) Bootstrapping (a resampling method. Distribution-free)

** Bootstrapping
The best way we know to compute a UCL95 from skewed data for n>=20

-Percentile Bootstrap
-BCA bootstrap (corrects for bias/skewness but needs more data)

With small (<20) data, may not capture the breadth of the population shape (may
be best to find a better fitting distribution)

** PYTHON


#+BEGIN_SRC jupyter-python :results silent
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
plt.style.use('seaborn-white')
#+END_SRC

#+TBLNAME: arsenic
| Arsenic |   ln conc |
|---------+-----------|
|     1.3 | 0.2623643 |
|     1.5 | 0.4054651 |
|     1.8 | 0.5877867 |
|     2.6 | 0.9555114 |
|     2.8 | 1.0296194 |
|     3.5 | 1.2527630 |
|     4.0 | 1.3862944 |
|     4.8 | 1.5686159 |
|     8.0 | 2.0794415 |
|     9.5 | 2.2512918 |
|    12.0 | 2.4849066 |
|    14.0 | 2.6390573 |
|    19.0 | 2.9444390 |
|    23.0 | 3.1354942 |
|    41.0 | 3.7135721 |
|    80.0 | 4.3820266 |
|   100.0 | 4.6051702 |
|   110.0 | 4.7004804 |
|   120.0 | 4.7874917 |
|   190.0 | 5.2470241 |
|   240.0 | 5.4806389 |
|   250.0 | 5.5214609 |
|   300.0 | 5.7037825 |
|   340.0 | 5.8289456 |
|   580.0 | 6.3630281 |

#+BEGIN_SRC jupyter-python :var data=arsenic
x = [x for x,_ in data]
y = [y for _,y in data]

fig, (ax1, ax2) = plt.subplots(1,2)
sns.boxplot(data=x, color='aquamarine', width=0.5, ax=ax1)
sns.stripplot(data=x, jitter=True, color='red', marker='o', alpha=0.5, ax=ax1)
ax1.set_title('Arsenic')

sns.boxplot(data=y, color='plum', width=0.5, ax=ax2)
sns.stripplot(data=y, jitter=True, color='black', marker='o', alpha=0.5, ax=ax2)
ax2.set_title('Log Arsenic')


plt.style.use('seaborn-white')
matplotlib.rcParams['figure.dpi'] = 100
plt.draw()
#+END_SRC
#+BEGIN_SRC jupyter-python
data = {'Arsenic': x, 'logAs':y}

for key, value in data.items():
    n= len(value)
    mean, sigma = np.mean(value), np.std(value, ddof=1)
    yerr = sigma/ np.sqrt(n)*stats.t.ppf(1-0.05/2, n-1) # 2-sided
    t, p = stats.ttest_1samp(value, 0)
    #ci = stats.norm.interval(0.95, loc=mean, scale=sigma) # this is wrong, not sure what it does
    print(f"{key} t-statistic= {t:.2e}, p-value= {p:.2e}, yes={yerr}, CI = {yerr+mean, mean-yerr}")

#+END_SRC

#+RESULTS:
: Arsenic t-statistic= 3.40e+00, p-value= 2.36e-03, yes=59.72305888934315, CI = (158.07505888934315, 38.628941110656854), (-185.22551356089733, 381.92951356089736)
: logAs t-statistic= 8.10e+00, p-value= 2.56e-08, yes=0.8088757852637691, CI = (3.9815426532637685, 2.3637910827362303), (-0.6680437089602149, 7.013377444960214)

BOOTSTRAP Confidence Intervals

statistics = []
for i in bootstraps:
	sample = select_sample_with_replacement(data)
	stat = calculate_statistic(sample)
	statistics.append(stat)

ordered = sort(statistics)
lower = percentile(ordered, (1-alpha)/2)
upper = percentile(ordered, alpha+((1-alpha)/2))

https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/
https://people.duke.edu/~ccc14/sta-663/ResamplingAndMonteCarloSimulations.html

cumsum below shows convergence and progression towards final values as more means are added to the bootstrap sample.
#+BEGIN_SRC jupyter-python
reps = 5000
n_size = 20
for key, value in data.items():
    n_size = len(value)
    xb = np.random.choice(value, (n_size, reps), replace=True)
    yb = 1/np.arange(1,n_size+1)[:, None] * np.cumsum(xb, axis=0)
    upper, lower = np.percentile(yb, [2.5, 97.5], axis=1)
    # plt.plot(np.arange(1, n+1)[:, None], yb, c='grey', alpha=0.02)
    # plt.plot(np.arange(1, n+1), yb[:, 0], c='red', linewidth=1)
    # plt.plot(np.arange(1, n+1), upper, 'b', np.arange(1, n+1), lower, 'b');
    print(f"{key}: upper= {upper[-1]}, lower={lower[-1]}")

#+END_SRC

#+RESULTS:
: Arsenic: upper= 49.3518, lower=159.53769999999992
: Log Arsenic: upper= 2.4480177363000006, lower=3.9192540403

#+BEGIN_SRC jupyter-python
data = {'Arsenic': x, 'logAs':y}
means = {x : [] for x in data.keys()}
reps=10000
n_size=20
alpha = 0.05
for key, value in data.items():
    mean, sigma = np.mean(value), np.std(value, ddof=1)
    for i in np.arange(1,reps+1):
        sample = np.random.choice(value, n_size, replace=True)
        stat = np.mean(sample)
        means[key].append(stat)
    ordered = np.sort(means[key])
    lower = np.percentile(ordered, 2.5, interpolation='midpoint')
    upper = np.percentile(ordered, 97.5, interpolation='midpoint')
    fig, ax = plt.subplots()
    ax.hist(ordered,bins=20,density=True)
    ax.set_title(f"{key}: mean values for {reps} draws")
    ax.axvline(x=lower, color='r')
    ax.axvline(x=upper, color='r')
    ax.axvline(x=np.mean(ordered), color='k')
    ax.axvline(x=mean,color='g')
    print(f"{(1-alpha)*100}% {key} CI: ({lower}, {upper}), mean: {mean}, {np.mean(ordered)}")

#+END_SRC

#+RESULTS:
:RESULTS:
: 95.0% Arsenic CI: (43.529999999999994, 165.59000000000003), mean: 98.352, 98.1115085
: 95.0% logAs CI: (2.3219106225, 4.0088886200000005), mean: 3.1726668679999994, 3.172938877997
[[file:./.ob-jupyter/7b374687e7a8b58eee8a1efe1ebe918e0f06c461.png]]
[[file:./.ob-jupyter/6bb053d99efbbd1ff94bb8b0ca4b09c432076873.png]]
:END:

** Prediction Intervals
what is the range of values that one (or more) future obs would lie between,
with a specified confidence? The interval will get wider the more observations
you want to predict. a Tolerance interval is used if you want to predict all of
the next year's samples for instance. or many new obs.

1-sided prediction interval: "what is a value that one (or more) future obs
would not exceed, with a specified confidence."

If a future obs falls outside (above) the expected range, it is evidence that
the obs is not from the same distribution as the previous data.

Ex1: used to establish the value from a set of blanks that a new measurement
would exceed in order to be considered "not a blank" Ex2: used to establish the
value from a set of background concentrations that a new measurement would
exceed in order to be considered "not background"

A prediction interval will always be larger than the confidence interval for the
same alpha. Why? The mean of 10 obs is always less variable than the location of
the 10 obs themselves.

Easiest way to compute them is to transform data and use a parametric method.
Compute the interval using, say, the t-interval for a normal dist and transform
its endpoints back to original units. This doesn't work for confidence
intervals/means, but it works fine for prediction intervals. Nonparametric
prediction intervals need much more data (probably more than 40, but more is better).


Transforming data to symmetry means that intervals are around the geometric
mean. This OK for prediction and tolerance intervals --NOT Confidence Intervals.
R> intrvls(X, confidence, # of new obs)
> intrvls(arsenic$conc, 95, 1, LOG=1)

** Tolerance Intervals
An interval around a proportion or quantile of the distribution
"What cutoff will cover 95% of all future observations, with a 90% confidence
interval."

Often, one-sided intervals are of interest.

"What value must any/all observations exceed in order to not be considered
'background', with a specified probability?"

Bootstrap Tolerance Intervals.E.g. The 95% upper tolerance limit of the 0.9
quantile
Resample the data 10k times with replacement.
For each resampling, calculate the 0.9 quantile.
Get the 95th percentile of these 10k values. This is the bootstrap UTL on the
90th quantile.

"We are 95% sure that no more than 90% of future obs under these same
conditions, will be greater than some value."

** Quantile Estimates
Sample quantiles have different definitions (different in python/R)

Default in R is type=7, where i is the rank of an observation, then the coverage
associated with that rank is (i-1)/(n-1), where n is the sample size.
For type=7, the highest observation (i=n) will always be at a probability of
1.0, so there is no probability of exceeding the current max. This is
unrealistic when using sample data to represent a larger population.

WEIBULL PLOTTING POSITION: Type=6. Rank is i/(n+1), so the current max has a
probability of < 1.0. Use Weibull for environmental data, unless there is some
reason you can never get a future value higher that the current max.

** PYTHON PS
Using the MOLY data column in the moly2.rda dataset, find
1) a 95% confidence interval for the population mean,
2) a 95% prediction interval for one future observation,
3) a 95% prediction interval to include the next 10 observations, and
(A) Assume the data is normal, and then that the data is lognormal.
HINT: intrvls (x, conf=95, npred =1, LOG =0)
LOG=0 assumes a normal distribution.
(B) Use the bootmean script to get
1) the two-sided 95% bootstrap interval on the mean.
2) an upper and lower one-sided 95% confidence limit on the mean by bootstrap.
HINT: Bootmean (x,conf = 95, R = 10000) for 2-sided interval
bootUCL (x,conf = 95, R = 10000) for 1-sided upper limit
bootLCL (x,conf = 95, R = 10000) for 1-sided lower limit
(C) Find the one-sided upper 95% tolerance limit on the 85 TH percentile for the
downgradient data, assuming a normal and lognormal distributions, and then
bootstrap the limit.
HINT: Tolerance (x, cover = 85 , conf = 95, TYPE = 6)
bootTOLupper (x, nrep = 10000, cover = 85, conf = 95, TYPE = 6)
(D) Prediction intervals on 1 new observation assuming NO distribution:
HINT: NPpred (MOLY, conf = 95, TYPE = 6)

1)
#+BEGIN_SRC jupyter-python :var data = moly
x = [x[0] for x in data if "DOWNGRAD" in x[1]]
y = [y[0] for y in data if "upgrad" in y[1]]
mannwhitneyu(x,y,alternative='less')
#+END_SRC
#+BEGIN_SRC jupyter-python :var data = moly
x = [x[0] for x in data]
means = []
reps=100000
n_size=10
alpha = 0.05
mean, sigma = np.mean(x), np.std(x, ddof=1)
for i in np.arange(1,reps+1):
    sample = np.random.choice(x, n_size, replace=True)
    stat = np.mean(sample)
    means.append(stat)
ordered = np.sort(means)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
fig, ax = plt.subplots()
ax.hist(ordered,bins=20,density=True)
ax.set_title(f"Molybdenum: 95% CI for mean values, {reps} draws")
ax.axvline(x=lower, color='r')
ax.axvline(x=upper, color='r')
ax.axvline(x=np.mean(ordered), color='k')
ax.axvline(x=mean,color='g')
print(f"{(1-alpha)*100}% {key} CI: ({lower}, {upper}), mean: {mean}, {np.mean(ordered)}")

#+END_SRC

#+RESULTS:
:RESULTS:
: 95.0% Log Arsenic CI: (0.1911, 2.2158), mean: 0.9385000000000001, 0.939389039
[[file:./.ob-jupyter/f2fd4ae4f7fc3894f020303b122f96ff3512ff1a.png]]
:END:
2) bootstrap: take np.percentile of both the lower and upper limit, then take
   the confidence limit of that data to get something like, "95% prediction
   limit with and 80% condifendence level" or similar.
   But wait! This doesn't account for 1 vs 10 additional predictions.
* Section 4: Testing Count Data, Paired Data, and Exceedance of Standards
** Contingency Tables
compare proportions of grouped data
Formed by two grouping variables -- are changes in row proportions associated
with changes in the column proportions?

Computer table of null hypothesis ratios, then compare with obs.
How large a difference is large enough to say proportions differ?

Permutations to computer all possible arrangements of counts.


Chi-square test statistic.(Large Sample Approximation)
Fischer's Exact Test (Just compute all possibilities if not too many)
Pearson's Chi-squared test (Just run a large number of permutations if too many
possible combinations)

histogram of permutation chi-square test statistics

#+TBLNAME: stream
| state  | stream |
|--------+--------|
| IMP    |      1 |
| IMP    |      1 |
| IMP    |      1 |
| IMP    |      1 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      2 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| IMP    |      3 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      1 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      2 |
| NonIMP |      3 |
| NonIMP |      3 |
| NonIMP |      3 |
| NonIMP |      3 |
| NonIMP |      3 |
| NonIMP |      3 |

#+BEGIN_SRC jupyter-python :var stream=stream
import numpy as np
import pandas as pd
import statsmodels.api as sm
df = pd.DataFrame(stream, columns=['State', 'Stream'])
tab = pd.crosstab(df['State'], df['Stream'])
table = sm.stats.Table(tab)
table.table_orig
#+END_SRC

#+RESULTS:
| State  |  1 |  2 |  3 |
|--------+----+----+----|
| IMP    |  4 |  8 | 12 |
| NonIMP | 16 | 12 |  6 |

Best-fitting independent distribution for our observed data (e.g. if the joint
distribution is independent, it can be written as the outer product of the row
and column marginal distributions)

(Association is the lack of independence)
#+BEGIN_SRC jupyter-python
table2 = sm.stats.Table.from_data(df)
table2.fittedvalues
#+END_SRC

#+RESULTS:
| State  |       1 |       2 |       3 |
|--------+---------+---------+---------|
| IMP    | 8.27586 | 8.27586 | 7.44828 |
| NonIMP | 11.7241 | 11.7241 | 10.5517 |

We can then view the residuals to identify particular cells that most strongly
violate independence

#+BEGIN_SRC jupyter-python
table2.resid_pearson
#+END_SRC

#+RESULTS:
| State  |        1 |          2 |        3 |
|--------+----------+------------+----------|
| IMP    | -1.48634 | -0.0958927 |  1.66782 |
| NonIMP |  1.24877 |  0.0805659 | -1.40125 |

Pearson's Chi-squared statistic (row's and columns unordered)

If the rows and columns of a table are unordered (i.e. are nominal factors),
then the most common approach for formally assessing independence is using
Pearson’s X2 statistic. It’s often useful to look at the cell-wise contributions
to the X2 statistic to see where the evidence for dependence is coming from.

#+BEGIN_SRC jupyter-python
rslt = table.test_nominal_association()
print(rslt.pvalue, rslt.df)
table.chi2_contribs
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.01405600064298329 2
| State   |       1 |          2 |       3 |
|---------+---------+------------+---------|
| IMP     | 2.2092  | 0.0091954  | 2.78161 |
| NonIMP  | 1.55943 | 0.00649087 | 1.96349 |
:END:
#+BEGIN_SRC jupyter-python :var stream=stream
import scipy.stats as stats
chi2, p, dof, ex = stats.chi2_contingency(stream, correction=False)
#(chi2, p) == stats.stats.chisquare(stream.ravel(), f_exp=ex.ravel(),
#                             ddof=stream.size - 1 - dof)
chi2
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example

TypeErrorTraceback (most recent call last)
<ipython-input-17-80b663e8c842> in <module>
      1 stream=[["IMP", 1], ["IMP", 1], ["IMP", 1], ["IMP", 1], ["IMP", 2], ["IMP", 2], ["IMP", 2], ["IMP", 2], ["IMP", 2], ["IMP", 2], ["IMP", 2], ["IMP", 2], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["IMP", 3], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 1], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 2], ["NonIMP", 3], ["NonIMP", 3], ["NonIMP", 3], ["NonIMP", 3], ["NonIMP", 3], ["NonIMP", 3]]
      2 import scipy.stats as stats
----> 3 chi2, p, dof, ex = stats.chi2_contingency(stream, correction=False)
      4 #(chi2, p) == stats.stats.chisquare(stream.ravel(), f_exp=ex.ravel(),
      5 #                             ddof=stream.size - 1 - dof)

~/.local/lib/python3.7/site-packages/scipy/stats/contingency.py in chi2_contingency(observed, correction, lambda_)
    242     """
    243     observed = np.asarray(observed)
--> 244     if np.any(observed < 0):
    245         raise ValueError("All values in `observed` must be nonnegative.")
    246     if observed.size == 0:

TypeError: '<' not supported between instances of 'numpy.ndarray' and 'int'
#+end_example
:END:
** Testing Differences Between Paired Values
- Two locations, sampled at the same times -- is one higher?
- Two methods, sampled at the same locations -- do they give similar results?
- Two times, sampled at the same locations -- has there been a change?

*** Matched Pair Tests
Direct relationship between observations in the same row. Most common blocks are
time or location. E.g. Urban vs Agricultural data at same times (Jan, Feb,
March) etc.

There may be an old method vs new method comparison at the same well for
instance.

EXAMPLE:
Soil lead was measured at the same sites in 1996 before a major fire and after
in 2001. Measurements were 'blocked' by location.

Q1: Are mean lead concentrations before the fire different than after the fire
(a two-sided test on means, i.e. a parametric test)?

Q2: Are concentrations consistently higher or lower after the fire (a two-sided
test on the median, i.e. a non-parametric test, a frequency test)?

Look at boxplot of residuals. Are they symmetric about zero? Normal?
*** The Sign Test
1. Compute differences between blocks, $d_i = x_i - y_i$
2. Record sign of d_i
3. Test Statistic S+ is the number of positive differences
4. The probability of getting as S+ equal to what was observed, or more extreme
   is the p-value. Null hypothesis is half above half below zero.
:pbsed:
#+TBLNAME: pbsed
| 2001 | 1996 |   D |
|------+------+-----|
|   18 |   16 |   2 |
|   18 |   17 |   1 |
|   21 |   17 |   4 |
|   20 |   17 |   3 |
|   10 |   16 |  -6 |
|   15 |   11 |   4 |
|   15 |   15 |   0 |
|   29 |   29 |   0 |
|   20 |   23 |  -3 |
|   12 |   11 |   1 |
|   17 |   19 |  -2 |
|   15 |   16 |  -1 |
|   13 |   11 |   2 |
|   18 |   18 |   0 |
|   16 |   18 |  -2 |
|   15 |   14 |   1 |
|   17 |   17 |   0 |
|   17 |   16 |   1 |
|   15 |   19 |  -4 |
|   16 |   21 |  -5 |
|   19 |   17 |   2 |
|   16 |   13 |   3 |
|   15 |   16 |  -1 |
|   18 |   15 |   3 |
|   18 |   16 |   2 |
|   22 |   17 |   5 |
|   16 |   19 |  -3 |
|   26 |   17 |   9 |
|   26 |   29 |  -3 |
|   14 |   33 | -19 |
|   16 |   16 |   0 |
|   13 |   10 |   3 |
|   14 |   13 |   1 |
|    8 |    9 |  -1 |
|   12 |   14 |  -2 |
|   11 |   20 |  -9 |
|   13 |   19 |  -6 |
|    9 |   10 |  -1 |
|   18 |   17 |   1 |
|   12 |   14 |  -2 |
|   11 |   11 |   0 |
|   11 |   13 |  -2 |
|   29 |   28 |   1 |
|    5 |    5 |   0 |
|    8 |    5 |   3 |
|    6 |    5 |   1 |
|    6 |    0 |   6 |
|   10 |   17 |  -7 |
|   21 |   18 |   3 |
|   39 |   24 |  15 |
|    8 |    9 |  -1 |
|   21 |   15 |   6 |
|   25 |   21 |   4 |
|   17 |   16 |   1 |
|    7 |    5 |   2 |
|    8 |    8 |   0 |
|    8 |    7 |   1 |
|   10 |    7 |   3 |
|   17 |   17 |   0 |
|   16 |   16 |   0 |
|   22 |   22 |   0 |
|   10 |   10 |   0 |
|   10 |    6 |   4 |
|   12 |   10 |   2 |
|   15 |   10 |   5 |
|   17 |   11 |   6 |
|   11 |   11 |   0 |
|    7 |    0 |   7 |
|   14 |    9 |   5 |
|    6 |    4 |   2 |
|   11 |   13 |  -2 |
|    9 |    8 |   1 |
|    4 |   10 |  -6 |
|    8 |    6 |   2 |
|   13 |   13 |   0 |
|    8 |    0 |   8 |
|   11 |    6 |   5 |
|    0 |    0 |   0 |
|    7 |    5 |   2 |
|    8 |    5 |   3 |
|   11 |    4 |   7 |
|    9 |    6 |   3 |


#+BEGIN_SRC jupyter-python :var pbsed = pbsed
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("white")
df = pd.DataFrame(pbsed, columns=["2001","1996","D"])

fig, ax = plt.subplots()
sns.boxplot(y=df["D"],
            color='lightblue',
            width=0.5,
            fliersize=0,
            ax=ax)
sns.stripplot(y=df["D"],
              jitter=True,
              color='black',
              marker='o',
              alpha=0.5,
              ax=ax)
ax.set_title('Pb Residuals 1996-2001')
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1.0, 'Pb Residuals 1996-2001')
[[file:./.ob-jupyter/27c053740d50bb4ff055a49f6bdddf3236ffd301.png]]
:END:
#+BEGIN_SRC jupyter-python
import statsmodels.stats as stats
stats.descriptivestats.sign_test(df["D"].values,0)
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
:
: AttributeErrorTraceback (most recent call last)
: <ipython-input-23-056440f5fe42> in <module>
:       1 import statsmodels.stats as stats
: ----> 2 stats.descriptivestats.sign_test(df["D"].values,0)
:
: AttributeError: module 'statsmodels.stats' has no attribute 'descriptivestats'
:END:
*** Matched Pair Wilcoxon Tests
more powerful than the sign test, takes into account the magnitude of the
difference.

A larger difference is stronger evidence than a smaller difference.

#+BEGIN_SRC jupyter-python
from scipy.stats import wilcoxon
w,p = wilcoxon(df["D"].values)
w,p
#+END_SRC

#+RESULTS:
| 758.0 | 0.016878270685203988 |
The null hypothesis that they have the same median is rejected. We expect about
2 in 100 times that this data would be as extreme as it is given that the
medians are from the same population. Expected p about 0.5.


*** Paired-t Test
Parametric test assumes normality of both datasets
null hypothesis: mean difference = 0
alt. hyp: mean diff is not =0

There is a permutation test if we think the data don't have a normal
distrubution. We can also use it for normal data, so we should just stick with
parametric test instead. If you do use this, do a Shapiro-Wilk Test to test for
normality.

*** Alternative: Permutation Analog of the t-test
1. compute differences for each pair of obs
2. compute and save the test statistic (z), the mean of paired differences,
3. compute a representation of H0: compute a random vector of plusses and minuses of the
  same size of the number of pairs or blocks (n)
4. Multiply the sign vector times the absolute value of the differences (i.e.
   randomly assign the sign of the residual). Compute
  test statistic. IF the null hypothesis is true, this should be something
   around zero.
5. Repeat 10,000 times, or compute the exact test 2^n times, whichever is
   smaller.
6. compute the p-value for the test.


This looks slightly different-- I think the main difference is it isn't paired,
so data randomly chosen
https://rasbt.github.io/mlxtend/user_guide/evaluate/permutation_test/

Under the null hypothesis (treatment = control), any permutations are equally
likely. (Note that there are (n+m)! permutations, where n is the number of
records in the treatment sample, and m is the number of records in the control
sample). For a two-sided test, we define the alternative hypothesis that the two
samples are different (e.g., treatment != control).

    1. Compute the difference (here: mean) of sample x and sample y
    2. Combine all measurements into a single dataset
    3. Draw a permuted dataset from all possible permutations of the dataset in 2.
    4. Divide the permuted dataset into two datasets x' and y' of size n and m,
       respectively
    5. Compute the difference (here: mean) of sample x' and sample y' and record
       this difference
    6. Repeat steps 3-5 until all permutations are evaluated
    7. Return the p-value as the number of times the recorded differences were more extreme than the original
    difference from 1. and divide this number by the total number of
    permutations

Here, the p-value is defined as the probability, given the null hypothesis (no
difference between the samples) is true, that we obtain results that are at
least as extreme as the results we observed (i.e., the sample difference from
1.).
   #+BEGIN_SRC jupyter-python
import mlxtend
from mlxtend.evaluate import permutation_test

p_value = permutation_test(df["2001"].values,
                           df["1996"].values,
                           method='approximate',
                           num_rounds=10000,
                           seed=0)
p_value

   #+END_SRC

   #+RESULTS:
   : 0.4063

   Ahh, here is a paired permutation test
https://statlab.github.io/permute/user/one-sample.html
https://github.com/statlab/permute/blob/master/permute/core.py
#+BEGIN_SRC jupyter-python
from numpy.random import RandomState
from permute.core import one_sample

seed = RandomState(37)
(p, diff_means, dist) = one_sample(df["2001"].values,
                             df["1996"].values,
                             reps=10**5,
                             stat="mean",
                             alternative="two-sided",
                             keep_dist=True,
                             seed=seed)
ax = sns.distplot(dist)
ax.axvline(x=diff_means,
           ymin=0,
           ymax=0.2,
           color='r')
ax.axvline(x=-diff_means,
           ymin=0,
           ymax=0.2,
           color='r')
ax.text(-2,0.9,f'p-value:{p:0.3}')
ax.text(-2,0.8, f'diff means:{diff_means:0.3}')
ax.text(-2,0.7, f'n = {len(dist)}')
ax.set_title("Mean Differences for Null Hypothesis")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1.0, 'Mean Differences for Null Hypothesis')
[[file:./.ob-jupyter/4505b460a4e4fcc31794bac0596e6733592c34bb.png]]
:END:
This means there's a 9% chance you can see the observed difference in means when
the null hypothesis is true. If we have a=0.05, this is therefore not
significant and we don't find a difference between the two means.

We *did* find a difference in the medians. These answer two different questions.

These tests answer different questions! You have to decide what is more important.
Mean: the *mass* is not significantly different before or after the fire (permutation).
Median: after the fire there is *more often* higher Pb concentrations (wilcoxon)

To check! do a bootstrap of the residuals normal density, a 95% CI boostrap line (is 0 in
the interval? Then we know it is a plausible estimate of the mean difference)

#+BEGIN_SRC jupyter-python
reps = 10000
n_size = len(df)
value = df["D"].values
means = []
mean, sigma = np.mean(value), np.std(value, ddof=1)
for i in np.arange(1,reps+1):
    sample = np.random.choice(value, n_size, replace=True)
    stat = np.mean(sample)
    means.append(stat)
ordered = np.sort(means)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
print(f"upper= {upper}, lower={lower}")
ax = sns.distplot(means, label=f"Bootstrap means,\n n = {reps}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.hlines(y=0,
          xmin=lower,
          xmax=upper,
          color='r',
          lw=2,
          label="95%CL")
ax.axvline(x=mean,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean:0.3}")
ax.set_title("Bootstrap of Differences of Means, 95%CL")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example

KeyErrorTraceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2645             try:
-> 2646                 return self._engine.get_loc(key)
   2647             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'D'

During handling of the above exception, another exception occurred:

KeyErrorTraceback (most recent call last)
<ipython-input-138-baf92027e009> in <module>
      1 reps = 10000
      2 n_size = len(df)
----> 3 value = df["D"].values
      4 means = []
      5 mean, sigma = np.mean(value), np.std(value, ddof=1)

~/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py in __getitem__(self, key)
   2798             if self.columns.nlevels > 1:
   2799                 return self._getitem_multilevel(key)
-> 2800             indexer = self.columns.get_loc(key)
   2801             if is_integer(indexer):
   2802                 indexer = [indexer]

~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2646                 return self._engine.get_loc(key)
   2647             except KeyError:
-> 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)
   2650         if indexer.ndim > 1 or indexer.size > 1:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'D'
#+end_example
:END:
0 is included in the 95%CL and is therefore a plausible mean value (the one we
expect if there is not difference in means)
** PYTHON -- HANDOUT :ATTACH:
:PROPERTIES:
:ID:       0ac17682-7727-44f4-97b4-2b714e045f98
:END:

#+TBLNAME: totalp
| date      |  dp2 |  dp3 | diff |
|-----------+------+------+------|
| 08Jul2010 |  8.3 |  9.4 | -1.1 |
| 27Jul2010 |  8.6 |  9.2 | -0.6 |
| 16Sep2010 |  8.9 |  6.4 |  2.5 |
| 07Oct2010 |  9.6 |  6.9 |  2.7 |
| 23Jun2011 | 14.0 |  9.8 |  4.2 |
| 30Jun2011 | 10.0 | 11.0 | -1.0 |
| 07Jul2011 | 11.0 |  8.6 |  2.4 |
| 14Jul2011 | 11.0 | 11.0 |  0.0 |
| 21Jul2011 |  8.9 |  8.9 |  0.0 |
| 28Jul2011 |  9.8 | 11.0 | -1.2 |
| 02Aug2011 | 10.0 |  8.3 |  1.7 |
| 11Aug2011 |  8.8 |  6.8 |  2.0 |
| 18Aug2011 |  7.1 |  7.1 |  0.0 |
| 24Aug2011 |  8.0 |  7.1 |  0.9 |
| 01Sep2011 |  8.5 |  6.7 |  1.8 |
| 08Sep2011 | 10.0 |  7.2 |  2.8 |
| 22Sep2011 | 10.0 |  7.9 |  2.1 |
| 28Sep2011 |  8.7 |  7.6 |  1.1 |
| 03Oct2011 |  8.3 |  7.3 |  1.0 |
| 12Oct2011 |  9.8 |  8.5 |  1.3 |
| 17Oct2011 |  9.9 |  9.6 |  0.3 |
| 04Jul2012 | 11.0 |  9.3 |  1.7 |
| 19Jul2012 | 10.0 |  7.8 |  2.2 |
| 01Aug2012 |  9.5 |  7.4 |  2.1 |
| 08Aug2012 |  8.8 |  7.3 |  1.5 |
| 15Aug2012 |  8.7 |  7.3 |  1.4 |
| 22Aug2012 |  8.1 |  7.6 |  0.5 |
| 30Aug2012 |  7.7 |  7.7 |  0.0 |
| 12Sep2012 |  9.1 |  7.5 |  1.6 |
| 20Sep2012 | 11.0 |  7.1 |  3.9 |
| 27Sep2012 |  7.3 |  6.4 |  0.9 |
| 03Oct2012 |  7.4 |  7.2 |  0.2 |
| 17Oct2012 |  8.3 |  7.9 |  0.4 |
Total Phosphorus (Total P) was measured at three sites below the impoundment on
the West Branch of the Penobscot River. Compare two of these downstream sites,
sites DP2 and DP3, to see if Total P is the same at both. What characteristic is used
to pair data between the groups? What characteristic will a nonparametric test
compare? What characteristic will a permutation test compare? Compute the
paired t-test, the signed-rank test, and the paired permutation test and interpret the
results.

*** Nonparametric Test (Wilcoxon) -- Median
For the wilcoxon test used in python scipy.stats, The two-sided test has the
null hypothesis that the median of the differences is zero against the
alternative that it is different from zero.


https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test
http://vassarstats.net/textbook/ch12a.html
In the following code, as nr increases, W converges to a normal distribution
nr> 20, a z-score can be calculated. Otherwise, with nr<10, a table of critical
W values must be used


Critical Values of ±z
| Level of Significance for a |       |       |       |       |       |
|-----------------------------+-------+-------+-------+-------+-------|
| Directional Test            |  0.05 |  .025 |   .01 |  .005 | .0005 |
| Non-Directional Test        |    -- |   .05 |   .02 |   .01 |  .001 |
| zcritical                   | 1.645 | 1.960 | 2.326 | 2.576 | 3.291 |

Critical Values of ±W for Small Samples:
| Level of Significance for a |     |      |     |       |
|-----------------------------+-----+------+-----+-------|
|            Directional Test | .05 | .025 | .01 | .005  |
|        Non-Directional Test |  -- |  .05 | .02 | .01   |
|                           N |     |      |     |       |
|                           5 |  15 |   -- |  -- | --    |
|                           6 |  17 |   21 |  -- | --    |
|                           7 |  22 |   24 |  28 | --    |
|                           8 |  26 |   30 |  34 | 36    |
|                           9 |  29 |   35 |  39 | 43    |

#+BEGIN_SRC jupyter-python :var totalp=totalp
from scipy.stats import wilcoxon
df = pd.DataFrame(totalp, columns=["date","dp2","dp3","diff"])
w,p = wilcoxon(df["diff"].values)

# Calculate reduced sample size, discard pairs where difference is 0
nr = len(df.loc[df["diff"]!=0])

# Calculate z-score for reduced sample size
sigma_w = np.sqrt(nr*(nr+1)*(2*nr+1)/6)
z = (w+0.5)/sigma_w # continuity correction
# +0.5 when W < sigma_w, -0.5 if W > sigma

w, p, sigma_w, z
#+END_SRC

#+RESULTS:
| 36.0 | 8.66632613586137e-05 | 92.49324299644812 | 0.39462342131739997 |
Zcritical is 1.96. Since zcritical > | z | , we do not reject H0. Remember
that for something to be considered significant (leading us to reject null
hypothesis) then the calculated Z score must be farther away from the mean than
the critical value.

[[download:0a/c17682-7727-44f4-97b4-2b714e045f98/_20200726_114416Critical Z
score.JPG]]

But the p-value is much smaller than 0.05, so we reject the null hypothesis that
this observation is likely to have occured if that data are from the same
population.

I am unsure why the p-value and z-score interpretations do not accord. We will
stick with the original test result of p-value. The null hypothesis that the
median of the differences is zero is rejected.

#+BEGIN_SRC jupyter-python
sns.boxplot(data=df, order=["dp2","dp3","diff"])
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f29c014bdd0>
[[file:./.ob-jupyter/93b64c8f6dd719e18899747fd32e715c79638845.png]]
:END:
We see these data are skewed. This could cause problems for a parametric test.

*** Permutation Test -- Mean
#+BEGIN_SRC jupyter-python
from numpy.random import RandomState
from permute.core import one_sample

# seed = RandomState(37)
(p, diff_means, dist) = one_sample(df["diff"].values,
                             reps=10**5,
                             stat="mean",
                             alternative="two-sided",
                             keep_dist=True)
                             #seed=seed)
ax = sns.distplot(dist)
ax.axvline(x=diff_means,
           ymin=0,
           ymax=0.2,
           color='r')
ax.axvline(x=-diff_means,
           ymin=0,
           ymax=0.2,
           color='r')
ax.text(-1.3,1.3,F'P-value:{p:0.3}')
ax.text(-1.3,1.2, f'diff means:{diff_means:0.3}')
ax.text(-1.3,1.1, f'n = {len(dist)}')
ax.set_title("Mean Differences for Null Hypothesis")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1.0, 'Mean Differences for Null Hypothesis')
[[file:./.ob-jupyter/00c7393819ea03652a8e553f8f366a634ae50efd.png]]
:END:

There is a 2 in 10000 chance we'd observe this difference in means if these come
from the same population. The mass loading of phosphorous is different in these sites.

Let's look at 95%CL on the mean

#+BEGIN_SRC jupyter-python
reps = 10000
n_size = len(df)
value = df["diff"].values
means = []
mean, sigma = np.mean(value), np.std(value, ddof=1)
for i in np.arange(1,reps+1):
    sample = np.random.choice(value, n_size, replace=True)
    stat = np.mean(sample)
    means.append(stat)
ordered = np.sort(means)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
print(f"upper= {upper}, lower={lower}")
ax = sns.distplot(means, label=f"Bootstrap means,\n n = {reps}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.hlines(y=0,
          xmin=lower,
          xmax=upper,
          color='r',
          lw=2,
          label=f"95%CI \n ({lower:0.02}, {upper:0.02})")
ax.axvline(x=mean,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean:0.3}")
ax.set_title("Bootstrap of Difference of Means, 95%CI")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: upper= 1.6393939393939394, lower=0.7469696969696968
: <matplotlib.legend.Legend at 0x7f29b7e88c50>
[[file:./.ob-jupyter/39b7d265e6571de3834825e0802f17674a449469.png]]
:END:

The 95%CI on the mean does not include 0. We expect 0 to be included if they are
plausibly from the same population.

*** Summary:
The nonparametric test (Wilcoxon) compares medians. We reject that they
come from the same population because of the very small p-value, indicating only
an extremely rare observation would result in the observed difference of medians
if they indeed come from the same sample population. Downstream site dp2 has
median concentrations of phosphorous that are generally greater than site dp3.

The permutation test shows that there is a 2 in 10000 chance we'd observe this
difference in means if these come from the same population. The mass loading of
phosphorous is different in these sites. The 95%CI does not include 0.

*** Questions

PS 4, total phosphorous, why am I gettin a zscore interpretation different from
p-value interpretation?
What's a good rule of thumb for bootstrap subsample size? Just choose the same
size as original sample data! Resampling might select the same point twice. This
is the largest and most defensible way.

** Comparing Data to a Standard
Testing against a numerical standard.
Based on a statistical test, do my data exceed the standard?

Comparing data to a standard:
1. Does the mean exceed a standard? Use the one-sample permutation or t-test
2. Does the median exceed a standard? (This is not a standard question to ask,
   standards rarely use the median) a sign or sign-rank test (wilcoxon) would
   answer this.
3. Do more than X% of data (a percentile) exceed a standard? USE A QUANTILE
   TEST
*** Quantile Test
Computes a tolerance interval on a percentile.
   This is parallel to a t-test (which computes an confidence interval compared
   with standard)

**** One-sample t-test: use confidence interval, Assume Compliance
   H0: assume compliance. To reject the null hypothesis at a=0.05, the 95% LCL
   on the mean must be above the standard.

   Example: the sample mean is above the standard. But this doesn't take into
   account the uncertainty in the where the population mean might be. We need to
   compute the lowel confidence level LCL95 to use as a comparison.

   IF we do not reject H0, we are in compliance.

   If you assume compliance, it may be difficult to prove if the data are not
   normal, and you are using a t-test. Permutation is best.
**** Alternately, you can assume non-compliance
(the pop mean is above the standard) To reject the null hypothesis at a =0.05
   the 95% UCL must be below the standard.

   Example: the sample mean is below the standard, but the UCL95 is above the
   standard. It is plausible at the 95% level that the population mean could
   have a value above the standard. Therefore, do not reject that the population
   mean is out of compliance.

   This is a strong burden of proof that the mean "in the field" must meet.

   To carry this out: compute differences: d_i = x_i - X0, where X0 is the
   standard.
   don't use two-sided. Use the alternative="greater"

**** Quantile Test
H0: the % exceedances of X0 <= 1-q

There are no more than 10% exceedances of the standard. Cannot reject the
assumed compliance.

H1: The % exceedances of X0 > 1-q

There are more than 10% exceedances of the standard. Reject compliance and find
the site to be out of compliance.

This is based on the binomial distribution and a version of the binomial test.
Similar to the sign test, but the percentile is being tested and not the median.

Compare each obs to the standard, count how many are larger, how many are
smaller.
*** PYTHON
#+BEGIN_SRC jupyter-python :var arsenic=arsenic
As = pd.DataFrame(arsenic, columns=["as", "ln_as"])

# Get difference from drinking water standard 10ug/L
As["diff"] = As["as"] - 10
As
#+END_SRC

#+RESULTS:
|    |  as |    ln_as | diff |
|----+-----+----------+------|
|  0 | 1.3 | 0.262364 | -8.7 |
|  1 | 1.5 | 0.405465 | -8.5 |
|  2 | 1.8 | 0.587787 | -8.2 |
|  3 | 2.6 | 0.955511 | -7.4 |
|  4 | 2.8 |  1.02962 | -7.2 |
|  5 | 3.5 |  1.25276 | -6.5 |
|  6 |   4 |  1.38629 |   -6 |
|  7 | 4.8 |  1.56862 | -5.2 |
|  8 |   8 |  2.07944 |   -2 |
|  9 | 9.5 |  2.25129 | -0.5 |
| 10 |  12 |  2.48491 |    2 |
| 11 |  14 |  2.63906 |    4 |
| 12 |  19 |  2.94444 |    9 |
| 13 |  23 |  3.13549 |   13 |
| 14 |  41 |  3.71357 |   31 |
| 15 |  80 |  4.38203 |   70 |
| 16 | 100 |  4.60517 |   90 |
| 17 | 110 |  4.70048 |  100 |
| 18 | 120 |  4.78749 |  110 |
| 19 | 190 |  5.24702 |  180 |
| 20 | 240 |  5.48064 |  230 |
| 21 | 250 |  5.52146 |  240 |
| 22 | 300 |  5.70378 |  290 |
| 23 | 340 |  5.82895 |  330 |
| 24 | 580 |  6.36303 |  570 |
#+BEGIN_SRC jupyter-python

#+END_SRC

#+BEGIN_SRC jupyter-python
from numpy.random import RandomState
from permute.core import one_sample

# seed = RandomState(37)
(p, diff_means, dist) = one_sample(As["diff"].values,
                             reps=10**5,
                             stat="mean",
                             alternative="greater",
                             keep_dist=True)
                             #seed=seed)
print(p,diff_means)
ax = sns.distplot(dist,
                  label = f'''p-value = {p:0.3} \n
                  diff means:{diff_means:0.3} \n
                  n = {len(dist):,}''')
ax.axvline(x=diff_means,
           color='r',
           label="Mean (obs)")
ax.legend(loc="upper left")
ax.set_title("Mean Differences for Null Hypothesis")
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.0005499945000549994 88.352
: Text(0.5, 1.0, 'Mean Differences for Null Hypothesis')
[[file:./.ob-jupyter/f7b5bf77cfab3019b035057384c303ac7141e365.png]]
:END:

#+BEGIN_SRC jupyter-python
from scipy import stats
exceedances = len(As.loc[As["as"]>10]["as"].values)
stats.binom_test(exceedances, n=len(As), p=0.1, alternative='greater')
#+END_SRC

#+RESULTS:
: 1.2237906522763768e-09

We reject null hypothesis. The p-value is 10^-9 that this sample set is drawn
from a population in which 90% of the data is below the drinking water standard.
add Confidence interval around

*** PYTHON Comparinmg Data to a Standard
Testing Compliance: Is a standard exceeded?

Nitrate (as nitrogen, NO3-N) was sampled in the groundwater below a county in
Nebraska in 1974 and 1984. An evaluation was done in 1984 by the State to
determine whether the mean concentration in each of its counties exceeded the
drinking water standard of 10 mg/L.

1. Perform a test on the 1984 data assuming compliance, with the alternative that
the mean concentration in this county exceeds 10 mg/L, to determine whether the
mean is significantly higher than 10. Use the NO3_84 column in ‘NEB2’.
Hint: to test whether the mean exceeds a standard, so assuming compliance,
compute the test with alternative="greater" and setting the hypothesized mean to
be the value of the standard.

2. What if it were assumed that concentrations were out of compliance unless
proven otherwise? Could it be shown that the mean concentration in the county
was significantly lower than 10 mg/L?

Hint: this test assuming noncompliance is done using alternative="less".

**** Data
#+TBLNAME: NO3
| depth74 | NO3_74 | depth84 | NO3_84 |
|---------+--------+---------+--------|
|      25 |   0.30 |      25 |   0.59 |
|      27 |   3.80 |      28 |   9.90 |
|      28 |   6.70 |      30 |   0.16 |
|      30 |   0.30 |      30 |   0.26 |
|      30 |  24.00 |      38 |   1.00 |
|      38 |   0.30 |      40 |  18.00 |
|      40 |  27.00 |      50 |  21.00 |
|      40 |   7.10 |      50 |  21.00 |
|      40 |   7.90 |      50 |  22.00 |
|      45 |  24.00 |      50 |  23.00 |
|      50 |   1.90 |      50 |  26.00 |
|      50 |   4.60 |      50 |  30.00 |
|      50 |   4.90 |      50 |   0.66 |
|      50 |   6.50 |      50 |   2.10 |
|      50 |   7.40 |      50 |   9.00 |
|      50 |   7.70 |      50 |  14.00 |
|      50 |   9.50 |      50 |  16.00 |
|      50 |  15.00 |      50 |  17.00 |
|      54 |   8.60 |      50 |  18.00 |
|      55 |  17.00 |      54 |  24.00 |
|      56 |   6.50 |      54 |  16.00 |
|      56 |  10.00 |      57 |   4.80 |
|      58 |   8.00 |      58 |  18.00 |
|      60 |   0.10 |      60 |   0.20 |
|      60 |   0.10 |      60 |   0.22 |
|      60 |   0.30 |      60 |  17.00 |
|      60 |   0.30 |      60 |  17.00 |
|      60 |  36.00 |      60 |  19.00 |
|      60 |   1.10 |      60 |  20.00 |
|      60 |   2.10 |      60 |  22.00 |
|      60 |   4.00 |      60 |  23.00 |
|      60 |   9.70 |      60 |  30.00 |
|      60 |  11.00 |      60 |  33.00 |
|      60 |  13.00 |      60 |  49.00 |
|      60 |  16.00 |      60 |   0.41 |
|      62 |   5.40 |      60 |   2.00 |
|      63 |  12.00 |      60 |   2.20 |
|      65 |   0.10 |      60 |   2.60 |
|      65 |   0.30 |      60 |  12.00 |
|      65 |  23.00 |      60 |  14.00 |
|      65 |   1.80 |      62 |   5.00 |
|      65 |  10.00 |      63 |  21.00 |
|      68 |  20.00 |      63 |   0.63 |
|      72 |  16.00 |      64 |  21.00 |
|      72 |   4.10 |      65 |   0.10 |
|      75 |  16.00 |      65 |   0.10 |
|      75 |  19.00 |      65 |  34.00 |
|      77 |   0.10 |      65 |  11.00 |
|      77 |  14.00 |      68 |  28.00 |
|      77 |  16.00 |      72 |   0.33 |
|      77 |   2.10 |      72 |  23.00 |
|      80 |   1.10 |      72 |   5.50 |
|      80 |   3.40 |      75 |  21.00 |
|      80 |   4.70 |      75 |  37.00 |
|      80 |   7.30 |      77 |  13.00 |
|      84 |   6.10 |      77 |  17.00 |
|      85 |   0.10 |      77 |  24.00 |
|      85 |   4.30 |      77 |  26.00 |
|      86 |   0.80 |      77 |  29.00 |
|      90 |   0.02 |      80 |  13.00 |
|      90 |   0.90 |      80 |   6.20 |
|      92 |   0.10 |      82 |   0.24 |
|      92 |  16.00 |      85 |   0.05 |
|      95 |   0.10 |      85 |   4.90 |
|      95 |   0.50 |      92 |  23.00 |
|     100 |   0.10 |     100 |   0.06 |
|     100 |  15.00 |     100 |   0.14 |
|     100 |   0.50 |     100 |  12.00 |
|     100 |   4.00 |     100 |  25.00 |
|     100 |   5.40 |     100 |   0.70 |
|     104 |  12.00 |     100 |   2.40 |
|     106 |   0.10 |     104 |  14.00 |
|     110 |   1.70 |     105 |  10.00 |
|     110 |   1.80 |     106 |   0.14 |
|     113 |   3.00 |     110 |   0.16 |
|     114 |   0.10 |     113 |   3.20 |
|     130 |  19.00 |     130 |   8.90 |
|     130 |   6.80 |     130 |  25.00 |
|     175 |   4.30 |     135 |   0.73 |
|      NA |     NA |     160 |   0.39 |
**** Part 1: Assume Compliance
 H0: assume compliance. To reject the null hypothesis at a=0.05, the 95% LCL
   on the mean must be above the standard.

#+BEGIN_SRC jupyter-python :var NO3=NO3
no3 = pd.DataFrame(NO3, columns=["depth74","NO3_74","depth84","NO3_84"])
no3["diff84"] = no3["NO3_84"] - 10
#+END_SRC

#+RESULTS:
|    | depth74 | NO3_74 | depth84 | NO3_84 | diff84 |
|----+---------+--------+---------+--------+--------|
|  0 |      25 |    0.3 |      25 |   0.59 |  -9.41 |
|  1 |      27 |    3.8 |      28 |    9.9 |   -0.1 |
|  2 |      28 |    6.7 |      30 |   0.16 |  -9.84 |
|  3 |      30 |    0.3 |      30 |   0.26 |  -9.74 |
|  4 |      30 |   24.0 |      38 |      1 |     -9 |
|  5 |      38 |    0.3 |      40 |     18 |      8 |
|  6 |      40 |   27.0 |      50 |     21 |     11 |
|  7 |      40 |    7.1 |      50 |     21 |     11 |
|  8 |      40 |    7.9 |      50 |     22 |     12 |
|  9 |      45 |   24.0 |      50 |     23 |     13 |
| 10 |      50 |    1.9 |      50 |     26 |     16 |
| 11 |      50 |    4.6 |      50 |     30 |     20 |
| 12 |      50 |    4.9 |      50 |   0.66 |  -9.34 |
| 13 |      50 |    6.5 |      50 |    2.1 |   -7.9 |
| 14 |      50 |    7.4 |      50 |      9 |     -1 |
| 15 |      50 |    7.7 |      50 |     14 |      4 |
| 16 |      50 |    9.5 |      50 |     16 |      6 |
| 17 |      50 |   15.0 |      50 |     17 |      7 |
| 18 |      54 |    8.6 |      50 |     18 |      8 |
| 19 |      55 |   17.0 |      54 |     24 |     14 |
| 20 |      56 |    6.5 |      54 |     16 |      6 |
| 21 |      56 |   10.0 |      57 |    4.8 |   -5.2 |
| 22 |      58 |    8.0 |      58 |     18 |      8 |
| 23 |      60 |    0.1 |      60 |    0.2 |   -9.8 |
| 24 |      60 |    0.1 |      60 |   0.22 |  -9.78 |
| 25 |      60 |    0.3 |      60 |     17 |      7 |
| 26 |      60 |    0.3 |      60 |     17 |      7 |
| 27 |      60 |   36.0 |      60 |     19 |      9 |
| 28 |      60 |    1.1 |      60 |     20 |     10 |
| 29 |      60 |    2.1 |      60 |     22 |     12 |
| 30 |      60 |    4.0 |      60 |     23 |     13 |
| 31 |      60 |    9.7 |      60 |     30 |     20 |
| 32 |      60 |   11.0 |      60 |     33 |     23 |
| 33 |      60 |   13.0 |      60 |     49 |     39 |
| 34 |      60 |   16.0 |      60 |   0.41 |  -9.59 |
| 35 |      62 |    5.4 |      60 |      2 |     -8 |
| 36 |      63 |   12.0 |      60 |    2.2 |   -7.8 |
| 37 |      65 |    0.1 |      60 |    2.6 |   -7.4 |
| 38 |      65 |    0.3 |      60 |     12 |      2 |
| 39 |      65 |   23.0 |      60 |     14 |      4 |
| 40 |      65 |    1.8 |      62 |      5 |     -5 |
| 41 |      65 |   10.0 |      63 |     21 |     11 |
| 42 |      68 |   20.0 |      63 |   0.63 |  -9.37 |
| 43 |      72 |   16.0 |      64 |     21 |     11 |
| 44 |      72 |    4.1 |      65 |    0.1 |   -9.9 |
| 45 |      75 |   16.0 |      65 |    0.1 |   -9.9 |
| 46 |      75 |   19.0 |      65 |     34 |     24 |
| 47 |      77 |    0.1 |      65 |     11 |      1 |
| 48 |      77 |   14.0 |      68 |     28 |     18 |
| 49 |      77 |   16.0 |      72 |   0.33 |  -9.67 |
| 50 |      77 |    2.1 |      72 |     23 |     13 |
| 51 |      80 |    1.1 |      72 |    5.5 |   -4.5 |
| 52 |      80 |    3.4 |      75 |     21 |     11 |
| 53 |      80 |    4.7 |      75 |     37 |     27 |
| 54 |      80 |    7.3 |      77 |     13 |      3 |
| 55 |      84 |    6.1 |      77 |     17 |      7 |
| 56 |      85 |    0.1 |      77 |     24 |     14 |
| 57 |      85 |    4.3 |      77 |     26 |     16 |
| 58 |      86 |    0.8 |      77 |     29 |     19 |
| 59 |      90 |   0.02 |      80 |     13 |      3 |
| 60 |      90 |    0.9 |      80 |    6.2 |   -3.8 |
| 61 |      92 |    0.1 |      82 |   0.24 |  -9.76 |
| 62 |      92 |   16.0 |      85 |   0.05 |  -9.95 |
| 63 |      95 |    0.1 |      85 |    4.9 |   -5.1 |
| 64 |      95 |    0.5 |      92 |     23 |     13 |
| 65 |     100 |    0.1 |     100 |   0.06 |  -9.94 |
| 66 |     100 |   15.0 |     100 |   0.14 |  -9.86 |
| 67 |     100 |    0.5 |     100 |     12 |      2 |
| 68 |     100 |    4.0 |     100 |     25 |     15 |
| 69 |     100 |    5.4 |     100 |    0.7 |   -9.3 |
| 70 |     104 |   12.0 |     100 |    2.4 |   -7.6 |
| 71 |     106 |    0.1 |     104 |     14 |      4 |
| 72 |     110 |    1.7 |     105 |     10 |      0 |
| 73 |     110 |    1.8 |     106 |   0.14 |  -9.86 |
| 74 |     113 |    3.0 |     110 |   0.16 |  -9.84 |
| 75 |     114 |    0.1 |     113 |    3.2 |   -6.8 |
| 76 |     130 |   19.0 |     130 |    8.9 |   -1.1 |
| 77 |     130 |    6.8 |     130 |     25 |     15 |
| 78 |     175 |    4.3 |     135 |   0.73 |  -9.27 |
| 79 |      NA |     NA |     160 |   0.39 |  -9.61 |
#+BEGIN_SRC jupyter-python
sns.boxplot(data=no3, order=["NO3_84", "diff84"])
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f29a2bcb0d0>
[[file:./.ob-jupyter/49f237411174bf31ed07a38402d237febb68cde1.png]]
:END:

#+BEGIN_SRC jupyter-python
from numpy.random import RandomState
from permute.core import one_sample

# seed = RandomState(37)
(p, diff_means, dist) = one_sample(no3["diff84"].values,
                             reps=10**5,
                             stat="mean",
                             alternative="greater",
                             keep_dist=True)
                             #seed=seed)
print(p,diff_means)
ax = sns.distplot(dist,
                  label = f"p-value = {p:0.3}")
ax.axvline(x=diff_means,
           color='r',
           label=f"Mean (obs) {diff_means:0.3}")
ax.legend(loc="upper left")
ax.set_title("Mean Differences for Null Hypothesis")
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.01001989980100199 3.0371250000000005
: Text(0.5, 1.0, 'Mean Differences for Null Hypothesis')
[[file:./.ob-jupyter/10592bcbd809a78ecc7fca44629f84fabc9fcb26.png]]
:END:

p-value < 0.05, so we reject the null hypothesis that this site is in compliance.


#+BEGIN_SRC jupyter-python
reps = 10000
n_size = len(no3)
value = no3["NO3_84"].values
means = []
mean, sigma = np.mean(value), np.std(value, ddof=1)
for i in np.arange(1,reps+1):
    sample = np.random.choice(value, n_size, replace=True)
    stat = np.mean(sample)
    means.append(stat)
ordered = np.sort(means)
lower = np.percentile(ordered, 5, interpolation='midpoint')
#upper = np.percentile(ordered, 95, interpolation='midpoint')
print(f"upper= {upper}")
ax = sns.distplot(means, label=f"Bootstrap means,\n n = {reps:,}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.axvline(x=lower,
          color='r',
          label=f"LCL95 \n ({lower:0.02})")
ax.axvline(x=mean,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean:0.3}")
ax.set_title("Bootstrap of Difference of Means")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: upper= 15.142499999999998
: <matplotlib.legend.Legend at 0x7f29a2418850>
[[file:./.ob-jupyter/2c65dfa62eb5f8590a000fe84564b64045210b32.png]]
:END:
The 95% Lower Confidence Level exceeds 10. The mean is reasonably greater than
the drinking water standard.
**** Part 2: Assume Non-compliance
H0: the pop mean is above the standard. To reject the null hypothesis at a = 0.05
   the 95% UCL must be below the standard.

#+BEGIN_SRC jupyter-python
from numpy.random import RandomState
from permute.core import one_sample

# seed = RandomState(37)
(p, diff_means, dist) = one_sample(no3["diff84"].values,
                             reps=10**5,
                             stat="mean",
                             alternative="less",
                             keep_dist=True)
                             #seed=seed)
print(p,diff_means)
ax = sns.distplot(dist,
                  label = f"p-value = {p:0.3}")
ax.axvline(x=diff_means,
           color='r',
           label=f"Mean (obs) {diff_means:0.3}")
ax.legend(loc="upper left")
ax.set_title("Mean Differences for Null Hypothesis")
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.990580094199058 3.0371250000000005
: Text(0.5, 1.0, 'Mean Differences for Null Hypothesis')
[[file:./.ob-jupyter/e7c54f2474cb397944ef0bd2bb78f069f9decc29.png]]
:END:

The p-value is greater than 0.05. We cannot reject H0.

#+BEGIN_SRC jupyter-python
reps = 10000
n_size = len(no3)
value = no3["NO3_84"].values
means = []
mean, sigma = np.mean(value), np.std(value, ddof=1)
for i in np.arange(1,reps+1):
    sample = np.random.choice(value, n_size, replace=True)
    stat = np.mean(sample)
    means.append(stat)
ordered = np.sort(means)
#lower = np.percentile(ordered, 5, interpolation='midpoint')
upper = np.percentile(ordered, 95, interpolation='midpoint')
print(f"upper= {upper}")
ax = sns.distplot(means, label=f"Bootstrap means,\n n = {reps:,}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.axvline(x=upper,
          color='r',
          label=f"UCL95 \n ({upper:0.02})")
ax.axvline(x=mean,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean:0.3}")
ax.set_title("Bootstrap of Difference of Means")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: upper= 15.142499999999998
: <matplotlib.legend.Legend at 0x7f29a22486d0>
[[file:./.ob-jupyter/3c9375e14311052a155aed3a179c5b6ff2c9a162.png]]
:END:
The UCL95 is above the drinking water standard. If the UCL fell below the
drinking water standard of 10, then we could say we are in compliance. The UCL
here is nowhere near that low, so we stick with H0 that we are out of
compliance.
* Section 5: Comparing Two Groups of Data
** Comparing 2 Independent Groups
No direct relation between an observation in one group and another. Just want to
know if means differ.

4 Tests to Answer "Is there a difference"
1. the two sample t-test
2. the two-sample t-test on logarithms (no longer tests diff in means, but if
   ratio of medians (geomean) = 1)
3. permutation test on difference in means
   H0: mean(group1)=mean(group2)
   randomly reassign diff in means and shuffle group assignment.
4. wilcoxon two-sample test (also called rank-sum test or mann-whitney test)
** Permutation Test with Miramar Data

Load Datasets
#+BEGIN_SRC jupyter-python
file = '~/Dropbox/Miramar.xlsx'

dtype_dict = {x:str for x in ['Al','Sb','As','Ba','Be','Cd','Cr','Co','Cu','Fe','Pb','Mn','Hg','Mo','Ni','Se','Ag','Tl','V','Zn']}
background = pd.read_excel(file, sheet_name='Background', converters=dtype_dict)
ir2 = pd.read_excel(file, sheet_name='IR Site 2', converters=dtype_dict)

# Create a list of flag columns, by selecting all the column names with a character length <= 2
f = list(ir2.columns)
flag = list(filter(lambda x: len(x) <= 2, f))

# Create a list of analytes
analytes = [item for item in f if 'mg/kg' in item]

# Create a dictionary to relate the flags and analytes
groupby_dict = {k:v for (k,v) in zip(flag, analytes)}

# Make sure the empty flag columns are strings and not NaN
background[flag]=background[flag].applymap(str).replace('nan','')
ir2[flag]=ir2[flag].applymap(str).replace('nan','')

# Convert any R-flagged data to NaN
for x in flag:
    background.loc[background[x].str.contains('R', regex=False), groupby_dict[x]] = np.nan
    ir2.loc[ir2[x].str.contains('R', regex=False), groupby_dict[x]] = np.nan
#+END_SRC

#+BEGIN_SRC jupyter-python
ax = sns.boxplot(ir2["Lead mg/kg"].loc[~ir2.Pb.str.contains('U')])
sns.boxplot(background["Lead mg/kg"].loc[~background.Pb.str.contains('U')], ax=ax)
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f29a1d6f550>
[[file:./.ob-jupyter/42a95e418d18dadf447f7804074ca259be477b62.png]]
:END:

#+BEGIN_SRC jupyter-python
lead = ir2["Lead mg/kg"].loc[~ir2.Pb.str.contains('U')].values
lead_bg = background["Lead mg/kg"].loc[~background.Pb.str.contains('U')].values
print(len(lead), len(lead_bg))
mean_lead = np.mean(lead)
mean_lead_bg = np.mean(lead_bg)
mean_diff_obs = mean_lead - mean_lead_bg
print(mean_lead, mean_lead_bg, mean_diff_obs)
total_lead = np.concatenate([lead, lead_bg])
total_lead


reps = 10000
lead_size = len(lead)
lead_bg_size = len(lead_bg)
value = total_lead
diffmeans = []
#mean, sigma = np.mean(value), np.std(value, ddof=1)
for i in np.arange(1,reps+1):
    sample = np.random.choice(value, lead_size, replace=True)
    bgsample = np.random.choice(value, lead_bg_size, replace=True)
    stat = np.mean(sample)
    bgstat = np.mean(bgsample)
    diffmeans.append(stat-bgstat)
ordered = np.sort(diffmeans)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
print(f"upper= {upper}")
ax = sns.distplot(diffmeans, label=f"Bootstrap means,\n n = {reps:,}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.axvline(x=lower,
          color='k',
          label=f"LCL95 \n ({lower:0.02})")
ax.axvline(x=upper,
          color='k',
          label=f"UCL95 \n ({upper:0.02})")
ax.axvline(x=mean_diff_obs,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean_diff_obs:0.3}")
ax.set_title("Bootstrap of Difference of Means")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: 40 29
: 13.855 9.934137931034483 3.9208620689655174
: upper= 3.7607370689655166
: <matplotlib.legend.Legend at 0x7f29a1e4c150>
[[file:./.ob-jupyter/bd6c0b567de8803e2b4e305cbcb80bef468cbd77.png]]
:END:

#+END_SRC

#+RESULTS:
:RESULTS:
: 40 29
: 13.855 9.934137931034483 3.9208620689655174
: array([ 6.1 ,  8.9 , 10.  ,  8.7 ,  9.  ,  4.8 ,  7.5 , 11.  , 14.  ,
:        16.  , 11.  ,  9.5 ,  9.3 , 12.  , 15.  , 11.  ,  6.3 ,  8.3 ,
:         5.6 , 18.  , 20.  , 19.  , 14.  , 17.  , 14.  , 19.  , 28.  ,
:        39.  , 24.  , 14.  , 22.  , 27.  , 13.  , 11.  ,  6.9 , 15.  ,
:        17.  ,  7.3 , 12.  , 13.  ,  8.18, 28.  ,  5.7 ,  4.7 ,  4.1 ,
:         9.  ,  2.2 , 16.7 ,  9.48,  5.75,  8.85,  4.2 ,  8.8 ,  4.2 ,
:         8.3 , 11.  , 50.7 , 15.1 , 11.3 ,  3.4 ,  8.  ,  8.5 ,  7.3 ,
:         5.7 ,  4.4 , 10.3 ,  6.6 ,  6.13, 11.5 ])
:END:
**
**
**
**
**
#+BEGIN_SRC jupyter-python
from numpy.random import RandomState
from permute.core import two_sample
x = lead
y = lead_bg
# we only care if lead is greater than the background
p, stat, dist = two_sample(x, y, reps=10**5, stat='mean', alternative="greater",
               keep_dist=True, seed=None, plus1=True)
print(p, stat)
ax = sns.distplot(dist, label=f"Null Distribution, \n n = 10^5")
ordered = np.sort(dist)
upper = np.percentile(ordered, 95, interpolation='midpoint')
print(f"upper= {upper}")
ax.axvline(x=upper,
          color='k',
          label=f"UCL95 \n ({upper:0.02})")
ax.axvline(x=mean_diff_obs,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean_diff_obs:0.3}")
ax.set_title(f"Bootstrap of Difference of Means, p-val:{p:0.3}")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.019379806201937983 3.9208620689655174
: upper= 3.2148017241379288
: <matplotlib.legend.Legend at 0x7f29a1faa450>
[[file:./.ob-jupyter/e92bc1e8142ccecf7d262950afa2a209cb1c18f2.png]]
:END:


#+BEGIN_SRC jupyter-python :var nh4=nh4
nh4 = pd.DataFrame(nh4, columns=["nh4", "landuse"])
nh4

from numpy.random import RandomState
from permute.core import two_sample
x = nh4.nh4.loc[nh4.landuse=='indust'].values
y = nh4.nh4.loc[nh4.landuse=='residen'].values
mean_diff_obs = np.mean(x) - np.mean(y)
print(mean_diff_obs)
# we only care if lead is greater than the background
p, stat, dist = two_sample(x, y, reps=10**5, stat='mean', alternative="two-sided",
               keep_dist=True, seed=None, plus1=True)
print(p, stat)
ax = sns.distplot(dist, label=f"Null Distribution, \n n = 10^5")
ordered = np.sort(dist)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
ax.hlines(y=0,
          xmin=lower,
          xmax=upper,
          color='r',
          lw=2,
          label=f"95%CI \n ({lower:0.02}, {upper:0.02})")
ax.axvline(x=mean_diff_obs,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean_diff_obs:0.3}")
ax.set_title(f"Bootstrap of Difference of Means, p-val:{p:0.3}")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: 0.028000000000000025
: 0.992370076299237 0.028000000000000025
: <matplotlib.legend.Legend at 0x7f29a164bc10>
[[file:./.ob-jupyter/1e120823f29775d1508fde4782ed6ea08a4d050b.png]]
:END:

#+END_SRC

Wilcoxon Rank-Sum Test
H0: Prob[x>y] = 0.5
or H0: median(x)=median(y)

#+BEGIN_SRC jupyter-python

from scipy.stats import mannwhitneyu
w,p = wilcoxon(x, y, alternative='two-sided')
print(w,p)

mannwhitneyu(x,y, alternative='two-sided')
#+END_SRC

#+RESULTS:
:RESULTS:
: 10.0 0.07446183141740546
: MannwhitneyuResult(statistic=76.5, pvalue=0.04910997473143368)
:END:
#+BEGIN_SRC jupyter-python :var moly=moly
mo = pd.DataFrame(moly, columns=["mo","locat"])
x = mo.mo.loc[mo.locat=='DOWNGRAD'].values
y = mo.mo.loc[mo.locat=='upgrad'].values
stat, p = mannwhitneyu(x, y, alternative='two-sided')
ax = sns.boxplot(x=mo.locat, y=mo.mo, palette='tab20')
ax.set_title(f"Molybdenum Comparison, p-val={p:.03}")
#+END_SRC

#+RESULTS:
:RESULTS:
: Text(0.5, 1.0, 'Molybdenum Comparison, p-val=0.0104')
[[file:./.ob-jupyter/29b8084211c7f9d198838c5277c2ce184d5dafcc.png]]
:END:

#+BEGIN_SRC jupyter-python :var nh4=nh4
from numpy.random import RandomState
from permute.core import two_sample
mean_diff_obs = np.mean(x) - np.mean(y)
print(mean_diff_obs)
# we only care if lead is greater than the background
p, stat, dist = two_sample(x, y, reps=10**5, stat='mean', alternative="less",
               keep_dist=True, seed=None, plus1=True)
print(p, stat)
ax = sns.distplot(dist, label=f"Null Distribution, \n n = 10^5")
ordered = np.sort(dist)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
ax.hlines(y=0,
          xmin=lower,
          xmax=upper,
          color='r',
          lw=2,
          label=f"95%CI \n ({lower:0.02}, {upper:0.02})")
ax.axvline(x=mean_diff_obs,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean_diff_obs:0.3}")
ax.set_title(f"Bootstrap of Difference of Means, p-val:{p:0.3}")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: -3.685948717948718
: 9.999900000945416e-06 -3.685948717948718
: <matplotlib.legend.Legend at 0x7f29a10b6d50>
[[file:./.ob-jupyter/6b8375aa3f48d017bbc2820294d30226496f85c3.png]]
:END:
#+BEGIN_SRC jupyter-python
mo

#+END_SRC

#+RESULTS:
|    |    mo | locat    |
|----+-------+----------|
|  0 |  0.85 | DOWNGRAD |
|  1 |  0.39 | DOWNGRAD |
|  2 |  0.32 | DOWNGRAD |
|  3 |   0.3 | DOWNGRAD |
|  4 |   0.3 | DOWNGRAD |
|  5 | 0.205 | DOWNGRAD |
|  6 |   0.2 | DOWNGRAD |
|  7 |   0.2 | DOWNGRAD |
|  8 |  0.14 | DOWNGRAD |
|  9 |  0.14 | DOWNGRAD |
| 10 |  0.09 | DOWNGRAD |
| 11 | 0.046 | DOWNGRAD |
| 12 | 0.035 | DOWNGRAD |
| 13 |   6.9 | upgrad   |
| 14 |   3.2 | upgrad   |
| 15 |   1.7 | upgrad   |
** PYTHON -Exercises Section 5 - Testing Differences between two independent groups
Dissolved oxygen (DO) was measured in a river/estuary in Florida over several
years. In 2008 a change was made upstream that was hoped to increase DO
conditions somewhat. However, the uncertainty in the effect was enough to test
also for a decrease in DO – the scientist did not want to ignore a decrease if it was
observed (a one-sided test for increased DO would ignore any decrease). Therefore,
run a two-sided test for whether DO has changed.

*** Data
#+TBLNAME:dofl
|   DO | range    |       date |     time |  dectime |
|------+----------+------------+----------+----------|
|  5.9 | Pre2008  | 2000-02-07 | 23:00:00 | 2000.110 |
|  6.9 | Pre2008  | 2000-03-06 | 23:00:00 | 2000.184 |
|  7.1 | Pre2008  | 2000-04-09 | 23:00:00 | 2000.277 |
|  7.4 | Pre2008  | 2000-04-19 | 23:00:00 | 2000.304 |
|  6.6 | Pre2008  | 2000-04-30 | 23:00:00 | 2000.334 |
|  5.7 | Pre2008  | 2000-07-30 | 23:00:00 | 2000.584 |
|  7.1 | Pre2008  | 2000-08-07 | 23:00:00 | 2000.605 |
|  5.7 | Pre2008  | 2000-08-22 | 23:00:00 | 2000.647 |
|  5.8 | Pre2008  | 2000-09-19 | 23:00:00 | 2000.723 |
|  5.4 | Pre2008  | 2000-09-25 | 23:00:00 | 2000.740 |
|  6.1 | Pre2008  | 2000-10-03 | 23:00:00 | 2000.762 |
|  7.5 | Pre2008  | 2000-10-15 | 23:00:00 | 2000.795 |
|  7.3 | Pre2008  | 2000-11-05 | 23:00:00 | 2000.852 |
|  6.4 | Pre2008  | 2000-11-14 | 23:00:00 | 2000.877 |
|  7.1 | Pre2008  | 2000-12-03 | 23:00:00 | 2000.929 |
|  6.6 | Pre2008  | 2000-12-13 | 23:00:00 | 2000.956 |
|  7.2 | Pre2008  | 2001-01-08 | 23:00:00 | 2001.027 |
|  6.4 | Pre2008  | 2001-02-04 | 23:00:00 | 2001.101 |
|  6.5 | Pre2008  | 2001-03-12 | 23:00:00 | 2001.197 |
|  4.9 | Pre2008  | 2001-04-08 | 23:00:00 | 2001.271 |
|  6.7 | Pre2008  | 2001-05-07 | 23:00:00 | 2001.351 |
|  8.1 | Pre2008  | 2001-06-20 | 23:00:00 | 2001.471 |
|  6.5 | Pre2008  | 2001-06-24 | 23:00:00 | 2001.482 |
|  6.6 | Pre2008  | 2001-07-01 | 23:00:00 | 2001.501 |
|  6.2 | Pre2008  | 2001-07-09 | 23:00:00 | 2001.523 |
|  5.3 | Pre2008  | 2001-07-30 | 23:00:00 | 2001.581 |
|  7.1 | Pre2008  | 2001-08-26 | 23:00:00 | 2001.655 |
|  5.9 | Pre2008  | 2002-03-26 | 23:00:00 | 2002.236 |
|  5.7 | Pre2008  | 2002-05-20 | 23:00:00 | 2002.386 |
|  5.8 | Pre2008  | 2002-05-28 | 23:00:00 | 2002.408 |
|  5.4 | Pre2008  | 2002-06-04 | 23:00:00 | 2002.427 |
|  7.1 | Pre2008  | 2002-10-28 | 23:00:00 | 2002.827 |
|  5.4 | Pre2008  | 2002-12-03 | 23:00:00 | 2002.926 |
|  6.2 | Pre2008  | 2003-02-05 | 23:00:00 | 2003.104 |
|  5.9 | Pre2008  | 2003-02-11 | 23:00:00 | 2003.121 |
|  5.7 | Pre2008  | 2003-04-07 | 23:00:00 | 2003.268 |
|  5.4 | Pre2008  | 2003-04-22 | 23:00:00 | 2003.310 |
|  6.1 | Pre2008  | 2003-05-05 | 23:00:00 | 2003.345 |
|  7.5 | Pre2008  | 2003-05-11 | 23:00:00 | 2003.362 |
|  5.4 | Pre2008  | 2003-07-23 | 23:00:00 | 2003.562 |
|  5.4 | Pre2008  | 2003-08-05 | 23:00:00 | 2003.597 |
|  5.9 | Pre2008  | 2003-08-27 | 23:00:00 | 2003.658 |
|  5.7 | Pre2008  | 2003-09-09 | 23:00:00 | 2003.693 |
|  5.4 | Pre2008  | 2003-10-27 | 23:00:00 | 2003.825 |
|  6.1 | Pre2008  | 2003-12-15 | 23:00:00 | 2003.959 |
|  7.5 | Pre2008  | 2003-12-21 | 23:00:00 | 2003.975 |
|  7.7 | Pre2008  | 2004-01-07 | 23:00:00 | 2004.025 |
|  6.1 | Pre2008  | 2004-04-14 | 23:00:00 | 2004.290 |
|  7.6 | Pre2008  | 2004-04-27 | 23:00:00 | 2004.326 |
|  7.4 | Pre2008  | 2004-05-10 | 23:00:00 | 2004.362 |
|  8.4 | Pre2008  | 2004-05-17 | 23:00:00 | 2004.381 |
|  6.4 | Pre2008  | 2004-06-02 | 23:00:00 | 2004.425 |
|  5.4 | Pre2008  | 2004-06-14 | 23:00:00 | 2004.458 |
|  4.3 | Pre2008  | 2004-06-20 | 23:00:00 | 2004.474 |
|  4.5 | Pre2008  | 2004-06-21 | 23:00:00 | 2004.477 |
|  5.7 | Pre2008  | 2004-10-12 | 23:00:00 | 2004.786 |
|  7.1 | Pre2008  | 2005-03-29 | 23:00:00 | 2005.244 |
|  4.4 | Pre2008  | 2006-06-18 | 23:00:00 | 2006.466 |
|  5.1 | Pre2008  | 2006-07-26 | 23:00:00 | 2006.570 |
|  6.0 | Pre2008  | 2006-08-15 | 23:00:00 | 2006.625 |
|  4.2 | Pre2008  | 2006-08-22 | 23:00:00 | 2006.644 |
|  7.8 | Pre2008  | 2007-02-20 | 23:00:00 | 2007.145 |
|  6.2 | Pre2008  | 2007-03-12 | 23:00:00 | 2007.197 |
|  6.9 | Pre2008  | 2007-04-22 | 23:00:00 | 2007.310 |
|  8.9 | Post2008 | 2009-03-03 | 23:00:00 | 2009.173 |
| 10.0 | Post2008 | 2009-05-06 | 23:00:00 | 2009.348 |
|  3.1 | Post2008 | 2009-07-08 | 23:00:00 | 2009.521 |
|  4.0 | Post2008 | 2009-07-13 | 23:00:00 | 2009.534 |
|  4.2 | Post2008 | 2009-08-30 | 23:00:00 | 2009.666 |
|  5.6 | Post2008 | 2009-09-14 | 23:00:00 | 2009.707 |
|  7.4 | Post2008 | 2009-12-02 | 23:00:00 | 2009.923 |
|  9.3 | Post2008 | 2009-12-07 | 23:00:00 | 2009.937 |
| 10.5 | Post2008 | 2010-01-11 | 23:00:00 | 2010.036 |
|  7.8 | Post2008 | 2010-02-15 | 23:00:00 | 2010.132 |
|  5.9 | Post2008 | 2010-03-07 | 23:00:00 | 2010.184 |
|  5.5 | Post2008 | 2010-07-06 | 23:00:00 | 2010.515 |
|  8.8 | Post2008 | 2010-08-03 | 23:00:00 | 2010.592 |
|  9.2 | Post2008 | 2010-08-25 | 23:00:00 | 2010.652 |
|  3.8 | Post2008 | 2010-08-27 | 23:00:00 | 2010.658 |
|  4.6 | Post2008 | 2010-09-26 | 23:00:00 | 2010.740 |
|  4.5 | Post2008 | 2010-10-13 | 23:00:00 | 2010.786 |
|  5.6 | Post2008 | 2010-11-21 | 23:00:00 | 2010.893 |
|  5.9 | Post2008 | 2010-12-19 | 23:00:00 | 2010.970 |
| 10.1 | Post2008 | 2010-12-20 | 23:00:00 | 2010.973 |
|  7.0 | Post2008 | 2011-01-18 | 23:00:00 | 2011.055 |
|  6.0 | Post2008 | 2011-01-23 | 23:00:00 | 2011.068 |
|  5.2 | Post2008 | 2011-04-03 | 23:00:00 | 2011.258 |
| 17.1 | Post2008 | 2011-07-10 | 23:00:00 | 2011.526 |
|  4.4 | Post2008 | 2011-07-18 | 23:00:00 | 2011.548 |
|  4.7 | Post2008 | 2011-09-12 | 23:00:00 | 2011.701 |
|  5.8 | Post2008 | 2011-10-03 | 23:00:00 | 2011.759 |
|  5.7 | Post2008 | 2011-12-13 | 23:00:00 | 2011.953 |
|  9.5 | Post2008 | 2012-06-10 | 23:00:00 | 2012.447 |
| 11.5 | Post2008 | 2012-06-27 | 23:00:00 | 2012.493 |
|  6.8 | Post2008 | 2012-08-06 | 23:00:00 | 2012.603 |
|  9.0 | Post2008 | 2012-11-13 | 23:00:00 | 2012.874 |
| 10.3 | Post2008 | 2013-03-19 | 23:00:00 | 2013.216 |
|  8.3 | Post2008 | 2013-04-07 | 23:00:00 | 2013.268 |
|  9.4 | Post2008 | 2013-07-07 | 23:00:00 | 2013.518 |
|  6.7 | Post2008 | 2013-11-18 | 23:00:00 | 2013.885 |
*** Analysis
1. Plot the Data

#+BEGIN_SRC jupyter-python :var dofl=dofl
do = pd.DataFrame(dofl, columns=["DO","range","date","time","dectime"])
ax = sns.boxplot(x=do.range, y=do.DO, palette="Set3")
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/ffea66a566554eae1750eb64f207e76aea28266a.png]]

2. Calculate t-Test
   #+BEGIN_SRC jupyter-python
from scipy.stats import ttest_ind, shapiro
a = do.DO.loc[do.range=='Pre2008'].values
b = do.DO.loc[do.range=='Post2008'].values
W1, pvalue1 = shapiro(a)
W2, pvalue2 = shapiro(b)
stat, p = ttest_ind(a,b)
data = {'Data':['pre2008','post2008'],
        'Shapiro':[pvalue1,pvalue2]}
df = pd.DataFrame(data, columns=['Data', 'Shapiro'])
print(f"t-Test results indicate a p-value of {p:0.03}")
df
#+END_SRC

#+RESULTS:
:RESULTS:
: t-Test results indicate a p-value of 0.00966
|    | Data     |    Shapiro |
|----+----------+------------|
|  0 | pre2008  | 0.443143   |
|  1 | post2008 | 0.00694626 |
:END:

There is probably little comparative power of this t-test because we reject H0
that the post2008 data are from a normal distribution. But the t-test signifies
that the true difference in means in not equal to 0 with 95 percent confidence.

3. Run a wilcoxon rank-sum test to determine whether the median has changed.

   #+BEGIN_SRC jupyter-python
stat, p = mannwhitneyu(a,b, alternative='two-sided')
p
   #+END_SRC

   #+RESULTS:
   : 0.20722448662990545
The result is greater than 0.05, so we cannot reject H0 that the median is the
same. There is not a significant difference in the medians of the two groups.
Therefore the frequency of high concentrations doesn't change, thought it does
appear that the magnitude does.

4. Run the 2-sample permutation test to see if results differ from the t-test (it is a
main subject of this course, after all; we could have just left the t-test off
the list).


#+BEGIN_SRC jupyter-python :var nh4=nh4
from numpy.random import RandomState
from permute.core import two_sample
mean_diff_obs = np.mean(a) - np.mean(b)
print(mean_diff_obs)
# we only care if lead is greater than the background
p, stat, dist = two_sample(a, b, reps=10**5, stat='mean', alternative="two-sided",
               keep_dist=True, seed=None, plus1=True)
print(p, stat)
ax = sns.distplot(dist, label=f"Null Distribution, \n n = 10^5")
ordered = np.sort(dist)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
ax.hlines(y=0,
          xmin=lower,
          xmax=upper,
          color='r',
          lw=2,
          label=f"95%CI \n ({lower:0.02}, {upper:0.02})")
ax.axvline(x=mean_diff_obs,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean_diff_obs:0.3}")
ax.set_title(f"Bootstrap of Difference of Means, p-val:{p:0.3}")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: -1.0180555555555566
: 0.010399896001040076 -1.0180555555555566
: <matplotlib.legend.Legend at 0x7f299e000210>
[[file:./.ob-jupyter/202a83f9069aaa14f2211ad416c4a40256c8c334.png]]
:END:
p-value is similar to t-Test. The true difference in means is not equal to 0.

5. If the interest is in whether DO is typically higher now than it was prior to the
change upstream, what is your answer?

We just need to do a one-sided wilcoxon test.

#+BEGIN_SRC jupyter-python
stat, p = mannwhitneyu(a,b, alternative='less')
p
#+END_SRC

#+RESULTS:
: 0.10361224331495272
We cannot reject H0 here either that the medians are the same.

6. If the interest is in whether the cumulative amount of oxygen over time is higher
now than it was prior to the change upstream, what is your answer?

Then we are interested in the difference in means. The p-value is less than 0.05
in this case from our permutation test, and we reject that they come from the
same population. Pre2008 DO is generally lower than Post2008 DO.

7. What is the primary difference in DO between the two time periods?
   The medians are similar, but there is a greater spread in data in post2008
   which skews the mean.
Changes in variances can obscure/cause a change in mean. This is called the
Behrens-Fisher problem in statistics. It is an issue when testing for differences in means,
whether that is done with a t-test or a permutation test. Here if the question is whether the
frequency of high DO increases after 2008, the wilcoxon test shows that it does not – the
percentiles for DO are generally not greater after 2008.
** Comparing Three or More Groups of Data.
Parametric: ANOVA (two-way ANOVA)
Permutation: perm1way, perm.fact.test
Nonparametric: Kruskal-Wallis, BDM test


*** Data
#+TBLNAME:iron
|           fe | mining    | rocktype  | group   |
|--------------+-----------+-----------+---------|
|   0.19999999 | Unmined   | limestone | lime_un |
|   0.24999997 | Unmined   | limestone | lime_un |
|   0.04000000 | Unmined   | limestone | lime_un |
|   0.05999999 | Unmined   | limestone | lime_un |
|   1.19999981 | Unmined   | limestone | lime_un |
|   0.31999999 | Unmined   | limestone | lime_un |
|   0.05000000 | Unmined   | limestone | lime_un |
|   0.34999996 | Unmined   | limestone | lime_un |
|   0.26999998 | Unmined   | limestone | lime_un |
|   0.50999999 | Unmined   | limestone | lime_un |
|   0.40999997 | Unmined   | limestone | lime_un |
|   0.19999999 | Unmined   | limestone | lime_un |
|   0.04000000 | Unmined   | limestone | lime_un |
|   0.48999995 | Unmined   | sandstone | sand_un |
|   2.99999952 | Unmined   | sandstone | sand_un |
|   0.05000000 | Unmined   | sandstone | sand_un |
|   0.94999993 | Unmined   | sandstone | sand_un |
|   0.83999991 | Unmined   | sandstone | sand_un |
|   0.79999995 | Unmined   | sandstone | sand_un |
|   0.38999999 | Unmined   | sandstone | sand_un |
|   1.09999991 | Unmined   | sandstone | sand_un |
|   2.79999971 | Unmined   | sandstone | sand_un |
|   0.65999997 | Unmined   | sandstone | sand_un |
|   0.70999992 | Unmined   | sandstone | sand_un |
|   0.59999990 | Unmined   | sandstone | sand_un |
|   3.19999981 | Unmined   | sandstone | sand_un |
|   1.59999991 | Abandoned | limestone | lime_ab |
|   0.31999999 | Abandoned | limestone | lime_ab |
|   5.59999943 | Abandoned | limestone | lime_ab |
|   0.86999989 | Abandoned | limestone | lime_ab |
|   2.29999971 | Abandoned | limestone | lime_ab |
|   3.19999981 | Abandoned | limestone | lime_ab |
|   6.59999943 | Abandoned | limestone | lime_ab |
|   9.89999962 | Abandoned | limestone | lime_ab |
|   0.51999998 | Abandoned | limestone | lime_ab |
|  22.99999619 | Abandoned | limestone | lime_ab |
|   1.29999995 | Abandoned | limestone | lime_ab |
|   5.99999905 | Abandoned | limestone | lime_ab |
|   5.99999905 | Abandoned | limestone | lime_ab |
| 699.99987790 | Abandoned | sandstone | sand_ab |
|  24.99999619 | Abandoned | sandstone | sand_ab |
|   1.49999976 | Abandoned | sandstone | sand_ab |
|   4.79999924 | Abandoned | sandstone | sand_ab |
|   0.41999996 | Abandoned | sandstone | sand_ab |
| 139.99996950 | Abandoned | sandstone | sand_ab |
|   1.09999991 | Abandoned | sandstone | sand_ab |
|   1.89999986 | Abandoned | sandstone | sand_ab |
|   0.39999998 | Abandoned | sandstone | sand_ab |
| 219.99996950 | Abandoned | sandstone | sand_ab |
|   0.30999994 | Abandoned | sandstone | sand_ab |
|   4.59999943 | Abandoned | sandstone | sand_ab |
|   1.89999986 | Abandoned | sandstone | sand_ab |
|   0.32999998 | Reclaimed | limestone | lime_re |
|   3.19999981 | Reclaimed | limestone | lime_re |
|   0.44000000 | Reclaimed | limestone | lime_re |
|   4.49999905 | Reclaimed | limestone | lime_re |
|   0.32999998 | Reclaimed | limestone | lime_re |
|   0.38000000 | Reclaimed | limestone | lime_re |
|   1.29999995 | Reclaimed | limestone | lime_re |
|   1.19999981 | Reclaimed | limestone | lime_re |
|   0.27999997 | Reclaimed | limestone | lime_re |
|   0.27999997 | Reclaimed | limestone | lime_re |
|   0.89999998 | Reclaimed | limestone | lime_re |
|   0.08999999 | Reclaimed | limestone | lime_re |
|   0.22000000 | Reclaimed | limestone | lime_re |
|   1.79999995 | Reclaimed | sandstone | sand_re |
|  18.99999619 | Reclaimed | sandstone | sand_re |
|   8.69999886 | Reclaimed | sandstone | sand_re |
|   0.86999989 | Reclaimed | sandstone | sand_re |
|   0.93999994 | Reclaimed | sandstone | sand_re |
|   0.92999995 | Reclaimed | sandstone | sand_re |
|   1.49999976 | Reclaimed | sandstone | sand_re |
|   1.29999995 | Reclaimed | sandstone | sand_re |
|  11.99999809 | Reclaimed | sandstone | sand_re |
|   0.59999990 | Reclaimed | sandstone | sand_re |
|  13.99999809 | Reclaimed | sandstone | sand_re |
|   0.80999994 | Reclaimed | sandstone | sand_re |
|   0.97999990 | Reclaimed | sandstone | sand_re |
|--------------+-----------+-----------+---------|
#+BEGIN_SRC jupyter-python :var iron=iron
iron=pd.DataFrame(iron, columns=["fe","mining", "rocktype", "group"])
iron.drop(iron.index[0],inplace=True)
iron.fe = iron.fe.apply(pd.to_numeric, errors='coerce')
iron
#+END_SRC

#+RESULTS:
|    |   fe | mining    | rocktype  | group   |
|----+------+-----------+-----------+---------|
|  1 |  0.2 | Unmined   | limestone | lime_un |
|  2 | 0.25 | Unmined   | limestone | lime_un |
|  3 | 0.04 | Unmined   | limestone | lime_un |
|  4 | 0.06 | Unmined   | limestone | lime_un |
|  5 |  1.2 | Unmined   | limestone | lime_un |
|  6 | 0.32 | Unmined   | limestone | lime_un |
|  7 | 0.05 | Unmined   | limestone | lime_un |
|  8 | 0.35 | Unmined   | limestone | lime_un |
|  9 | 0.27 | Unmined   | limestone | lime_un |
| 10 | 0.51 | Unmined   | limestone | lime_un |
| 11 | 0.41 | Unmined   | limestone | lime_un |
| 12 |  0.2 | Unmined   | limestone | lime_un |
| 13 | 0.04 | Unmined   | limestone | lime_un |
| 14 | 0.49 | Unmined   | sandstone | sand_un |
| 15 |    3 | Unmined   | sandstone | sand_un |
| 16 | 0.05 | Unmined   | sandstone | sand_un |
| 17 | 0.95 | Unmined   | sandstone | sand_un |
| 18 | 0.84 | Unmined   | sandstone | sand_un |
| 19 |  0.8 | Unmined   | sandstone | sand_un |
| 20 | 0.39 | Unmined   | sandstone | sand_un |
| 21 |  1.1 | Unmined   | sandstone | sand_un |
| 22 |  2.8 | Unmined   | sandstone | sand_un |
| 23 | 0.66 | Unmined   | sandstone | sand_un |
| 24 | 0.71 | Unmined   | sandstone | sand_un |
| 25 |  0.6 | Unmined   | sandstone | sand_un |
| 26 |  3.2 | Unmined   | sandstone | sand_un |
| 27 |  1.6 | Abandoned | limestone | lime_ab |
| 28 | 0.32 | Abandoned | limestone | lime_ab |
| 29 |  5.6 | Abandoned | limestone | lime_ab |
| 30 | 0.87 | Abandoned | limestone | lime_ab |
| 31 |  2.3 | Abandoned | limestone | lime_ab |
| 32 |  3.2 | Abandoned | limestone | lime_ab |
| 33 |  6.6 | Abandoned | limestone | lime_ab |
| 34 |  9.9 | Abandoned | limestone | lime_ab |
| 35 | 0.52 | Abandoned | limestone | lime_ab |
| 36 |   23 | Abandoned | limestone | lime_ab |
| 37 |  1.3 | Abandoned | limestone | lime_ab |
| 38 |    6 | Abandoned | limestone | lime_ab |
| 39 |    6 | Abandoned | limestone | lime_ab |
| 40 |  700 | Abandoned | sandstone | sand_ab |
| 41 |   25 | Abandoned | sandstone | sand_ab |
| 42 |  1.5 | Abandoned | sandstone | sand_ab |
| 43 |  4.8 | Abandoned | sandstone | sand_ab |
| 44 | 0.42 | Abandoned | sandstone | sand_ab |
| 45 |  140 | Abandoned | sandstone | sand_ab |
| 46 |  1.1 | Abandoned | sandstone | sand_ab |
| 47 |  1.9 | Abandoned | sandstone | sand_ab |
| 48 |  0.4 | Abandoned | sandstone | sand_ab |
| 49 |  220 | Abandoned | sandstone | sand_ab |
| 50 | 0.31 | Abandoned | sandstone | sand_ab |
| 51 |  4.6 | Abandoned | sandstone | sand_ab |
| 52 |  1.9 | Abandoned | sandstone | sand_ab |
| 53 | 0.33 | Reclaimed | limestone | lime_re |
| 54 |  3.2 | Reclaimed | limestone | lime_re |
| 55 | 0.44 | Reclaimed | limestone | lime_re |
| 56 |  4.5 | Reclaimed | limestone | lime_re |
| 57 | 0.33 | Reclaimed | limestone | lime_re |
| 58 | 0.38 | Reclaimed | limestone | lime_re |
| 59 |  1.3 | Reclaimed | limestone | lime_re |
| 60 |  1.2 | Reclaimed | limestone | lime_re |
| 61 | 0.28 | Reclaimed | limestone | lime_re |
| 62 | 0.28 | Reclaimed | limestone | lime_re |
| 63 |  0.9 | Reclaimed | limestone | lime_re |
| 64 | 0.09 | Reclaimed | limestone | lime_re |
| 65 | 0.22 | Reclaimed | limestone | lime_re |
| 66 |  1.8 | Reclaimed | sandstone | sand_re |
| 67 |   19 | Reclaimed | sandstone | sand_re |
| 68 |  8.7 | Reclaimed | sandstone | sand_re |
| 69 | 0.87 | Reclaimed | sandstone | sand_re |
| 70 | 0.94 | Reclaimed | sandstone | sand_re |
| 71 | 0.93 | Reclaimed | sandstone | sand_re |
| 72 |  1.5 | Reclaimed | sandstone | sand_re |
| 73 |  1.3 | Reclaimed | sandstone | sand_re |
| 74 |   12 | Reclaimed | sandstone | sand_re |
| 75 |  0.6 | Reclaimed | sandstone | sand_re |
| 76 |   14 | Reclaimed | sandstone | sand_re |
| 77 | 0.81 | Reclaimed | sandstone | sand_re |
| 78 | 0.98 | Reclaimed | sandstone | sand_re |

*** Analysis
#+BEGIN_SRC jupyter-python
ax=sns.boxplot(x=iron.mining.values, y=iron.fe.values, palette="Accent")
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/697f1b097e27bf7b712a121b26d870879d403ee3.png]]
*** ANOVA
Assumptions
1. data in each group is in normal distribution
   residual (e) = observation - its group mean
2. each group has same variance ("homoscedasticity")

   H0: all means are equal
   H1: at least one group mean differs (always 2-sided test)
   Anova does not tell which group means differ from the others.

Testing differences between means with variances?
- *Total sum of squares SS* = sum of squared deviations about the overall mean -- the
  measure of overall variation in the data broken into 2 components:
  -treatment sum of squares = how much the group means vary around the overall
  mean of the dataset -- the measure of signal. "treatment"="factor"=the
  collection of groups
 - *Error sum of squares* = the variability in the data around its group mean. The
   overall variability of the residuals--the measure of noise.

 To perform the test, first standardize the two sums of squares to the same
  units (called a '*mean square' MS*) and compute the magnitude of the signal
  (variance of group means) divided by the magnitude of the noise (variance of
  data within groups)

  MS = SS/df , degrees freedom

  F-test, ANOVA test is a signal to noise ratio. The treatment MS is the signal.
  The error MS is the noise.

  F = MS treatment/MS error

  H0 is true signifies means aren't significantly different, F will be around 1
*** F-test Python

#+BEGIN_SRC jupyter-python
iron.mining.unique()

#+END_SRC

#+RESULTS:
: array(['Unmined', 'Abandoned', 'Reclaimed'], dtype=object)

#+BEGIN_SRC jupyter-python
from scipy.stats import f_oneway
a = iron.fe[iron.mining=='Unmined'].values
b = iron.fe[iron.mining=='Abandoned'].values
c = iron.fe[iron.mining=='Reclaimed'].values
F, p = f_oneway(a, b, c)
F,p

#+END_SRC

#+RESULTS:
| 2.385304988270449 | 0.09901207027414509 |
#+BEGIN_SRC jupyter-python
iron.groupby('mining').describe(include='float64').unstack()
#+END_SRC

#+RESULTS:
#+begin_example
           mining
fe  count  Abandoned     26.000000
           Reclaimed     26.000000
           Unmined       26.000000
    mean   Abandoned     44.966916
           Reclaimed      2.956923
           Unmined        0.749615
    std    Abandoned    142.394150
           Reclaimed      4.883534
           Unmined        0.893091
    min    Abandoned      0.310000
           Reclaimed      0.090000
           Unmined        0.040000
    25%    Abandoned      1.150000
           Reclaimed      0.395000
           Unmined        0.212500
    50%    Abandoned      2.750000
           Reclaimed      0.935000
           Unmined        0.450000
    75%    Abandoned      6.449999
           Reclaimed      1.725000
           Unmined        0.830000
    max    Abandoned    699.999878
           Reclaimed     18.999996
           Unmined        3.200000
dtype: float64
#+end_example
The non-normality is pushing up the p-values. The means are clearly very
different, but ANOVA F-test isn't rejecting H0. It's not normal, so permutation
test is better.

Welch's adjustment on t-test will account for heteroscedascity, non-homogenous
variance.

AFTER ANOVA: which means are different?
only perform after ANOVA shows there are differences between groups.

A > B alpha=0.05
B = C alpha=0.05  individual error rates
A > C alpha=0.05

A > B|C alpha=0.14 family error rate

$\alpha_{family} = 1 - (1 - \alpha_{pairwise})^c$
$c = k(k-1)/2$

Tukeys HSD test has the most power.
https://www.statsmodels.org/stable/generated/statsmodels.stats.multicomp.pairwise_tukeyhsd.html
*** Permutation Tests for One Factor
H0: all means are equal
If H0 is true the data could be randomly reassigned to any groups

"Shuffle" the factor name (or the response values) many times.

Compute an F-ratio or similar statistic for each shuffle
The p-value equals the proportion of shuffles with a test statistic that equals
or exceeds the original observed statistic from the data.

perm1way in R

#+BEGIN_SRC jupyter-python
from sklearn.utils import shuffle
data = {'a':[1,2,3],'b':[4,5,6],'c':[7,8,9]}
test = pd.DataFrame(data, columns=['a','b','c'])
test.a = np.random.permutation(test.a.values)
test
#+END_SRC

#+RESULTS:
|   | a | b | c |
|---+---+---+---|
| 0 | 2 | 4 | 7 |
| 1 | 3 | 5 | 8 |
| 2 | 1 | 6 | 9 |
#+BEGIN_SRC jupyter-python
from scipy.stats import f_oneway
a = iron.fe[iron.mining=='Unmined'].values
b = iron.fe[iron.mining=='Abandoned'].values
c = iron.fe[iron.mining=='Reclaimed'].values
F0, p = f_oneway(a, b, c)

df = iron[['fe','mining']].copy()
reps = 5000
ff = []
for i in np.arange(1,reps+1):
    df.fe = np.random.permutation(df.fe.values)
    a = df.fe[df.mining=='Unmined'].values
    b = df.fe[df.mining=='Abandoned'].values
    c = df.fe[df.mining=='Reclaimed'].values
    F, p = f_oneway(a, b, c)
    ff.append(F)
ordered = np.sort(ff)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
ax = sns.distplot(ff,
                  color='coral',
                  label=f"Null Distribution F stats,\n n = {reps:,}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.hlines(y=0,
          xmin=lower,
          xmax=upper,
          color='r',
          label=f"95% \n ({lower:0.02},{upper:0.02})")
ax.axvline(x=F0,
           ls='--',
           color='r',
           label=f"Obs. F {F0:0.3}")
ff_exceed = [x for x in ff if x >= F0]
p = len(ff_exceed)/reps
ax.set_title(f"Permutation F-tests, p-value: {p:0.03}")
ax.legend()


#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7f299c3e9fd0>
[[file:./.ob-jupyter/45a37760cdad4cc9e77561c9009eab698a4c740b.png]]
:END:
YES!! This matches perm1way (permKS)
These means are not equal.


*** Kruskal-Wallis
The Kruskal-Wallis H-test tests the null hypothesis that the population median
of all of the groups are equal. It is a non-parametric version of ANOVA. The
test works on 2 or more independent samples, which may have different sizes.
Note that rejecting the null hypothesis does not indicate which of the groups
differs. Post hoc comparisons between groups are required to determine which
groups are different.

H0: medians all the same (data really all 1 group)
H1: medians are not all the same (2-sided test)

#+BEGIN_SRC jupyter-python
from scipy.stats import kruskal
a = iron.fe[iron.mining=='Unmined'].values
b = iron.fe[iron.mining=='Abandoned'].values
c = iron.fe[iron.mining=='Reclaimed'].values
H,p = kruskal(a,b,c)
H,p
#+END_SRC

#+RESULTS:
| 21.298592853931947 | 2.3717522738296127e-05 |
These data do not have medians all the same.

Once you know there are differences between groups, must find which medians are
different. The one with the power equivalent to Tukeys is the "False Discovery
Rate", using the "BH" option.

R: pairwise.wilcox.test(x, group, "BH")

*** Summary
Difference in medians: Kruskal-Wallis
Difference in means: permutation test
*** 2-way tests
Brunner-Dette-Munk BDM.2way in asbio

*** Adjust error rate for individual tests for family error rate = 0,05

- Bonferroni test. 0.05/n, where n is number of groups
- Holm method usualy finds more tests that significant for the same objective
  (minimizw overall error rate)
- Now, False Discovery Rate--minimizing false positives, measured using the
  Benjamini-Hochberg (BH) adjustment. FDR provides a large gain in the power of
  seeing differences.


Set overall FDR to 0,05 for instance
i/(c*FDR) starting at largest p-value, where c is n(n-1)/2
to report back, must be converted to compare with family error rate.
usually presented in triangular p-value table
* Section 6: Testing Differences in Precision/Variance
Testing whether groups have equal variance.

Why?
- Do labs have the same precision?
- Is the variability in the natural system changing?
- Has human intervention (a dam) changed the distribution (of flouds, droughts)?
- Can older tests that assumed equal variance (t-test,ANOVA) be used on these data?
** Don't use the Barlett test
it's too sensitive to non-normality, and LOWERS p-values. It rejects too
frequently H0. never use for environmental data.
** Parametric: Levene's Test
computer z = |x - median|
run one-way ANOVA on z
** Nonparametric: Fligner-Killeen Test
compute score = weighted (rank |x - median|)
weighted by sample sizes in each group.

Answers the question, "Is the median distance from the center the same for all groups?"
* Section 7: Correlation
** Correlation and Theil-Sen line
Pearson's r, Spearman's rho and Kendall's tau. Their differences and when to use
them. Also the Their-Sen line, a 'linear median' that is a nonparametric,
outlier-resistant alternative to regression.

** 3 Correlation coefficients
range from -1 (perfect corr. neg. slope) to 0 (no corr.) to +1 (perfect
correlation positive slope)
** Pearson's r -- Parametric
assumes normality
measures linear correlation

H0: r = 0; Y and X move independently
H1: r != 0; Y changes linearly with X

You can test this with a t-test
t-test assumes joint normality

$\displaystyle t = \frac{r}{\frac{\sqrt{1-r^2}}{\sqrt{n-2}}$
t = signal / noise

** Spearman's rho rank correlation-- nonparametric
measure monotonic correlation

H0: rho = 0; Y and X move independently
H1: rho != 0; Y changes linearly or nonlinearly with X

Test: t-test on the ranks

** Kendall's tau -- nonparametric
measure monotonic correlation
this is preferable to spearman's rho

H0: rho = 0; Y and X move independently
H1: rho != 0; Y changes linearly or nonlinearly with X

Test: test for $\tau$

Order X's, then for each Y count the plusses and minuses

Kendall's tau can be considered the difference in probabilities of + and -

| X |  Y |      |
| 1 |  2 | ++++ |
| 2 |  4 | +++  |
| 3 |  6 | -+   |
| 4 |  5 | +    |
| 5 | 10 |      |

So P=9, M=1

$$ S = P - M = 8 $$
$$\displaystyle \tau = \frac{S}{n(n-1)/2} = 8/10 = 0.80 $$
p-value = 0.084
** Theil-Sen line
- A straight line related to Kendall's tau
- a "linear median"
- use instead of regression when there are outliers, skewed residuals

  1. compute all possible pairwise slopes (is one slope for every + or -
     comparison when computing tau)

  2. (Sen) slope = median of all possible slopes

  3. intercept = Y_med - slope*X_med

| X |  Y |             |
| 1 |  2 | +2 +2 +1 +2 |
| 2 |  4 | +2 +0.5 +2  |
| 3 |  6 | -1 +2       |
| 4 |  5 | +5          |
| 5 | 10 |             |

median of slopes is +2
tau = 0.8
p = 0.084

intercept (most common method)
 X_med = 3, Y_med = 5
 intercept = Y_med - slope * X_med
 intercept = 5 - 2*3 = -1

 Theil-Sen Line:
 Y = -1 + 2*X

https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.theilslopes.html
** PYTHON

#+TBLNAME:kendex
| X | Y1 |  Y2 |
|---+----+-----|
| 1 | 10 |  10 |
| 2 | 40 |  40 |
| 3 | 30 |  30 |
| 4 | 55 |  55 |
| 5 | 62 | 200 |
| 6 | 56 |  56 |


#+BEGIN_SRC jupyter-python
import pandas as pd
import numpy as np
import scipy.stats as stats
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
plt.style.use('seaborn-white')
#+END_SRC

#+RESULTS:
 scipy.stats.theilslopes(y, x=None, alpha=0.95)[source]

    Computes the Theil-Sen estimator for a set of points (x, y).

    theilslopes implements a method for robust linear regression. It computes the slope as the median of all slopes between paired values.

    Parameters

        y arrayy_like

            Dependent variable.
        x : array_like or None, optional

            Independent variable. If None, use arange(len(y)) instead.
        alpha : float, optional

            Confidence degree between 0 and 1. Default is 95% confidence. Note that alpha is symmetric around 0.5, i.e. both 0.1 and 0.9 are interpreted as “find the 90% confidence interval”.

    Returns

        medslope : float

            Theil slope.
        medintercept : float

            Intercept of the Theil line, as median(y) - medslope*median(x).
        lo_slope : float

            Lower bound of the confidence interval on medslope.
        up_slope : float

            Upper bound of the confidence interval on medslope.


#+BEGIN_SRC jupyter-python :var kendex=kendex
from scipy.stats import theilslopes
from scipy.stats import kendalltau
from scipy.stats import linregress

df = pd.DataFrame(kendex, columns=['x','y1','y2'])
x = df.x.values
y1 = df.y1.values
y2 = df.y2.values
alpha = 0.95
res = stats.theilslopes(y1, x, alpha=alpha)
lsq_res = stats.linregress(x,y1)
tau, p = stats.kendalltau(x,y1)
res2 = stats.theilslopes(y2, x, alpha=alpha)
lsq_res2 = stats.linregress(x,y2)
tau2, p2 = stats.kendalltau(x,y2)

fig, (ax1, ax2) = plt.subplots(1, 2)
ax1.plot(x, y1, 'b.')
ax1.plot(x, res[1] + res[0]*x, 'r-', label='Theil-Sen Line')

# lower bound CI on theilslope
ax1.plot(x, res[1] + res[2]*x, 'r--', alpha=0.5, label=f"{alpha*100}% CI")
# upper bound CI on theilslope
ax1.plot(x, res[1] + res[3]*x, 'r--', alpha=0.5)
ax1.plot(x, lsq_res[1] + lsq_res[0] * x, 'g-.', label="Least-squares regression")
ax1.set_title(f'Tau stat: {tau:0.3}, p-val: {p:0.03}')
ax1.legend()
ax2.plot(x, y2, 'b.')
ax2.plot(x, res2[1] + res2[0]*x, 'r-', label='Theil-Sen Line')

# lower bound CI on theilslope
ax2.plot(x, res2[1] + res2[2]*x, 'r--', alpha=0.5, label=f"{alpha*100}% CI")
# upper bound CI on theilslope
ax2.plot(x, res2[1] + res2[3]*x, 'r--', alpha=0.5)
ax2.plot(x, lsq_res2[1] + lsq_res2[0] * x, 'g-.', label="Least-squares regression")
ax2.set_title(f'Tau stat: {tau2:0.3}, p-val: {p2:0.03}')
ax2.legend()
plt.tight_layout()
plt.show()
#+END_SRC

#+BEGIN_SRC jupyter-python
name=['slope', 'intercept', 'lower bound CI slope', 'upper bound CI slope']
list(zip(name, np.round_(res, decimals=3)))
#+END_SRC

#+RESULTS:
| slope                |  8.667 |
| intercept            | 17.167 |
| lower bound CI slope |   -6.0 |
| upper bound CI slope |   25.0 |

(2) Compute the nonparametric regression equation for Y1 as a function of X.
That is, given an X value we would like to estimate Y1.

median(intercept) = median(y) - theil_slope*median(x)

Y1 = median(intercept) + theil_slope*(median(X))
   = 17.167 + 8.67*X

** PYTHON HANDOUT
*** Data
: (10.0, 12.5, 0.5, 53.333333333333336)

(2) Compute the nonparametric regression equation for Y1 as a function of X.
That is, given an X value we would like to estimate Y1.

a
#+TBLNAME:urband
|        TN |         DA | newDA     |
|-----------+------------+-----------|
| 13.104032 | 0.70700326 | 0.0301974 |
|  2.185009 | 0.09799978 | 0.0820850 |
|  3.270000 | 0.04699989 | 0.2231300 |
|  5.819998 | 0.25599945 | 0.6065310 |
|  3.257989 | 0.11700016 | NA        |
|  5.042994 | 0.02599997 | NA        |
| 16.933942 | 0.02999994 | NA        |
| 47.319856 | 0.44999896 | NA        |
|  1.524002 | 0.02000006 | NA        |
|  4.314019 | 0.07000000 | NA        |
|  6.600992 | 0.10999944 | NA        |
|  3.776018 | 0.02999994 | NA        |
|  1.522007 | 0.06000004 | NA        |
|  1.805992 | 0.08999960 | NA        |
|  1.657997 | 0.11900021 | NA        |
|  3.250991 | 0.86300051 | NA        |
|  4.445979 | 0.04500013 | NA        |
|  9.296984 | 0.07000000 | NA        |
|  2.617998 | 0.05099998 | NA        |
|  5.754027 | 0.09799978 | NA        |
|  2.629990 | 0.01899993 | NA        |
|  2.604993 | 0.01899993 | NA        |
| 40.125819 | 0.82999965 | NA        |
|  6.331011 | 0.15000000 | NA        |
| 10.872971 | 0.13000011 | NA        |
| 11.134963 | 0.33000087 | NA        |
|  8.245025 | 0.12000042 | NA        |
| 15.903035 | 0.47000121 | NA        |
|  4.964984 | 0.21999950 | NA        |
|  5.420998 | 0.10699962 | NA        |
|  3.619008 | 0.02899998 | NA        |
|  3.794983 | 0.04299979 | NA        |
|  2.664989 | 0.06400014 | NA        |
|  1.754002 | 0.03999983 | NA        |
|  1.326999 | 0.14000040 | NA        |
|  3.724006 | 0.29200043 | NA        |
|  6.287981 | 0.17200014 | NA        |
| 10.285036 | 0.15599989 | NA        |
| 11.050990 | 0.50599930 | NA        |
| 22.284024 | 0.30300075 | NA        |
|  3.725012 | 0.04600018 | NA        |
|  4.407995 | 0.06600004 | NA        |
*** Analysis
This one done before

#+BEGIN_SRC jupyter-python :var urbanda = urbanda
from scipy.stats import kendalltau
urb = pd.DataFrame(urbanda, columns=['TN','DA','newDA'])
urb.drop(columns='newDA')
y = urb.TN.values
x = urb.DA.values
tau, p = stats.kendalltau(x,y)
tau,p
ax = sns.scatterplot(x='DA', y='TN', data=urb,
                     label=f"tau: {tau:0.3}, \np-value: {p:0.03}")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7f067fc3f450>
[[file:./.ob-jupyter/06d68717c75c40efb571e6b93c9303cbe53d227f.png]]
:END:
#+BEGIN_SRC jupyter-python
from scipy.stats import pearsonr
reps=10**4
n_size = urb.TN.size
pvals = []
r1,p1 = stats.pearsonr(x,y)
for i in np.arange(1,reps+1):
    # samples are paired and must be randomly selected as a row
    df = urb.sample(n=n_size, replace=True)
    xs = df.DA.values
    ys = df.TN.values
    r,ps = stats.pearsonr(xs,ys)
    pvals.append(r)
ordered = np.sort(pvals)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
fig, ax = plt.subplots()
sns.distplot(ordered, ax=ax, label='Bootstrap r replicates')
ax.set_title(f"Pearson r distribution, {reps:,} samples")
ax.axhline(y=0,
           xmin=lower,
           xmax=upper,
           color='r',
           lw=2,
           label=f'95% CI ({lower:0.2}, {upper:0.02})')
ax.axvline(x=r1, color='k', label=f'Observed r = {r1:0.02}')
ax.set_xlabel('r statistic')
ax.legend()


#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7f067ee28a50>

#+ATTR_HTML: :width 300%
#+ATTR_ORG: :width 300
[[file:./.ob-jupyter/f029c7638f783ca59d796316c502651c0153760e.png]]
:END:
* Section 8: Linear Regression
Regression, residuals and re-learning what to do and what not to do.


All models are wrong, some models are useful
--George Box


** Parametric method of fitting a straight line.
assumption:
1. linear pattern of data
2. variance of resideuals is constant for all X
3. normal distribution of residuals

   Line is a "linear mean", an estimate of mean(Y) given X.

Regression Hypothesis Test
   H0: no change in Y with X

   this implies:
       slope = 0
       correlation coeff. r = 0
       for each X, best prediction is mean(Y)
   H1: Y changes linearly with X

   this implies:
       slope != 0
       correlation coeff. r != 0
       better prediction of Y than mean(Y)

  Regression hypothesis test
  t-test on slope b1:

  $$\displaystyle \frac{b_1}{s/\sqrt{\sum(x-\hat{x})^2}}$$ signal/noise

  where s is std of residuals
  $$\displaystyle s = \sqrt{\frac{\sum e_i^2}{n-2}}$$
  also called the standard error of the regression

  r^2
  The percent variance of Y explained by the regression model.
  It can be compared among models if the units of Y have not changed.

it's just the r from Pearson, but also in regression:

$$\displaystyle r^2 = 1 - \frac {SS_{residuals}} {SS_{total(Y)}} = \frac
{SS_{regression}} {SS_{total(Y)}}$$

Plot the data! High r^2 can result from one outlier. r^2 is "in the units of Y"


** How to build a good regression model?
Check three assumptions of linear regression

1. Check for linearity -- the residuals plot
   plot: if data are curved, transform X or Y
         residuals vs fitted is yhat on the x-axis and y-yhat on the y-axis
   numerical: r^2 if Y units are not changed

   
2. Check constant variance -- std error vs fitted plot
   plot: if std error is changing, transform Y
         scale-location plot
         y-axis = sqrt standardized residuals vs fitted values
   numerical: Breusch-Pagan test (bptest)

3. Check normality -- QQ plot of residuals
   plot: if residuals non-normal, transform Y
   numerical: Shapiro-Wilk test of residuals
** PYTHON example
Does TDS preduct Uranium concentration sufficiently to be useful?
*** Data
#+TBLNAME:uratds
|       tds |    uranium | hco3 | def   |
|-----------+------------+------+-------|
|  682.6499 |  0.9315000 |    0 | <=50% |
|  819.1199 |  1.9380000 |    0 | <=50% |
|  303.7600 |  0.2919000 |    0 | <=50% |
| 1151.3999 | 11.9041977 |    0 | <=50% |
|  582.4199 |  1.5674000 |    0 | <=50% |
| 1043.3899 |  2.0622997 |    0 | <=50% |
|  634.8400 |  3.8857999 |    0 | <=50% |
| 1087.2500 |  0.9771999 |    0 | <=50% |
| 1123.5100 |  1.9353998 |    0 | <=50% |
|  688.0900 |  0.4367000 |    0 | <=50% |
| 1174.5398 | 10.1141987 |    0 | <=50% |
|  599.5000 |  0.7550999 |    0 | <=50% |
| 1240.8101 |  6.8558998 |    0 | <=50% |
|  538.3500 |  0.4805999 |    0 | <=50% |
|  607.7500 |  1.1451998 |    0 | <=50% |
|  705.8899 |  6.0875998 |    0 | <=50% |
| 1290.5698 | 10.8822994 |    0 | <=50% |
|  526.0900 |  0.1473000 |    0 | <=50% |
|  784.6799 |  2.6740994 |    0 | <=50% |
|  953.1399 |  3.0917997 |    0 | <=50% |
| 1149.3101 |  0.7591999 |    0 | <=50% |
| 1074.2200 |  3.7100997 |    0 | <=50% |
| 1116.5898 |  7.2445993 |    0 | <=50% |
|  301.2000 |  5.7189999 |    1 | >50%  |
|  265.4500 |  4.7365999 |    1 | >50%  |
|  295.8799 |  2.8056998 |    1 | >50%  |
|  442.3600 |  5.6289997 |    1 | >50%  |
|  342.7100 |  3.0949998 |    1 | >50%  |
|  361.3000 |  3.5773997 |    1 | >50%  |
|  262.0699 |  1.7710998 |    1 | >50%  |
|  546.2200 | 11.2723999 |    1 | >50%  |
|  273.8900 |  4.9806995 |    1 | >50%  |
|  281.3799 |  4.0832996 |    1 | >50%  |
|  588.8600 | 14.6341991 |    1 | >50%  |
|  574.1100 | 12.3834992 |    1 | >50%  |
|  307.0900 |  1.5290999 |    1 | >50%  |
|  409.3700 |  4.4646997 |    1 | >50%  |
|  327.0699 |  2.4573998 |    1 | >50%  |
|  425.6899 |  6.3041992 |    1 | >50%  |
|  310.0500 |  4.5440998 |    1 | >50%  |
|  289.7500 |  0.9671999 |    1 | >50%  |
|  408.1800 |  2.1567998 |    1 | >50%  |
|  383.0400 |  8.3809986 |    1 | >50%  |
|  255.1900 |  2.7956996 |    1 | >50%  |
*** Analysis

#+BEGIN_SRC jupyter-python :var uratds = uratds
ur = pd.DataFrame(uratds, columns=['tds', 'uranium', 'hco3', 'definition'])
print("#+CAPTION: TDS Uranium Data")
ur
#+END_SRC

#+RESULTS:
:RESULTS:
: #+CAPTION: TDS Uranium Data
|    |     tds |   uranium |   hco3 | definition   |
|----+---------+-----------+--------+--------------|
|  0 |  682.65 |    0.9315 |      0 | <=50%        |
|  1 |  819.12 |    1.938  |      0 | <=50%        |
|  2 |  303.76 |    0.2919 |      0 | <=50%        |
|  3 | 1151.4  |   11.9042 |      0 | <=50%        |
|  4 |  582.42 |    1.5674 |      0 | <=50%        |
|  5 | 1043.39 |    2.0623 |      0 | <=50%        |
|  6 |  634.84 |    3.8858 |      0 | <=50%        |
|  7 | 1087.25 |    0.9772 |      0 | <=50%        |
|  8 | 1123.51 |    1.9354 |      0 | <=50%        |
|  9 |  688.09 |    0.4367 |      0 | <=50%        |
| 10 | 1174.54 |   10.1142 |      0 | <=50%        |
| 11 |  599.5  |    0.7551 |      0 | <=50%        |
| 12 | 1240.81 |    6.8559 |      0 | <=50%        |
| 13 |  538.35 |    0.4806 |      0 | <=50%        |
| 14 |  607.75 |    1.1452 |      0 | <=50%        |
| 15 |  705.89 |    6.0876 |      0 | <=50%        |
| 16 | 1290.57 |   10.8823 |      0 | <=50%        |
| 17 |  526.09 |    0.1473 |      0 | <=50%        |
| 18 |  784.68 |    2.6741 |      0 | <=50%        |
| 19 |  953.14 |    3.0918 |      0 | <=50%        |
| 20 | 1149.31 |    0.7592 |      0 | <=50%        |
| 21 | 1074.22 |    3.7101 |      0 | <=50%        |
| 22 | 1116.59 |    7.2446 |      0 | <=50%        |
| 23 |  301.2  |    5.719  |      1 | >50%         |
| 24 |  265.45 |    4.7366 |      1 | >50%         |
| 25 |  295.88 |    2.8057 |      1 | >50%         |
| 26 |  442.36 |    5.629  |      1 | >50%         |
| 27 |  342.71 |    3.095  |      1 | >50%         |
| 28 |  361.3  |    3.5774 |      1 | >50%         |
| 29 |  262.07 |    1.7711 |      1 | >50%         |
| 30 |  546.22 |   11.2724 |      1 | >50%         |
| 31 |  273.89 |    4.9807 |      1 | >50%         |
| 32 |  281.38 |    4.0833 |      1 | >50%         |
| 33 |  588.86 |   14.6342 |      1 | >50%         |
| 34 |  574.11 |   12.3835 |      1 | >50%         |
| 35 |  307.09 |    1.5291 |      1 | >50%         |
| 36 |  409.37 |    4.4647 |      1 | >50%         |
| 37 |  327.07 |    2.4574 |      1 | >50%         |
| 38 |  425.69 |    6.3042 |      1 | >50%         |
| 39 |  310.05 |    4.5441 |      1 | >50%         |
| 40 |  289.75 |    0.9672 |      1 | >50%         |
| 41 |  408.18 |    2.1568 |      1 | >50%         |
| 42 |  383.04 |    8.381  |      1 | >50%         |
| 43 |  255.19 |    2.7957 |      1 | >50%         |
:END:




#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots()
sns.scatterplot(x=ur.tds, y=ur.uranium, ax=ax, label='observations')
slope, intercept, r_value, p_value, std_err = stats.linregress(ur.tds.values, ur.uranium.values)
x = ur.tds.values
plt.plot(x, intercept + slope*x, 'g--', label='fitted line')
plt.legend()
plt.show()
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/a893d86d9457522c3494ea899312981a788e936a.png]]

Basic diagnostic plots:
1) residuals vs fitted
2) normal qq (of standardized residuals)
3) scale-location
4) residuals vs leverage

1. Residuals vs fitted
#+BEGIN_SRC jupyter-python
import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
from statsmodels.graphics.gofplots import ProbPlot

x = ur.tds
y = ur.uranium

model = sm.OLS(y, sm.add_constant(x))
model_fit = model.fit()

# model values
model_fitted_y = model_fit.fittedvalues
# model residuals
model_residuals = model_fit.resid
# normalized residuals
model_norm_residuals = model_fit.get_influence().resid_studentized_internal
# absolute squared normalized residuals
model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))
# absolute residuals
model_abs_resid = np.abs(model_residuals)
# leverage, from statsmodels internals
model_leverage = model_fit.get_influence().hat_matrix_diag
# cook's distance, from statsmodels internals
model_cooks = model_fit.get_influence().cooks_distance[0]



plot_lm_1 = plt.figure()
plot_lm_1.axes[0] = sns.residplot(model_fitted_y, y, data=ur,
                          lowess=True,
                          scatter_kws={'alpha': 0.5},
                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})

plot_lm_1.axes[0].set_title('Residuals vs Fitted')
plot_lm_1.axes[0].set_xlabel('Fitted values')
plot_lm_1.axes[0].set_ylabel('Residuals');
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/eabee9b5becdc9bd9f2d9d5e5963050bc3377e8b.png]]

2) QQ-Plot
#+BEGIN_SRC jupyter-python
QQ = ProbPlot(model_norm_residuals)
plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)
plot_lm_2.axes[0].set_title('Normal Q-Q')
plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')
plot_lm_2.axes[0].set_ylabel('Standardized Residuals');
# annotations
abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)
abs_norm_resid_top_3 = abs_norm_resid[:3]
for r, i in enumerate(abs_norm_resid_top_3):
    plot_lm_2.axes[0].annotate(i,
                               xy=(np.flip(QQ.theoretical_quantiles, 0)[r],
                                   model_norm_residuals[i]));


#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/8d9c84f721148f1b7061d5cd85e8317a745a73f0.png]]
3) Scale-Location Plot

#+BEGIN_SRC jupyter-python
plot_lm_3 = plt.figure()
plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5);
sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt,
            scatter=False,
            ci=False,
            lowess=True,
            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});
plot_lm_3.axes[0].set_title('Scale-Location')
plot_lm_3.axes[0].set_xlabel('Fitted values')
plot_lm_3.axes[0].set_ylabel('$\sqrt{|Standardized Residuals|}$');

# annotations
abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)
abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]
for i in abs_norm_resid_top_3:
    plot_lm_3.axes[0].annotate(i,
                               xy=(model_fitted_y[i],
                                   model_norm_residuals_abs_sqrt[i]));


#+END_SRC

   #+RESULTS:
   [[file:./.ob-jupyter/9fcac278d16e790fa71dffbafbb48b6a17892858.png]]
4) Residuals vs Leverage Plot

#+BEGIN_SRC jupyter-python
plot_lm_4 = plt.figure();
plt.scatter(model_leverage, model_norm_residuals, alpha=0.5);
sns.regplot(model_leverage, model_norm_residuals,
            scatter=False,
            ci=False,
            lowess=True,
            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});
plot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)
plot_lm_4.axes[0].set_ylim(-3, 5)
plot_lm_4.axes[0].set_title('Residuals vs Leverage')
plot_lm_4.axes[0].set_xlabel('Leverage')
plot_lm_4.axes[0].set_ylabel('Standardized Residuals');

# annotations
leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]
for i in leverage_top_3:
    plot_lm_4.axes[0].annotate(i,
                               xy=(model_leverage[i],
                                   model_norm_residuals[i]));
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/5a677328305f8b57cab5d02af513c3d4dd1428e6.png]]


https://robert-alvarez.github.io/2018-06-04-diagnostic_plots/
#+NAME:diagnostic
#+BEGIN_SRC jupyter-python :results silent
def graph(formula, x_range, label=None):
    """
    Helper function for plotting cook's distance lines
    """
    x = x_range
    y = formula(x)
    plt.plot(x, y, label=label, lw=1, ls='--', color='red')


def diagnostic_plots(X, y, model_fit=None):
  """
  Function to reproduce the 4 base plots of an OLS model in R.

  ---
  Inputs:

  X: A numpy array or pandas dataframe of the features to use in building the linear regression model

  y: A numpy array or pandas series/dataframe of the target variable of the linear regression model

  model_fit [optional]: a statsmodel.api.OLS model after regressing y on X. If not provided, will be
                        generated from X, y
  """

  if not model_fit:
      model_fit = sm.OLS(y, sm.add_constant(X)).fit()

  # create dataframe from X, y for easier plot handling
  dataframe = pd.concat([X, y], axis=1)

  # model values
  model_fitted_y = model_fit.fittedvalues
  # model residuals
  model_residuals = model_fit.resid
  # normalized residuals
  model_norm_residuals = model_fit.get_influence().resid_studentized_internal
  # absolute squared normalized residuals
  model_norm_residuals_abs_sqrt = np.sqrt(np.abs(model_norm_residuals))
  # absolute residuals
  model_abs_resid = np.abs(model_residuals)
  # leverage, from statsmodels internals
  model_leverage = model_fit.get_influence().hat_matrix_diag
  # cook's distance, from statsmodels internals
  model_cooks = model_fit.get_influence().cooks_distance[0]

  plot_lm_1 = plt.figure()
  plot_lm_1.axes[0] = sns.residplot(model_fitted_y, dataframe.columns[-1], data=dataframe,
                            lowess=True,
                            scatter_kws={'alpha': 0.5},
                            line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})

  plot_lm_1.axes[0].set_title('Residuals vs Fitted')
  plot_lm_1.axes[0].set_xlabel('Fitted values')
  plot_lm_1.axes[0].set_ylabel('Residuals');

  # annotations
  abs_resid = model_abs_resid.sort_values(ascending=False)
  abs_resid_top_3 = abs_resid[:3]
  for i in abs_resid_top_3.index:
      plot_lm_1.axes[0].annotate(i,
                                 xy=(model_fitted_y[i],
                                     model_residuals[i]));

  QQ = ProbPlot(model_norm_residuals)
  plot_lm_2 = QQ.qqplot(line='45', alpha=0.5, color='#4C72B0', lw=1)
  plot_lm_2.axes[0].set_title('Normal Q-Q')
  plot_lm_2.axes[0].set_xlabel('Theoretical Quantiles')
  plot_lm_2.axes[0].set_ylabel('Standardized Residuals');
  # annotations
  abs_norm_resid = np.flip(np.argsort(np.abs(model_norm_residuals)), 0)
  abs_norm_resid_top_3 = abs_norm_resid[:3]
  for r, i in enumerate(abs_norm_resid_top_3):
      plot_lm_2.axes[0].annotate(i,
                                 xy=(np.flip(QQ.theoretical_quantiles, 0)[r],
                                     model_norm_residuals[i]));

  plot_lm_3 = plt.figure()
  plt.scatter(model_fitted_y, model_norm_residuals_abs_sqrt, alpha=0.5);
  sns.regplot(model_fitted_y, model_norm_residuals_abs_sqrt,
              scatter=False,
              ci=False,
              lowess=True,
              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});
  plot_lm_3.axes[0].set_title('Scale-Location')
  plot_lm_3.axes[0].set_xlabel('Fitted values')
  plot_lm_3.axes[0].set_ylabel('$\sqrt{|Standardized Residuals|}$');

  # annotations
  abs_sq_norm_resid = np.flip(np.argsort(model_norm_residuals_abs_sqrt), 0)
  abs_sq_norm_resid_top_3 = abs_sq_norm_resid[:3]
  for i in abs_norm_resid_top_3:
      plot_lm_3.axes[0].annotate(i,
                                 xy=(model_fitted_y[i],
                                     model_norm_residuals_abs_sqrt[i]));


  plot_lm_4 = plt.figure();
  plt.scatter(model_leverage, model_norm_residuals, alpha=0.5);
  sns.regplot(model_leverage, model_norm_residuals,
              scatter=False,
              ci=False,
              lowess=True,
              line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8});
  plot_lm_4.axes[0].set_xlim(0, max(model_leverage)+0.01)
  plot_lm_4.axes[0].set_ylim(-3, 5)
  plot_lm_4.axes[0].set_title('Residuals vs Leverage')
  plot_lm_4.axes[0].set_xlabel('Leverage')
  plot_lm_4.axes[0].set_ylabel('Standardized Residuals');

  # annotations
  leverage_top_3 = np.flip(np.argsort(model_cooks), 0)[:3]
  for i in leverage_top_3:
      plot_lm_4.axes[0].annotate(i,
                                 xy=(model_leverage[i],
                                     model_norm_residuals[i]));

  p = len(model_fit.params) # number of model parameters
  graph(lambda x: np.sqrt((0.5 * p * (1 - x)) / x),
        np.linspace(0.001, max(model_leverage), 50),
        'Cook\'s distance') # 0.5 line
  graph(lambda x: np.sqrt((1 * p * (1 - x)) / x),
        np.linspace(0.001, max(model_leverage), 50)) # 1 line
  plot_lm_4.legend(loc='upper right');

#+END_SRC



#+RESULTS:
#+BEGIN_SRC jupyter-python
diagnostic_plots(x, y)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/c257210b7f873ab1dfad66b15b5f7aa801dbcdce.png]]
[[file:./.ob-jupyter/486e24a9437239911e12f626fd3846002a26808d.png]]
[[file:./.ob-jupyter/856935f675e3784305801994cf12a2136b31ecf6.png]]
[[file:./.ob-jupyter/0e92aac2ac87cd4d3c968492e346ba638e72cc66.png]]
:END:

#+BEGIN_SRC jupyter-python
diagnostic_plots(x, np.log(y))
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/8aa3abd62bd7f94fc32a85eefcfb0edf709c338d.png]]
[[file:./.ob-jupyter/973cca266ab3776fc0d68bfe6469e2ab6c2dfaa8.png]]
[[file:./.ob-jupyter/2986df632501cec4dd764ad4fdf201e85aabb3a7.png]]
[[file:./.ob-jupyter/40cd3cb29d5751acc8818fe1b4686c12b0d70ec7.png]]
:END:

*** TODO add: numerical tests and tabular output for each plot.
** PYTHON handouts

#+BEGIN_SRC jupyter-python :var urband = urband
tn = pd.DataFrame(urband, columns=['TN', 'DA', 'newDA'])
x=tn.DA
y=tn.TN
diagnostic_plots(x,y)
print('-----')
diagnostic_plots(x, np.log(y))
#+END_SRC

#+RESULTS:
:RESULTS:
: -----
[[file:./.ob-jupyter/20365c2784faff956ab50d004dea65e10749f986.png]]
[[file:./.ob-jupyter/3c42662ea5f4791925a2b958dd3bfc6c632e1b51.png]]
[[file:./.ob-jupyter/dba68e660fbc2e5931c0fe35485dbc8c3f8b40aa.png]]
[[file:./.ob-jupyter/b3495e822fc917f9006f6f547c181ca10cca05d2.png]]
[[file:./.ob-jupyter/104ccd2e3026fd5f1a222579dec123e998dcdc4b.png]]
[[file:./.ob-jupyter/768c3755f71a6a8a28b8b3e5e1a4ea1bd4e62694.png]]
[[file:./.ob-jupyter/580a3eee1550e9a2bc30b6492e9a6245b7fd9c63.png]]
[[file:./.ob-jupyter/c99e99b5ae40c546fb0ece35961ec85e040e7ba4.png]]
:END:
:RESULTS:
: -----
[[file:./.ob-jupyter/3615eedb317c452247eba2459ada720201152e7c.png]]
[[file:./.ob-jupyter/290eda49844bdc19f25dd90a58e500d49e2b6b75.png]]
[[file:./.ob-jupyter/f7bf187d6a348cd44b8957ad161ab280857632bf.png]]
[[file:./.ob-jupyter/49e667eedafc7e29551df684dda7ce23d625117f.png]]
[[file:./.ob-jupyter/ea21b470afe69d6ec30cb7a0af5cd87e70644727.png]]
[[file:./.ob-jupyter/8ec8f862b72f653a0ff2184329345c7562197c03.png]]
[[file:./.ob-jupyter/cc9718a6781a39906098cdaa470c3d0ac0ae523f.png]]
[[file:./.ob-jupyter/3b0c4df311676ea36f6372e23cc8dab1426815c1.png]]

#+BEGIN_SRC jupyter-python
from statsmodels.compat import lzip

model = smf.ols('DA ~ np.log(TN)', data = tn).fit()
names = ['Lagrange multiplier statistic', 'p-value',
        'f-value', 'f p-value']
test = het_breuschpagan(model.resid, model.model.exog)

dat = dict(zip(names, test))
pd.DataFrame.from_dict(dat, orient='index',
                       columns=['Statistic'])
#+END_SRC

#+RESULTS:
|                               | Statistic |
|-------------------------------+-----------|
| Lagrange multiplier statistic |  0.515495 |
| p-value                       |   0.47277 |
| f-value                       |  0.497049 |
| f p-value                     |  0.484885 |
:RESULTS:
#+BEGIN_SRC jupyter-python
model.summary2().tables[1]
#+END_SRC

#+RESULTS:
|            |      Coef. |  Std.Err. |         t | P>t          |    [0.025 |    0.975] |
|------------+------------+-----------+-----------+--------------+-----------+-----------|
| Intercept  | -0.0421775 | 0.0611287 | -0.689979 |     0.494191 | -0.165723 | 0.0813682 |
| np.log(TN) |   0.136591 | 0.0332196 |   4.11174 |  0.000189627 | 0.0694512 |   0.20373 |
* Section 9: Multiple Linear Regression

$$ Y = b_0 + b_1 X_1 + b_2 X_2 + \ldots + b_j X_j $$
a slope b for each explantory variable X

Parametric method of fitting a sruface.
Assumptions same as simple regression but now with j X variables:
1) A linear pattern of data (a surface in j = 3+ dimensions)
2) The variance of residuals around the surface is constant across the ranges of all X_j
3) A normal distribution of residuals

** Data

#+TBLNAME:cuyahoga
| month |    year | tds |    Q | month_txt |
|-------+---------+-----+------+-----------|
|     1 | 1974.04 | 490 |  458 | Jan       |
|     2 | 1974.12 | 540 |  469 | Feb       |
|     4 | 1974.29 | 220 | 4630 | Apr       |
|     7 | 1974.54 | 390 |  321 | Jul       |
|    10 | 1974.79 | 450 |  541 | Oct       |
|     1 | 1975.04 | 230 | 1640 | Jan       |
|     4 | 1975.29 | 360 | 1060 | Apr       |
|     7 | 1975.54 | 460 |  264 | Jul       |
|    10 | 1975.79 | 430 |  665 | Oct       |
|     1 | 1976.04 | 430 |  680 | Jan       |
|     4 | 1976.29 | 620 |  650 | Apr       |
|     8 | 1976.62 | 460 |  490 | Aug       |
|    10 | 1976.79 | 450 |  380 | Oct       |
|     1 | 1977.04 | 580 |  325 | Jan       |
|     4 | 1977.29 | 350 | 1020 | Apr       |
|     7 | 1977.54 | 440 |  460 | Jul       |
|    10 | 1977.79 | 530 |  583 | Oct       |
|    11 | 1977.87 | 380 |  777 | Nov       |
|    12 | 1977.96 | 440 | 1230 | Dec       |
|     1 | 1978.04 | 430 |  565 | Jan       |
|     2 | 1978.12 | 680 |  533 | Feb       |
|     3 | 1978.21 | 250 | 4930 | Mar       |
|     4 | 1978.29 | 250 | 3810 | Apr       |
|     5 | 1978.37 | 450 |  469 | May       |
|     6 | 1978.46 | 500 |  473 | Jun       |
|     7 | 1978.54 | 510 |  593 | Jul       |
|     8 | 1978.62 | 490 |  500 | Aug       |
|     9 | 1978.71 | 700 |  266 | Sep       |
|    10 | 1978.79 | 420 |  495 | Oct       |
|    11 | 1978.87 | 710 |  245 | Nov       |
|    12 | 1978.96 | 430 |  736 | Dec       |
|     1 | 1979.04 | 410 |  508 | Jan       |
|     2 | 1979.12 | 700 |  578 | Feb       |
|     3 | 1979.21 | 260 | 4590 | Mar       |
|     4 | 1979.29 | 260 | 4670 | Apr       |
|     5 | 1979.37 | 500 |  503 | May       |
|     6 | 1979.46 | 450 |  469 | Jun       |
|     7 | 1979.54 | 500 |  314 | Jul       |
|     8 | 1979.62 | 620 |  432 | Aug       |
|     9 | 1979.71 | 670 |  279 | Sep       |
|    10 | 1979.79 | 410 |  542 | Oct       |
|    11 | 1979.87 | 470 |  499 | Nov       |
|    12 | 1979.96 | 370 |  741 | Dec       |
|     1 | 1980.04 | 410 |  569 | Jan       |
|     2 | 1980.12 | 540 |  360 | Feb       |
|     3 | 1980.21 | 550 |  513 | Mar       |
|     4 | 1980.29 | 220 | 3910 | Apr       |
|     5 | 1980.37 | 460 |  364 | May       |
|     6 | 1980.46 | 390 |  472 | Jun       |
|     7 | 1980.54 | 550 |  245 | Jul       |
|     8 | 1980.62 | 320 | 1500 | Aug       |
|     9 | 1980.71 | 570 |  224 | Sep       |
|    10 | 1980.79 | 480 |  342 | Oct       |
|    12 | 1980.96 | 520 |  732 | Dec       |
|     1 | 1981.04 | 620 |  240 | Jan       |
|     2 | 1981.12 | 520 |  472 | Feb       |
|     3 | 1981.21 | 430 |  679 | Mar       |
|     4 | 1981.29 | 400 | 1080 | Apr       |
|     5 | 1981.37 | 430 |  920 | May       |
|     6 | 1981.46 | 490 |  488 | Jun       |
|     7 | 1981.54 | 560 |  444 | Jul       |
|     8 | 1981.62 | 370 |  595 | Aug       |
|     9 | 1981.71 | 460 |  295 | Sep       |
|    10 | 1981.79 | 390 |  542 | Oct       |
|    12 | 1981.96 | 330 | 1500 | Dec       |
|     3 | 1982.21 | 350 | 1080 | Mar       |
|     5 | 1982.37 | 480 |  334 | May       |
|     6 | 1982.46 | 390 |  423 | Jun       |
|     8 | 1982.62 | 500 |  216 | Aug       |
|    11 | 1982.87 | 410 |  366 | Nov       |
|     2 | 1983.12 | 470 |  750 | Feb       |
|     5 | 1983.37 | 280 | 1260 | May       |
|     8 | 1983.62 | 510 |  223 | Aug       |
|    11 | 1983.87 | 470 |  462 | Nov       |
|     2 | 1984.12 | 310 | 7640 | Feb       |
|     5 | 1984.37 | 230 | 2340 | May       |
|     7 | 1984.54 | 470 |  239 | Jul       |
|    11 | 1984.87 | 330 | 1400 | Nov       |
|     3 | 1985.21 | 320 | 3070 | Mar       |
|     5 | 1985.37 | 500 |  244 | May       |
#+BEGIN_SRC jupyter-python :var cuyahoga=cuyahoga
cuyahoga=pd.DataFrame(cuyahoga,
                      columns=["month","year","tds","Q","month_txt"])
#+END_SRC


** Analysis
#+BEGIN_SRC jupyter-python
est = smf.ols('tds ~ Q + year', data=cuyahoga).fit()
est.summary2().tables[0]

#+END_SRC



|   | 0                   |                1 | 2                   |        3 |
|---+---------------------+------------------+---------------------+----------|
| 0 | Model:              |              OLS | Adj. R-squared:     |    0.394 |
| 1 | Dependent Variable: |              tds | AIC:                |  948.858 |
| 2 | Date:               | 2020-08-05 00:55 | BIC:                |  956.004 |
| 3 | No. Observations:   |               80 | Log-Likelihood:     |  -471.43 |
| 4 | Df Model:           |                2 | F-statistic:        |    26.71 |
| 5 | Df Residuals:       |               77 | Prob (F-statistic): | 1.54e-09 |
| 6 | R-squared:          |            0.410 | Scale:              |     7991 |


|           |      Coef. |   Std.Err. |         t |     P>t     |     [0.025 |     0.975] |
|-----------+------------+------------+-----------+-------------+------------+------------|
| Intercept |    3461.23 |    7229.08 |  0.478792 |    0.633443 |   -10933.7 |    17856.2 |
| Q         | -0.0546536 | 0.00752576 |   -7.2622 | 2.64035e-10 | -0.0696393 | -0.0396679 |
| year      |   -1.49489 |     3.6518 | -0.409357 |    0.683414 |   -8.76655 |    5.77677 |

#+BEGIN_SRC jupyter-python
sns.scatterplot(x="Q", y="tds", data=cuyahoga)
plt.style.use('seaborn-white')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/af9ab8aedfbef74e92b8983d114cfbdc1e131041.png]]

** Interpretation
H0: all slopes = 0
The overall f-test is not of much use and just asks if haaving no X variable is better than mean(Y)

H1: at least one slope != 0
Uses all X variables. Does not imply this is "best" model

Reject H0 when F is plarge, p-value < alpha (2-sided)
i.e. your model is better than a model with no X variables!


** Regression tests for each X variable
t-test for each slope coefficient b
H0: b = 0; there is no influence of X on Y
H1: b != 0; X influences Y

This is called a *Partial t-test*
$$ t_j = \frac{b_j}{SE(b_j)} $$ signal/noise

Two caveats:
1) Non-normality and non-constant variance will decrease t statistic (and increase p-value), making it less significant
2) *Multicollinearity* inflates the std error $SE(b_j)$, and so lowers t statistic, which increases the p-value, making it less significant

** Data SO2

#+TBLNAME:so2
| city           | so2 | temp | mnfctr |  pop | wind | precip | wetdays |
|----------------+-----+------+--------+------+------+--------+---------|
| PHOENIX        |  10 | 70.3 |    213 |  582 |  6.0 |   7.05 |      36 |
| LITTLE_ROCK    |  13 | 61.0 |     91 |  132 |  8.2 |  48.52 |     100 |
| SAN_FRAN.      |  12 | 56.7 |    453 |  716 |  8.7 |  20.66 |      67 |
| DENVER         |  17 | 51.9 |    454 |  515 |  9.0 |  12.95 |      86 |
| HARTFORD       |  56 | 49.1 |    412 |  158 |  9.0 |  43.37 |     127 |
| WILMINGTON     |  36 | 54.0 |     80 |   80 |  9.0 |  40.25 |     114 |
| WASHINGTON     |  29 | 57.3 |    434 |  757 |  9.3 |  38.89 |     111 |
| JACKSONVILLE   |  14 | 68.4 |    136 |  529 |  8.8 |  54.47 |     116 |
| MIAMI          |  10 | 75.5 |    207 |  335 |  9.0 |  59.80 |     128 |
| ATLANTA        |  24 | 61.5 |    368 |  497 |  9.1 |  48.34 |     115 |
| CHICAGO        | 110 | 50.6 |   3344 | 3369 | 10.4 |  34.44 |     122 |
| INDIANAPOLIS   |  28 | 52.3 |    361 |  746 |  9.7 |  38.74 |     121 |
| DES_MOINES     |  17 | 49.0 |    104 |  201 | 11.2 |  30.85 |     103 |
| WICHITA        |   8 | 56.6 |    125 |  277 | 12.7 |  30.58 |      82 |
| LOUISVILLE     |  30 | 55.6 |    291 |  593 |  8.3 |  43.11 |     123 |
| NEW_ORLEANS    |   9 | 68.3 |    204 |  361 |  8.4 |  56.77 |     113 |
| BALTIMORE      |  47 | 55.0 |    625 |  905 |  9.6 |  41.31 |     111 |
| DETROIT        |  35 | 49.9 |   1064 | 1513 | 10.1 |  30.96 |     129 |
| MINN-ST.PAUL   |  29 | 43.5 |    699 |  744 | 10.6 |  25.94 |     137 |
| KANSAS_CITY    |  14 | 54.5 |    381 |  507 | 10.0 |  37.00 |      99 |
| ST.LOUIS       |  56 | 55.9 |    775 |  622 |  9.5 |  35.89 |     105 |
| OMAHA          |  14 | 51.5 |    181 |  347 | 10.9 |  30.18 |      98 |
| ALBUQUERQUE    |  11 | 56.8 |     46 |  244 |  8.9 |   7.77 |      58 |
| ALBANY         |  46 | 47.6 |     44 |  116 |  8.8 |  33.36 |     135 |
| BUFFALO        |  11 | 47.1 |    391 |  463 | 12.4 |  36.11 |     166 |
| CINCINNATI     |  23 | 54.0 |    462 |  453 |  7.1 |  39.04 |     132 |
| CLEVELAND      |  65 | 49.7 |   1007 |  751 | 10.9 |  34.99 |     155 |
| COLUMBUS       |  26 | 51.5 |    266 |  540 |  8.6 |  37.01 |     134 |
| PHILADELPHIA   |  69 | 54.6 |   1692 | 1950 |  9.6 |  39.93 |     115 |
| PITTSBURGH     |  61 | 50.4 |    347 |  520 |  9.4 |  36.22 |     147 |
| PROVIDENCE     |  94 | 50.0 |    343 |  179 | 10.6 |  42.75 |     125 |
| MEMPHIS        |  10 | 61.6 |    337 |  624 |  9.2 |  49.10 |     105 |
| NASHVILLE      |  18 | 59.4 |    275 |  448 |  7.9 |  46.00 |     119 |
| DALLAS         |   9 | 66.2 |    641 |  844 | 10.9 |  35.94 |      78 |
| HOUSTON        |  10 | 68.9 |    721 | 1233 | 10.8 |  48.19 |     103 |
| SALT_LAKE_CITY |  28 | 51.0 |    137 |  176 |  8.7 |  15.17 |      89 |
| NORFOLK        |  31 | 59.3 |     96 |  308 | 10.6 |  44.68 |     116 |
| RICHMOND       |  26 | 57.8 |    197 |  299 |  7.6 |  42.59 |     115 |
| SEATTLE        |  29 | 51.1 |    379 |  531 |  9.4 |  38.79 |     164 |
| CHARLESTON     |  31 | 55.2 |     35 |   71 |  6.5 |  40.75 |     148 |
| MILWAUKEE      |  16 | 45.7 |    569 |  717 | 11.8 |  29.07 |     123 |
#+BEGIN_SRC jupyter-python :var so2=so2
mnf = pd.DataFrame(so2,
                   columns=["city","so2","temp","mnfctr",
                            "popl","wind","precip","wetdays"])
#+END_SRC

#+RESULTS:

** Analysis SO2
#+BEGIN_SRC jupyter-python
est1 = smf.ols('so2 ~ mnfctr + popl + precip + temp + wind + wetdays', data=mnf).fit()
est.summary2().tables[0]

#+END_SRC

#+RESULTS:
|   | 0                   |                1 | 2                   |        3 |
|---+---------------------+------------------+---------------------+----------|
| 0 | Model:              |              OLS | Adj. R-squared:     |    0.593 |
| 1 | Dependent Variable: |      np.log(so2) | AIC:                |  56.8339 |
| 2 | Date:               | 2020-08-06 00:44 | BIC:                |  68.8289 |
| 3 | No. Observations:   |               41 | Log-Likelihood:     |  -21.417 |
| 4 | Df Model:           |                6 | F-statistic:        |    10.72 |
| 5 | Df Residuals:       |               34 | Prob (F-statistic): | 1.13e-06 |
| 6 | R-squared:          |            0.654 | Scale:              |   0.2007 |

|   | 0                   |                1 | 2                   |        3 |
|---+---------------------+------------------+---------------------+----------|
| 0 | Model:              |              OLS | Adj. R-squared:     |    0.611 |
| 1 | Dependent Variable: |              so2 | AIC:                |  342.723 |
| 2 | Date:               | 2020-08-05 01:29 | BIC:                |  354.718 |
| 3 | No. Observations:   |               41 | Log-Likelihood:     |  -164.36 |
| 4 | Df Model:           |                6 | F-statistic:        |    11.48 |
| 5 | Df Residuals:       |               34 | Prob (F-statistic): | 5.42e-07 |
| 6 | R-squared:          |            0.670 | Scale:              |   214.21 |

|           |      Coef. |  Std.Err. |        t |         P>t |     [0.025 |      0.975] |
|-----------+------------+-----------+----------+-------------+------------+-------------|
| Intercept |    111.728 |   47.3181 |  2.36122 |   0.0240867 |    15.5665 |      207.89 |
| mnfctr    |  0.0649182 | 0.0157483 |  4.12225 | 0.000227786 |  0.0329139 |   0.0969225 |
| pop       | -0.0392767 | 0.0151327 | -2.59548 |   0.0138462 | -0.0700302 | -0.00852332 |
| precip    |   0.512359 |  0.362755 |  1.41241 |    0.166918 |  -0.224848 |     1.24957 |
| temp      |   -1.26794 |   0.62118 | -2.04118 |   0.0490557 |   -2.53033 | -0.00555242 |
| wind      |   -3.18137 |   1.81502 |  -1.7528 |   0.0886504 |   -6.86993 |    0.507197 |
| wetdays   | -0.0520502 |  0.162014 | -0.32127 |    0.749972 |  -0.381302 |    0.277202 |

|   | 0              |      1 | 2                 |      3 |
|---+----------------+--------+-------------------+--------|
| 0 | Omnibus:       | 16.598 | Durbin-Watson:    |  1.535 |
| 1 | Prob(Omnibus): |      0 | Jarque-Bera (JB): | 22.894 |
| 2 | Skew:          |  1.197 | Prob(JB):         |      0 |
| 3 | Kurtosis:      |  5.769 | Condition No.:    |  22799 |
#+BEGIN_SRC jupyter-python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
X = mnf[['mnfctr','popl','precip','temp','wind','wetdays']]
X=add_constant(X)
vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]

list(zip(vif,X.columns))
#+END_SRC

#+RESULTS:
|   |       0 | 1       |
|---+---------+---------|
| 0 |  428.54 | const   |
| 1 | 14.7037 | mnfctr  |
| 2 | 14.3408 | pop     |
| 3 | 3.40492 | precip  |
| 4 |   3.764 | temp    |
| 5 | 1.25552 | wind    |
| 6 | 3.44365 | wetdays |

mnfct and pop are strongly related to each other because they are both above 10.

https://stackoverflow.com/questions/42658379has/variance-inflation-factor-in-python
ctr
R has different VIF output.

*variance_inflation_factor* expects the presence of a constant in the matrix of explanatory variables. One can use add_constant from statsmodels to add the required constant to the dataframe before passing its values to the function.

** Multicollinearity measure VIF
Variance Inflation Factor
$$ VIF_j = \frac{1}{1-r^2_j} $$
where $r^2_j$ is the $r^2$ between $x_j$ and all the other $X$
ONE VIF for each of the j X variables
Want all VIFs < 10 ($r^2_j < 0.9$)

** Steps!
1) Diagnostic Plots: is the y variable in the right units? Do we need to transform it?
   linear? r^2 value
   normal? shapiro-wilk test (Normal QQ plot)
   equal variance? breusch-pagan test (scale-location plot)
2) vif test
   is there multicollinearity?
   - drop one or more redundant variables.
   "subset model selection"
   - alter the design -- collect additional data
   - centering (subtracting mean) for one variable sometimes corrects multicollinearity caused by numerical problems and interaction terms.


** Another look
#+BEGIN_SRC jupyter-python
est = smf.ols('np.log(so2) ~ popl + temp + mnfctr + precip + wetdays + wind', data=mnf).fit()
diagnostic_plots(mnf.popl, np.log(mnf.so2), model_fit=est)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/01f82e47c02991691351feeb1b3b8ad14e5426c6.png]]
[[file:./.ob-jupyter/0cc385f1167d73db677d81c57aac519991e44b88.png]]
[[file:./.ob-jupyter/b0d190799f52aed467a09935e50e7e422b138b44.png]]
[[file:./.ob-jupyter/501e7fb73c6f2b801eda4adb381c9ee64454c25a.png]]
:END:
#+BEGIN_SRC jupyter-python
names = ['Shapiro test stat','Shapiro p-value','BP Lagrange multiplier statistic',
         'BP p-value','BP f-value', 'BP f p-value']
shap = shapiro(est.resid.values)
test = het_breuschpagan(est.resid, est.model.exog)

list(zip(names, shap + test))
#+END_SRC

#+RESULTS:
|   | 0                                |        1 |
|---+----------------------------------+----------|
| 0 | Shapiro test stat                | 0.987989 |
| 1 | Shapiro p-value                  | 0.936969 |
| 2 | BP Lagrange multiplier statistic |  6.22661 |
| 3 | BP p-value                       | 0.398289 |
| 4 | BP f-value                       |  1.01469 |
| 5 | BP f p-value                     | 0.432292 |
#+BEGIN_SRC jupyter-python
est.summary2().tables[1]

#+END_SRC

|           |        Coef. |    Std.Err. |         t | P>           t |      [0.025 |      0.975] |
|-----------+--------------+-------------+-----------+----------------+-------------+-------------|
| Intercept |      7.25325 |     1.44837 |   5.00787 |     1.6779e-05 |     4.30981 |     10.1967 |
| popl      | -0.000707707 | 0.000463201 |  -1.52786 |       0.135798 | -0.00164904 | 0.000233631 |
| temp      |   -0.0599017 |   0.0190138 |  -3.15043 |     0.00339133 |  -0.0985424 |   -0.021261 |
| mnfctr    |   0.00126391 | 0.000482041 |     2.622 |       0.012984 | 0.000284286 |  0.00224354 |
| precip    |    0.0173723 |   0.0111036 |   1.56456 |       0.126947 | -0.00519299 |   0.0399376 |
| wetdays   |  0.000434694 |  0.00495911 | 0.0876556 |       0.930665 | -0.00964344 |   0.0105128 |
| wind      |    -0.169717 |   0.0555563 |  -3.05487 |     0.00435794 |   -0.282621 |  -0.0568132 |

|   | 0                   |                1 | 2                   |        3 |
|---+---------------------+------------------+---------------------+----------|
| 0 | Model:              |              OLS | Adj. R-squared:     |    0.593 |
| 1 | Dependent Variable: |      np.log(so2) | AIC:                |  56.8339 |
| 2 | Date:               | 2020-08-05 23:29 | BIC:                |  68.8289 |
| 3 | No. Observations:   |               41 | Log-Likelihood:     |  -21.417 |
| 4 | Df Model:           |                6 | F-statistic:        |    10.72 |
| 5 | Df Residuals:       |               34 | Prob (F-statistic): | 1.13e-06 |
| 6 | R-squared:          |            0.654 | Scale:              |   0.2007 |

|   | 0              |     1 | 2                 |     3 |
|---+----------------+-------+-------------------+-------|
| 0 | Omnibus:       | 0.527 | Durbin-Watson:    | 1.805 |
| 1 | Prob(Omnibus): | 0.768 | Jarque-Bera (JB): | 0.621 |
| 2 | Skew:          | 0.237 | Prob(JB):         | 0.733 |
| 3 | Kurtosis:      | 2.628 | Condition No.:    | 22799 |
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(figsize=(12,11))
sm.graphics.plot_ccpr_grid(est, fig=fig)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/6b50f562df87415608280b57c4758072f52af618.png]]
[[file:./.ob-jupyter/6b50f562df87415608280b57c4758072f52af618.png]]
:END:
#+BEGIN_SRC jupyter-python
sm.graphics.plot_partregress_grid(est)

#+END_SRC

** FLOWCHART: BUILD A GOOD MULTIPLE REGRESSION MODEL

1) Choose the best units for Y
   Run regression with all variables. check scale-location and qqplot
   run bp and shapiro tests
2) Choose the best units for each X using Component + Residual plots.
   goal is a linear relationship.
   Component-Component Plus Residual Plot
   sm.graphics.plot_ccpr_grid(model, fig=fig)
   If linear-ish then table of P>|t| is still okay--but we're not done!
right now, this says, mnfct, temp, and wind

1) Use quality of measures to choose the best model (in descending order of preference)

   AIC/AICc (default preference)
   Mallow's Cp
   PRESS (PRediction Error Sum of Squares)
   BIC
   adj r^2

2) Final Check
   compute the selected regression equation and look for:
   - all VIFs < 10 if avlues for slopes are important

   - linearity

   - constant variance

   - normal residuals

   - small p-values (<0.10?) on all X variables

If transforming Y: r^2 and t-stats, AIC, etc. may or may not be higher in the better model. For that, need to use diagnostic plots.

"All models are wrong, some models are useful." --G.E.P. Box

https://xavierbourretsicotte.github.io/subset_selection.html
#+name:subset_select
#+BEGIN_SRC jupyter-python
#Initializing useful variables
Y = np.log(mnf.so2)
m = len(Y)
p = 11
hat_sigma_squared = (1/(m - p -1)) * min(df1['RSS'])

#Computing
df1['C_p'] = (1/m) * (df1['RSS'] + 2 * df1['numb_features'] * hat_sigma_squared )
df1['AIC'] = (1/(m*hat_sigma_squared)) * (df1['RSS'] + 2 * df1['numb_features'] * hat_sigma_squared )
df1['BIC'] = (1/(m*hat_sigma_squared)) * (df1['RSS'] +  np.log(m) * df1['numb_features'] * hat_sigma_squared )
df1['R_squared_adj'] = 1 - ( (1 - df1['R_squared'])*(m-1)/(m-df1['numb_features'] -1))
df1
#+END_SRC
** PYTHON Handout
*** Data
#+TBLNAME:urbanall
|          TN |           CA |         IMP | MJTEMP | MSRAIN | PRES | PNON | PCOMM | PIND |
|-------------+--------------+-------------+--------+--------+------+------+-------+------|
|  374.041681 | 0.4500699322 |  4446.92700 |   15.3 |  20.24 |   48 |   28 |     5 |   19 |
|    6.048114 | 0.0047556513 | 14469.04942 |   15.3 |  20.24 |    0 |    0 |     0 |  100 |
|   15.303473 | 0.0008757699 | 16409.29792 |   15.3 |  20.24 |   67 |    0 |    33 |    0 |
|   57.716502 | 0.0433930323 |  2061.53162 |   15.3 |  20.24 |   55 |   34 |     0 |   10 |
|   15.174347 | 0.0071518764 |  4473.62936 |   15.3 |  20.24 |   48 |    0 |     0 |   52 |
|   41.495404 | 0.0002240474 |  2329.70000 |   24.9 |  24.66 |  100 |    0 |     0 |    0 |
|  675.025222 | 0.0003114871 | 18908.62722 |   24.9 |  24.66 |   84 |    0 |    16 |    0 |
| 7193.330259 | 0.1590340064 | 12426.24752 |   20.4 |  23.31 |   81 |    6 |    13 |    0 |
|    2.638396 | 0.0001224560 | 39732.87583 |   11.4 |  20.31 |    0 |    0 |   100 |    0 |
|   28.964764 | 0.0021914926 | 24518.84454 |   11.4 |  20.31 |   30 |    0 |    70 |    0 |
|   77.129408 | 0.0062046887 | 11954.73628 |   16.2 |  11.83 |   33 |   37 |    30 |    0 |
|   21.314247 | 0.0003114871 | 38366.30721 |   58.7 |  59.05 |    0 |    2 |    98 |    0 |
|    2.630450 | 0.0015367032 |  6052.15441 |   58.7 |  62.00 |  100 |    0 |     0 |    0 |
|    3.900408 | 0.0039088590 |  3832.83391 |   58.7 |  62.00 |    0 |   60 |    40 |    0 |
|    3.203391 | 0.0074365243 |    44.53075 |   10.8 |  21.81 |    6 |   58 |    36 |    0 |
|   15.099411 | 0.7122951709 |    79.81968 |   10.8 |  21.81 |   13 |   81 |     6 |    0 |
|   31.045596 | 0.0007923371 | 22069.88143 |   11.4 |  20.31 |   26 |    0 |    74 |    0 |
|  169.703188 | 0.0021914926 | 24659.25775 |   11.4 |  20.31 |   31 |    0 |    56 |   13 |
|    9.170859 | 0.0010569879 |  8702.41921 |   11.4 |  20.31 |  100 |    0 |     0 |    0 |
|   56.221186 | 0.0047556513 |  8166.19977 |   11.4 |  20.31 |  100 |    0 |     0 |    0 |
|    9.267871 | 0.0001088128 | 39917.19357 |   11.4 |  20.31 |    0 |    0 |   100 |    0 |
|    9.066298 | 0.0001088128 | 40287.47045 |   11.4 |  20.31 |    0 |    0 |   100 |    0 |
| 4920.621955 | 0.6511334403 |  3360.16033 |   17.0 |  22.46 |   83 |    7 |    10 |    0 |
|   70.058365 | 0.0126730165 |    24.33829 |    3.2 |  20.60 |   33 |   67 |     0 |    0 |
|  243.377262 | 0.0091154896 |   249.97121 |    3.2 |  20.60 |   87 |   12 |     1 |    0 |
|  257.092851 | 0.0778645898 |  1233.21841 |    3.2 |  20.60 |   83 |   17 |     0 |    0 |
|  128.709321 | 0.0075812357 | 17721.07240 |    3.2 |  20.60 |   10 |    6 |    85 |    0 |
|  584.130657 | 0.1757842660 |  3592.11308 |    3.2 |  20.60 |   80 |    1 |    20 |    0 |
|   40.032250 | 0.0306104701 |  2329.70000 |    3.2 |  20.60 |   97 |    0 |     4 |    0 |
|   49.009469 | 0.0058219705 |  2010.11081 |   25.0 |  24.41 |  100 |    0 |     0 |    0 |
|   19.328582 | 0.0002880978 |  3360.16033 |   25.0 |  24.41 |   88 |   12 |     0 |    0 |
|   21.561551 | 0.0007135756 |  3337.41336 |   25.0 |  24.41 |   78 |   23 |     0 |    0 |
|    9.554325 | 0.0017829124 |  1132.40036 |   25.0 |  24.41 |   93 |    7 |     0 |    0 |
|    3.646700 | 0.0006041155 | 39183.20894 |   32.2 |  46.18 |    0 |    0 |   100 |    0 |
|    1.918315 | 0.0108116008 |   393.77669 |   32.2 |  46.18 |   96 |    0 |     4 |    0 |
|   20.644292 | 0.0587489353 |  5708.73653 |   32.2 |  46.18 |   65 |    1 |    35 |    0 |
|   68.966788 | 0.0173676077 |  1059.96128 |   22.5 |  22.53 |   79 |    2 |    16 |    4 |
|  214.136155 | 0.0138707518 |  3071.63778 |   22.5 |  22.53 |    8 |   29 |    63 |    0 |
|  252.650406 | 0.2083435213 |  1976.24143 |   28.5 |  41.36 |   84 |   12 |     2 |    2 |
| 1270.193833 | 0.0639705375 | 37558.65228 |   50.1 |  49.38 |   55 |   21 |    25 |    0 |
|   20.657131 | 0.0008334701 |   393.77669 |   50.1 |  49.38 |   48 |   52 |     0 |    0 |
|   30.438272 | 0.0019138152 |   583.87516 |   50.1 |  49.38 |   89 |   11 |     0 |    0 |
#+BEGIN_SRC jupyter-python :var urbanall=urbanall
urb = pd.DataFrame(urbanall, columns=["TN","CA","IMP","MJTEMP","MSRAIN","PRES","PNON","PCOMM","PIND"])
#+END_SRC

#+RESULTS:


*** Analysis

Step 1: See if Y needs transformation.

#+BEGIN_SRC jupyter-python
sns.scatterplot(x='PIND',y='TN',data=urb)
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f557714ad30>
[[file:./.ob-jupyter/0eef44393d812779a3613cadd81694a19109df68.png]]
:END:
No obvious Y transformation needed looking at TN compared with other values.
Try diagnostic. ln(Y) looks better:
#+BEGIN_SRC jupyter-python
urb_est = smf.ols('np.log(TN) ~ np.log(CA) + IMP + MJTEMP + MSRAIN + PRES + PNON + PCOMM + PIND', data=urb).fit()
diagnostic_plots(np.log(urb.TN), urb[["CA","IMP","MJTEMP","MSRAIN","PRES","PNON","PCOMM","PIND"]], model_fit=urb_est)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/17197c26a49f5e9d0932de76a29f80a68e5f8ff4.png]]
[[file:./.ob-jupyter/8c638d314df34554c6367da9fe2e333ea59a6dde.png]]
[[file:./.ob-jupyter/3f2d2e229b3594ab6200fbe46b48a3dc7a15b66c.png]]
[[file:./.ob-jupyter/8bc7180111aa34a8ba6f2a3bac0f3c0f3fa1002f.png]]
:END:

Step 2: See if X needs adjustment (ccpr)
#+BEGIN_SRC jupyter-python
fig, ax = plt.subplots(figsize=(12,25))
sm.graphics.plot_ccpr_grid(urb_est, fig=fig)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/04a674a397be6c7ac6021883afa3dbed36004251.png]]
[[file:./.ob-jupyter/04a674a397be6c7ac6021883afa3dbed36004251.png]]
:END:


#+BEGIN_SRC jupyter-python
urb['lnTN'] = np.log(urb['TN'])
urb['lnCA'] = np.log(urb['CA'])
X = urb[["lnTN","lnCA","IMP","MSRAIN","PIND","PRES","PNON"]]
         #'MJTEMP'"PCOMM"]]
X=add_constant(X)
vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
list(zip(vif,X.columns))
#+END_SRC

#+RESULTS:
|   |       0 | 1      |
|---+---------+--------|
| 0 | 58.3504 | const  |
| 1 | 2.15938 | lnTN   |
| 2 | 2.41039 | lnCA   |
| 3 | 4.55614 | IMP    |
| 4 | 1.20188 | MSRAIN |
| 5 |  1.4681 | PIND   |
| 6 | 4.45179 | PRES   |
| 7 | 3.07499 | PNON   |




#+BEGIN_SRC jupyter-python
sns.scatterplot(x='MJTEMP',y='MSRAIN',data=urb)
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f4f8e0f41f0>
[[file:./.ob-jupyter/ca6bfd2cd7724c59c109447e8cc26080f827d82e.png]]
:END:


#+BEGIN_SRC jupyter-python
urb_est2 = smf.ols('lnTN ~ lnCA + IMP + MSRAIN + PIND + PRES + PNON', data=urb).fit()
urb_est2.summary2().tables[0]

#+END_SRC

**** urb_est2 'lnTN ~ lnCA + IMP + MSRAIN + PIND + PRES + PNON'
|   | 0                   |                1 | 2                   |        3 |
|---+---------------------+------------------+---------------------+----------|
| 0 | Model:              |              OLS | Adj. R-squared:     |    0.458 |
| 1 | Dependent Variable: |             lnTN | AIC:                |  156.676 |
| 2 | Date:               | 2020-08-06 02:23 | BIC:                |   168.84 |
| 3 | No. Observations:   |               42 | Log-Likelihood:     |  -71.338 |
| 4 | Df Model:           |                6 | F-statistic:        |    6.763 |
| 5 | Df Residuals:       |               35 | Prob (F-statistic): | 8.05e-05 |
| 6 | R-squared:          |            0.537 | Scale:              |   2.0991 |

**** All Variables urb_est
|   | 0                   |                1 | 2                   |          3 |
|---+---------------------+------------------+---------------------+------------|
| 0 | Model:              |              OLS | Adj. R-squared:     |       0.11 |
| 1 | Dependent Variable: |               TN | AIC:                |    725.993 |
| 2 | Date:               | 2020-08-06 01:26 | BIC:                |    741.632 |
| 3 | No. Observations:   |               42 | Log-Likelihood:     |       -354 |
| 4 | Df Model:           |                8 | F-statistic:        |      1.636 |
| 5 | Df Residuals:       |               33 | Prob (F-statistic): |      0.152 |
| 6 | R-squared:          |            0.284 | Scale:              | 1.5601e+06 |




* Section 10: Testing Differences in Regression Lines
Analysis of Covariance (ANCOVA)

Are there two regression lines that differ?
- ANCOVA
- Theil-Sen on residuals

Two types of explanatory variables
Regression model
1. Cts variable X

   $$ Y = b_0 + b_1 X + b_2 G $$

2. Binary (group) variable G
   G= 0 or 1
   indicates group assignment (say, 0= winter, 1 = summer)

H0 : there is only one regression line, b2 = 0
HA : two regression lines, if b2 != 0
** ANCOVA is just multiple regression
1) use basic diagnostic plots to determine units of Y
2) use component partial plots to determine if units of X correct
3) use subset model selection to determine best model based on BIC or other measure of quality
4) check the selected model to see if residuals plots look correct, t-stats are significant, and VIFs are <10

** Example
*** Data
#+TBLNAME:fecoli
| group | fecoli |    year | season |
|-------+--------+---------+--------|
|     0 |    100 | 1971.00 | warm   |
|     0 |    220 | 1972.00 | warm   |
|     0 |    300 | 1973.00 | warm   |
|     0 |    430 | 1974.00 | warm   |
|     0 |    640 | 1975.00 | warm   |
|     0 |   1600 | 1976.00 | warm   |
|     0 |     65 | 1971.25 | warm   |
|     0 |    120 | 1972.25 | warm   |
|     0 |    210 | 1973.25 | warm   |
|     0 |    280 | 1974.25 | warm   |
|     0 |    500 | 1975.25 | warm   |
|     0 |   1100 | 1976.25 | warm   |
|     1 |     28 | 1971.50 | cool   |
|     1 |     58 | 1972.50 | cool   |
|     1 |    120 | 1973.50 | cool   |
|     1 |    230 | 1974.50 | cool   |
|     1 |    310 | 1975.50 | cool   |
|     1 |    500 | 1976.50 | cool   |
|     1 |     22 | 1971.75 | cool   |
|     1 |     53 | 1972.75 | cool   |
|     1 |    110 | 1973.75 | cool   |
|     1 |    140 | 1974.75 | cool   |
|     1 |    320 | 1975.75 | cool   |
|     1 |   1300 | 1976.75 | cool   |

#+BEGIN_SRC jupyter-python
diagnostic_plots(fecal.year, fecal.lnfec)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/bfb2c7fd593c44d18561880927a0f2c34570d5b7.png]]
[[file:./.ob-jupyter/06d726a73f24e6eb87b5ac6d7864b6e3f1e50c5f.png]]
[[file:./.ob-jupyter/962c470c1361a342b0f2634f344826ab48989172.png]]
[[file:./.ob-jupyter/9eed16c0d937a9ae1db8e261ed5e23ea17181579.png]]
:END:

#+BEGIN_SRC jupyter-python
sm.graphics.plot_ccpr_grid(fec_est)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/9f94e78351e7b3613f8fc54b7cbd76d0b895e831.png]]
[[file:./.ob-jupyter/9f94e78351e7b3613f8fc54b7cbd76d0b895e831.png]]
:END:





#+BEGIN_SRC jupyter-python :var fecoli=fecoli
fecal = pd.DataFrame(fecoli, columns=['group','fecoli','year','season'])
fecal['lnfec'] = np.log(fecal['fecoli'])
fec_est = smf.ols('lnfec ~ year + season', data=fecal).fit()
fec_est.summary2().tables[1]
#+END_SRC

|                |    Coef. |  Std.Err. |        t |         P>t |   [0.025 |   0.975] |
|----------------+----------+-----------+----------+-------------+----------+----------|
| Intercept      | -1140.52 |   78.4132 | -14.5449 | 1.94596e-12 | -1303.58 | -977.446 |
| season[T.warm] |  1.08643 |  0.137476 |  7.90266 | 1.00155e-07 |  0.80053 |  1.37233 |
| year           | 0.580232 | 0.0397205 |  14.6079 |  1.7912e-12 | 0.497628 | 0.662835 |

#+BEGIN_SRC jupyter-python

sns.lmplot(x='year', y='lnfec', data=fecal, hue='season', palette='Set1')
plt.style.use('seaborn-white')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/822453de62e48c6a7d33f1a89f18e2b82a3ab929.png]]


** Testing for differences in slope
H0 : no difference in slope
create a 'cross-product' variable by multiply the X and G variables together.

$$ Y = b_0 + b_1X + b_2 G + b_3 (X * G) $$
Test whether b_3 significantly differs from 0 (parallel lines)

When using crossproducts, VIFs are often bad
*** Crossproducts
#+BEGIN_SRC jupyter-python
fecal['cross'] = fecal.group*fecal.year
fec_est_cross = smf.ols('lnfec ~ year + season + cross', data=fecal).fit()
fec_est_cross.summary2().tables[1]
#+END_SRC

|                |    Coef. |  Std.Err. |        t |        P> t |      [0.025 |   0.975] |
|----------------+----------+-----------+----------+-------------+-------------+----------|
| Intercept      | -1289.36 |   103.425 | -12.4665 | 6.90906e-11 |     -1505.1 | -1073.61 |
| season[T.warm] |  298.728 |   146.247 |  2.04263 |   0.0544939 |     -6.3372 |  603.794 |
| year           | 0.504836 | 0.0523904 |  9.63604 | 5.87965e-09 |    0.395552 | 0.614121 |
| cross          | 0.150791 | 0.0740912 |   2.0352 |   0.0552997 | -0.00376088 | 0.305342 |


$$ lnfec = -1289 - 299 season + 0.505 year + 0.151 cross $$

G*X term can induce *correctable* multicollinearity with G or X.
Centering the X data beforehand will remove this numerically-induced multicollinearity.

Centering X is done by subtracting the mean of X from the X data,
X.centered = X - mean(X)
and then recomputing the crossproduct
GX_centered = G*X_centered

#+BEGIN_SRC jupyter-python
fecal['year_centered'] = fecal['year'] - np.mean(fecal['year'])
fecal['cross_centered'] = fecal['year_centered']*fecal['group']
fec_est_cross1 = smf.ols('lnfec ~ year_centered + season + cross_centered', data=fecal).fit()
fec_est_cross1.summary2().tables[1]
X = fecal[["lnfec","year_centered","season","cross_centered"]]
         #'MJTEMP'"PCOMM"]]
X=add_constant(X)
vif = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
list(zip(vif,X.columns))
#there's an error maybe from -0 ?
#+END_SRC


|                |    Coef. |  Std.Err. |       t |         P>t |      [0.025 |   0.975] |
|----------------+----------+-----------+---------+-------------+-------------+----------|
| Intercept      |  4.77037 | 0.0906641 | 52.6159 | 6.36976e-23 |     4.58125 |  4.95949 |
| season[T.warm] |  1.08643 |  0.128218 | 8.47326 |  4.7431e-08 |    0.818969 |  1.35389 |
| year_centered  | 0.504836 | 0.0523904 | 9.63604 | 5.87965e-09 |    0.395552 | 0.614121 |
| cross_centered | 0.150791 | 0.0740912 |  2.0352 |   0.0552997 | -0.00376088 | 0.305342 |

lnfec = 4.77 + 1.09season + 0.5year_centered + 0.151cross_centered
fecHAT = exp(4.77 + 1.09season + 0.5year_centered + 0.151cross_centered)


** Summary-- Testing for more than one line
H0 : one line
$$Y = b_0 + b_1 X$$
1) Test for two parallel lines
   $$Y = b_0 + b_1 X + b_2 G$$
2) If significant, test for change in slope
   $$ Y = b_0 + b_1X + b_2G + b_3GX $$

   Quick way: use subsets model selection
** More than 2 categories?

3 conditions:

| Category | R | F |
|----------+---+---|
| Rising   | 1 | 0 |
| Falling  | 0 | 1 |
| BaseFlow | 0 | 0 |

conc = b0 + b1(Flow) + b2(R) + b3(F)

* Section 11: Trend Analysis
1) Tests where time is an explanatory variable
2) Parametric versus nonparametric trend tests
3) How to add variables in addition to time
4) How to deal with seasonal variation

   H0 for all trend tests: no signal, only random noise over time
HA: either a linear or non-linear(monotonic) trend over time (kendall's tau, nonparametric trend test)

linear model, but not normality, required for Theil-Sen line.
** Continuous or Step Trend
- If time is measured continuously, use a regression or theil-sen
- If time is a grouped variable: step trend
  e.g. 1990s vs now, or pre- vs post-
  2-sample permutation test, or Wilcoxon tests
|               | Simple (only X is time T) | Multiple Xs                      |
|---------------+---------------------------+----------------------------------|
| Nonparametric | Theil-Sen                 | Theil-Sen on residuals on lowess |
| Parametric    | Regression                | Multiple Regression              |
** PYTHON
#+BEGIN_SRC jupyter-python
cuyahoga/cuyahog

#+END_SRC

#+BEGIN_SRC jupyter-python :var cuyahoga=cuyahoga
cuyahoga=pd.DataFrame(cuyahoga,
                      columns=["month","year","tds","Q","month_txt"])
import pymannkendall as mk
a = mk.original_test(cuyahoga.tds)
b = mk.multivariate_test(cuyahoga[['year','tds']].to_numpy())
print(a, b)
#+END_SRC

#+RESULTS:
: Mann_Kendall_Test(trend='no trend', h=False, p=0.48737355019743966, z=-0.6944922646184538, Tau=-0.053164556962025315, s=-168.0, var_s=57822.666666666664, slope=-0.4301572617946346, intercept=466.99121184088807) Multivariate_Mann_Kendall_Test(trend='increasing', h=True, p=0.0, z=8.791129092667026, Tau=0.47341772151898737, s=2992.0, var_s=115756.0, slope=0.1022857142857122, intercept=1337.9541428571429)
#+BEGIN_SRC jupyter-python
diagnostic_plots(cuyahoga.year,cuyahoga.tds)

#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/3fb3f85ce4db18f4ba6e5d6f96b2d8970e01dd6c.png]]
[[file:./.ob-jupyter/d06b3d3a14c33e32a608f05e8df973ff844a0a5a.png]]
[[file:./.ob-jupyter/c18482a176a44af4525d9dd29df55c49443bf24b.png]]
[[file:./.ob-jupyter/aa656ea0d18a2178cc05176d85c8623692407cb4.png]]
:END:
#+BEGIN_SRC jupyter-python
from statsmodels.nonparametric.smoothers_lowess import lowess
low = lowess(cuyahoga.tds, cuyahoga.year, frac=0.6)
ax = sns.scatterplot(x='year',y='tds',data=cuyahoga)
x,y=[],[]
for row in low:
    x.append(row[0])
    y.append(row[1])
sns.lineplot(x,y)
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7f8bb737ef10>
[[file:./.ob-jupyter/6bcd3f594d61b6a4cb533400b0c8b004e7407b2e.png]]
:END:

join lowess fitted values to dataframe on year, calculate residuals, run mk test on residuals

#+BEGIN_SRC jupyter-python
lo = pd.DataFrame(low, columns=['year','tds_fit'])
cuya = pd.merge(cuyahoga, lo, on='year', how='left')
cuya['tds_resid'] = cuya.tds - cuya.tds_fit
mk.original_test(cuya.tds_resid)
#+END_SRC

#+RESULTS:
: Mann_Kendall_Test(trend='no trend', h=False, p=0.9569267099000329, z=-0.054010611411038904, Tau=-0.004430379746835443, s=-14.0, var_s=57933.333333333336, slope=-0.03241129072605224, intercept=-3.1516630699791817)

#+name:nptrend
#+BEGIN_SRC jupyter-python
'''Cuzick's non-parametric test of trend

Author : Josef Perktold

'''
import numpy as np

from scipy import stats
from scipy.stats.stats import rankdata, tiecorrect #sum_of_squares

class Munch(object):
    pass


# written based on scipy.stats.kruskal
def trend_cuzick(data_list, levels=None):
    """
    Compute the Cuzick trend test for independent samples

    Parameters
    ----------
    sample1, sample2, ... : array_like
       Two or more arrays with the sample measurements can be given as
       arguments.
    levels : None, or array_like
       These are the numberic values associated with the groups. If it is None,
       then integers 1, 2, 3, ... are used.

    Returns
    -------
    res : a Bunch instance with
        p_value, test statistic and additional attributes

    Notes
    -----
    The p-value is based on the approximation by the normal distribution.
    To be a good approximation the number of samples in each group should
    not be too small.

    """
    args = list(map(np.asarray, data_list)) # convert to a numpy array
    na = len(args)               # Kruskal-Wallis on 'na' groups, each in it's own array
    if na < 2:
        raise ValueError("Need at least two groups in stats.kruskal()")
    n = np.asarray(list(map(len, args)))

    if levels is None:
        levels = np.arange(1, na + 1)
    else:
        levels = np.asarray(levels)

    alldata = np.concatenate(args)

    ranked = rankdata(alldata)  # Rank the data
    T = tiecorrect(ranked)      # Correct for ties
    if T == 0:
        raise ValueError('All numbers are identical in kruskal')

    L = (levels * n).sum()

    # group indices interval
    idx = np.insert(np.cumsum(n), 0, 0)
    ranksum = [ranked[idx[i]:idx[i+1]].sum() for i in range(na)]
    # could use reduceat instead

    value = (levels * ranksum).sum()  # T in Stata manual

    nobs = n.sum()
    #tie_correction is np.sqrt(T)
    var = (nobs + 1.) / 12. * (nobs * (levels**2 * n).sum() - L**2) * T
    se = np.sqrt(var)

    value_standardized = (value - 0.5 * (nobs + 1) * L) / se #/ np.sqrt(T)

    #res = Munch()
    res = []
    res.value = value
    res.p_value = stats.norm.sf(value_standardized)*2
    res.statistic = value_standardized
    #res.ranksum = ranksum
    res.tiecorrection = T
    res.nobs_groups = n
    res.n_groups = na
    res.nobs = nobs
    return res


#+END_SRC

#+RESULTS: nptrend

#+BEGIN_SRC jupyter-python
cuya

#+END_SRC

#+RESULTS:
|    | month |    year | tds |    Q | month_txt | tds_fit | tds_resid |
|----+-------+---------+-----+------+-----------+---------+-----------|
|  0 |     1 | 1974.04 | 490 |  458 | Jan       | 413.341 |    76.659 |
|  1 |     2 | 1974.12 | 540 |  469 | Feb       | 414.333 |   125.667 |
|  2 |     4 | 1974.29 | 220 | 4630 | Apr       | 416.442 |  -196.442 |
|  3 |     7 | 1974.54 | 390 |  321 | Jul       | 419.516 |   -29.516 |
|  4 |    10 | 1974.79 | 450 |  541 | Oct       | 422.517 |    27.483 |
|  5 |     1 | 1975.04 | 230 | 1640 | Jan       |  425.41 |   -195.41 |
|  6 |     4 | 1975.29 | 360 | 1060 | Apr       | 428.158 |  -68.1583 |
|  7 |     7 | 1975.54 | 460 |  264 | Jul       | 430.757 |   29.2434 |
|  8 |    10 | 1975.79 | 430 |  665 | Oct       | 433.232 |  -3.23191 |
|  9 |     1 | 1976.04 | 430 |  680 | Jan       | 435.632 |  -5.63191 |
| 10 |     4 | 1976.29 | 620 |  650 | Apr       | 438.047 |   181.953 |
| 11 |     8 | 1976.62 | 460 |  490 | Aug       |  441.59 |   18.4098 |
| 12 |    10 | 1976.79 | 450 |  380 | Oct       | 443.861 |   6.13944 |
| 13 |     1 | 1977.04 | 580 |  325 | Jan       | 448.545 |   131.455 |
| 14 |     4 | 1977.29 | 350 | 1020 | Apr       | 454.308 |  -104.308 |
| 15 |     7 | 1977.54 | 440 |  460 | Jul       | 458.729 |  -18.7288 |
| 16 |    10 | 1977.79 | 530 |  583 | Oct       | 461.524 |   68.4764 |
| 17 |    11 | 1977.87 | 380 |  777 | Nov       | 462.025 |  -82.0246 |
| 18 |    12 | 1977.96 | 440 | 1230 | Dec       | 462.235 |   -22.235 |
| 19 |     1 | 1978.04 | 430 |  565 | Jan       | 462.325 |  -32.3249 |
| 20 |     2 | 1978.12 | 680 |  533 | Feb       |  462.16 |    217.84 |
| 21 |     3 | 1978.21 | 250 | 4930 | Mar       | 462.078 |  -212.078 |
| 22 |     4 | 1978.29 | 250 | 3810 | Apr       | 461.754 |  -211.754 |
| 23 |     5 | 1978.37 | 450 |  469 | May       |  461.69 |  -11.6902 |
| 24 |     6 | 1978.46 | 500 |  473 | Jun       | 461.378 |   38.6224 |
| 25 |     7 | 1978.54 | 510 |  593 | Jul       | 461.437 |   48.5625 |
| 26 |     8 | 1978.62 | 490 |  500 | Aug       | 461.402 |   28.5984 |
| 27 |     9 | 1978.71 | 700 |  266 | Sep       | 461.746 |   238.254 |
| 28 |    10 | 1978.79 | 420 |  495 | Oct       | 462.147 |  -42.1471 |
| 29 |    11 | 1978.87 | 710 |  245 | Nov       | 462.698 |   247.302 |
| 30 |    12 | 1978.96 | 430 |  736 | Dec       |  463.44 |  -33.4398 |
| 31 |     1 | 1979.04 | 410 |  508 | Jan       | 464.143 |  -54.1426 |
| 32 |     2 | 1979.12 | 700 |  578 | Feb       | 465.015 |   234.985 |
| 33 |     3 | 1979.21 | 260 | 4590 | Mar       | 465.934 |  -205.934 |
| 34 |     4 | 1979.29 | 260 | 4670 | Apr       | 466.897 |  -206.897 |
| 35 |     5 | 1979.37 | 500 |  503 | May       | 467.632 |   32.3677 |
| 36 |     6 | 1979.46 | 450 |  469 | Jun       | 468.501 |  -18.5006 |
| 37 |     7 | 1979.54 | 500 |  314 | Jul       | 469.143 |   30.8569 |
| 38 |     8 | 1979.62 | 620 |  432 | Aug       | 469.859 |   150.141 |
| 39 |     9 | 1979.71 | 670 |  279 | Sep       | 470.517 |   199.483 |
| 40 |    10 | 1979.79 | 410 |  542 | Oct       | 471.054 |  -61.0542 |
| 41 |    11 | 1979.87 | 470 |  499 | Nov       | 471.334 |  -1.33392 |
| 42 |    12 | 1979.96 | 370 |  741 | Dec       | 471.476 |  -101.476 |
| 43 |     1 | 1980.04 | 410 |  569 | Jan       | 471.146 |  -61.1458 |
| 44 |     2 | 1980.12 | 540 |  360 | Feb       | 470.875 |   69.1254 |
| 45 |     3 | 1980.21 | 550 |  513 | Mar       | 470.239 |   79.7613 |
| 46 |     4 | 1980.29 | 220 | 3910 | Apr       | 469.496 |  -249.496 |
| 47 |     5 | 1980.37 | 460 |  364 | May       | 468.096 |   -8.0961 |
| 48 |     6 | 1980.46 | 390 |  472 | Jun       | 466.815 |  -76.8151 |
| 49 |     7 | 1980.54 | 550 |  245 | Jul       | 465.066 |   84.9337 |
| 50 |     8 | 1980.62 | 320 | 1500 | Aug       | 463.745 |  -143.745 |
| 51 |     9 | 1980.71 | 570 |  224 | Sep       | 461.813 |   108.187 |
| 52 |    10 | 1980.79 | 480 |  342 | Oct       | 460.416 |   19.5843 |
| 53 |    12 | 1980.96 | 520 |  732 | Dec       | 457.349 |    62.651 |
| 54 |     1 | 1981.04 | 620 |  240 | Jan       | 455.848 |   164.152 |
| 55 |     2 | 1981.12 | 520 |  472 | Feb       | 454.529 |   65.4707 |
| 56 |     3 | 1981.21 | 430 |  679 | Mar       | 452.827 |  -22.8267 |
| 57 |     4 | 1981.29 | 400 | 1080 | Apr       | 451.459 |   -51.459 |
| 58 |     5 | 1981.37 | 430 |  920 | May       | 450.056 |  -20.0564 |
| 59 |     6 | 1981.46 | 490 |  488 | Jun       | 448.611 |   41.3893 |
| 60 |     7 | 1981.54 | 560 |  444 | Jul       | 447.369 |   112.631 |
| 61 |     8 | 1981.62 | 370 |  595 | Aug       |  446.07 |  -76.0704 |
| 62 |     9 | 1981.71 | 460 |  295 | Sep       | 444.523 |   15.4773 |
| 63 |    10 | 1981.79 | 390 |  542 | Oct       | 442.963 |  -52.9626 |
| 64 |    12 | 1981.96 | 330 | 1500 | Dec       | 439.301 |  -109.301 |
| 65 |     3 | 1982.21 | 350 | 1080 | Mar       | 433.052 |  -83.0525 |
| 66 |     5 | 1982.37 | 480 |  334 | May       |  428.74 |   51.2595 |
| 67 |     6 | 1982.46 | 390 |  423 | Jun       | 426.346 |  -36.3455 |
| 68 |     8 | 1982.62 | 500 |  216 | Aug       |  422.46 |   77.5402 |
| 69 |    11 | 1982.87 | 410 |  366 | Nov       | 417.028 |   -7.0283 |
| 70 |     2 | 1983.12 | 470 |  750 | Feb       | 411.938 |   58.0624 |
| 71 |     5 | 1983.37 | 280 | 1260 | May       |  406.96 |   -126.96 |
| 72 |     8 | 1983.62 | 510 |  223 | Aug       | 402.009 |   107.991 |
| 73 |    11 | 1983.87 | 470 |  462 | Nov       | 397.063 |   72.9368 |
| 74 |     2 | 1984.12 | 310 | 7640 | Feb       |   392.1 |  -82.0999 |
| 75 |     5 | 1984.37 | 230 | 2340 | May       |  387.09 |   -157.09 |
| 76 |     7 | 1984.54 | 470 |  239 | Jul       | 383.658 |   86.3423 |
| 77 |    11 | 1984.87 | 330 | 1400 | Nov       | 376.971 |  -46.9705 |
| 78 |     3 | 1985.21 | 320 | 3070 | Mar       | 370.084 |  -50.0844 |
| 79 |     5 | 1985.37 | 500 |  244 | May       | 366.864 |   133.136 |


#+BEGIN_SRC jupyter-python
res=trend_cuzick(cuya[['tds','Q']].to_numpy())

#[(x, res[x]) for x in res]
res.keys()
#+END_SRC

#+RESULTS:
:RESULTS:
# [goto error]
:
: AttributeErrorTraceback (most recent call last)
: <ipython-input-36-8b6c07a6b9be> in <module>
:       2
:       3 #[(x, res[x]) for x in res]
: ----> 4 res.keys()
:
: AttributeError: 'munch' object has no attribute 'keys'
:END:
** Step Trend Tests
Before/after, early vs now, tests for differences between two tim groups.


 |               | simple (only X is time T)          | Multiple Xs               |
 |---------------+------------------------------------+---------------------------|
 | Nonparametric | 2-sample wilcoxon (mann whitney u) | 2-w on lowess resids      |
 | Test on means | permutation test                   | ANCOVA (multiple regress) |
** PYTHON Step trend exercise
neb.rda
*** Data
#+TBLNAME:neb
| year |     depth |     nitrate | period |
|------+-----------+-------------+--------|
| 1974 |  24.99997 |  0.29999989 |      0 |
| 1974 |  26.99996 |  3.79999781 |      0 |
| 1974 |  27.99996 |  6.69999504 |      0 |
| 1974 |  29.99996 |  0.29999989 |      0 |
| 1974 |  29.99996 | 23.99996948 |      0 |
| 1974 |  37.99995 |  0.29999989 |      0 |
| 1974 |  39.99995 | 26.99996185 |      0 |
| 1974 |  39.99995 |  7.09999466 |      0 |
| 1974 |  39.99995 |  7.89999390 |      0 |
| 1974 |  44.99993 | 23.99996948 |      0 |
| 1974 |  49.99993 |  1.89999938 |      0 |
| 1974 |  49.99993 |  4.59999657 |      0 |
| 1974 |  49.99993 |  4.89999676 |      0 |
| 1974 |  49.99993 |  6.49999523 |      0 |
| 1974 |  49.99993 |  7.39999580 |      0 |
| 1974 |  49.99993 |  7.69999504 |      0 |
| 1974 |  49.99993 |  9.49999237 |      0 |
| 1974 |  49.99993 | 14.99998474 |      0 |
| 1974 |  53.99992 |  8.59999275 |      0 |
| 1974 |  54.99991 | 16.99997711 |      0 |
| 1974 |  55.99991 |  6.49999523 |      0 |
| 1974 |  55.99991 |  9.99999046 |      0 |
| 1974 |  57.99991 |  7.99999332 |      0 |
| 1974 |  59.99991 |  0.09999996 |      0 |
| 1974 |  59.99991 |  0.09999996 |      0 |
| 1974 |  59.99991 |  0.29999989 |      0 |
| 1974 |  59.99991 |  0.29999989 |      0 |
| 1974 |  59.99991 | 35.99994659 |      0 |
| 1974 |  59.99991 |  1.09999967 |      0 |
| 1974 |  59.99991 |  2.09999895 |      0 |
| 1974 |  59.99991 |  3.99999762 |      0 |
| 1974 |  59.99991 |  9.69999123 |      0 |
| 1974 |  59.99991 | 10.99998665 |      0 |
| 1974 |  59.99991 | 12.99998856 |      0 |
| 1974 |  59.99991 | 15.99998283 |      0 |
| 1974 |  61.99992 |  5.39999676 |      0 |
| 1974 |  62.99989 | 11.99998856 |      0 |
| 1974 |  64.99994 |  0.09999996 |      0 |
| 1974 |  64.99994 |  0.29999989 |      0 |
| 1974 |  64.99994 | 22.99997330 |      0 |
| 1974 |  64.99994 |  1.79999947 |      0 |
| 1974 |  64.99994 |  9.99999046 |      0 |
| 1974 |  67.99991 | 19.99997711 |      0 |
| 1974 |  71.99991 | 15.99998283 |      0 |
| 1974 |  71.99991 |  4.09999752 |      0 |
| 1974 |  74.99992 | 15.99998283 |      0 |
| 1974 |  74.99992 | 18.99997711 |      0 |
| 1974 |  76.99994 |  0.09999996 |      0 |
| 1974 |  76.99994 | 13.99998474 |      0 |
| 1974 |  76.99994 | 15.99998283 |      0 |
| 1974 |  76.99994 |  2.09999895 |      0 |
| 1974 |  79.99991 |  1.09999967 |      0 |
| 1974 |  79.99991 |  3.39999819 |      0 |
| 1974 |  79.99991 |  4.69999695 |      0 |
| 1974 |  79.99991 |  7.29999447 |      0 |
| 1974 |  83.99989 |  6.09999657 |      0 |
| 1974 |  84.99988 |  0.09999996 |      0 |
| 1974 |  84.99988 |  4.29999733 |      0 |
| 1974 |  85.99992 |  0.79999983 |      0 |
| 1974 |  89.99989 |  0.02000000 |      0 |
| 1974 |  89.99989 |  0.89999998 |      0 |
| 1974 |  91.99989 |  0.09999996 |      0 |
| 1974 |  91.99989 | 15.99998283 |      0 |
| 1974 |  94.99989 |  0.09999996 |      0 |
| 1974 |  94.99989 |  0.49999994 |      0 |
| 1974 |  99.99989 |  0.09999996 |      0 |
| 1974 |  99.99989 | 14.99998474 |      0 |
| 1974 |  99.99989 |  0.49999994 |      0 |
| 1974 |  99.99989 |  3.99999762 |      0 |
| 1974 |  99.99989 |  5.39999676 |      0 |
| 1974 | 103.99988 | 11.99998856 |      0 |
| 1974 | 105.99980 |  0.09999996 |      0 |
| 1974 | 109.99985 |  1.69999957 |      0 |
| 1974 | 109.99985 |  1.79999947 |      0 |
| 1974 | 112.99986 |  2.99999905 |      0 |
| 1974 | 113.99980 |  0.09999996 |      0 |
| 1974 | 129.99985 | 18.99997711 |      0 |
| 1974 | 129.99985 |  6.79999447 |      0 |
| 1974 | 174.99976 |  4.29999733 |      0 |
| 1984 |  24.99997 |  0.58999991 |      1 |
| 1984 |  27.99996 |  9.89999008 |      1 |
| 1984 |  29.99996 |  0.15999997 |      1 |
| 1984 |  29.99996 |  0.25999993 |      1 |
| 1984 |  37.99995 |  1.00000000 |      1 |
| 1984 |  39.99995 | 17.99997711 |      1 |
| 1984 |  49.99993 | 20.99997330 |      1 |
| 1984 |  49.99993 | 20.99997330 |      1 |
| 1984 |  49.99993 | 21.99996948 |      1 |
| 1984 |  49.99993 | 22.99997330 |      1 |
| 1984 |  49.99993 | 25.99996948 |      1 |
| 1984 |  49.99993 | 29.99996185 |      1 |
| 1984 |  49.99993 |  0.65999997 |      1 |
| 1984 |  49.99993 |  2.09999895 |      1 |
| 1984 |  49.99993 |  8.99999428 |      1 |
| 1984 |  49.99993 | 13.99998474 |      1 |
| 1984 |  49.99993 | 15.99998283 |      1 |
| 1984 |  49.99993 | 16.99997711 |      1 |
| 1984 |  49.99993 | 17.99997711 |      1 |
| 1984 |  53.99992 | 23.99996948 |      1 |
| 1984 |  53.99992 | 15.99998283 |      1 |
| 1984 |  56.99991 |  4.79999638 |      1 |
| 1984 |  57.99991 | 17.99997711 |      1 |
| 1984 |  59.99991 |  0.19999993 |      1 |
| 1984 |  59.99991 |  0.21999997 |      1 |
| 1984 |  59.99991 | 16.99997711 |      1 |
| 1984 |  59.99991 | 16.99997711 |      1 |
| 1984 |  59.99991 | 18.99997711 |      1 |
| 1984 |  59.99991 | 19.99997711 |      1 |
| 1984 |  59.99991 | 21.99996948 |      1 |
| 1984 |  59.99991 | 22.99997330 |      1 |
| 1984 |  59.99991 | 29.99996185 |      1 |
| 1984 |  59.99991 | 32.99994659 |      1 |
| 1984 |  59.99991 | 48.99991608 |      1 |
| 1984 |  59.99991 |  0.40999997 |      1 |
| 1984 |  59.99991 |  1.99999928 |      1 |
| 1984 |  59.99991 |  2.19999886 |      1 |
| 1984 |  59.99991 |  2.59999895 |      1 |
| 1984 |  59.99991 | 11.99998856 |      1 |
| 1984 |  59.99991 | 13.99998474 |      1 |
| 1984 |  61.99992 |  4.99999714 |      1 |
| 1984 |  62.99989 | 20.99997330 |      1 |
| 1984 |  62.99989 |  0.63000000 |      1 |
| 1984 |  63.99994 | 20.99997330 |      1 |
| 1984 |  64.99994 |  0.09999996 |      1 |
| 1984 |  64.99994 |  0.09999996 |      1 |
| 1984 |  64.99994 | 33.99994659 |      1 |
| 1984 |  64.99994 | 10.99998665 |      1 |
| 1984 |  67.99991 | 27.99996185 |      1 |
| 1984 |  71.99991 |  0.32999998 |      1 |
| 1984 |  71.99991 | 22.99997330 |      1 |
| 1984 |  71.99991 |  5.49999618 |      1 |
| 1984 |  74.99992 | 20.99997330 |      1 |
| 1984 |  74.99992 | 36.99994659 |      1 |
| 1984 |  76.99994 | 12.99998856 |      1 |
| 1984 |  76.99994 | 16.99997711 |      1 |
| 1984 |  76.99994 | 23.99996948 |      1 |
| 1984 |  76.99994 | 25.99996948 |      1 |
| 1984 |  76.99994 | 28.99996185 |      1 |
| 1984 |  79.99991 | 12.99998856 |      1 |
| 1984 |  79.99991 |  6.19999600 |      1 |
| 1984 |  81.99991 |  0.23999995 |      1 |
| 1984 |  84.99988 |  0.04999999 |      1 |
| 1984 |  84.99988 |  4.89999676 |      1 |
| 1984 |  91.99989 | 22.99997330 |      1 |
| 1984 |  99.99989 |  0.05999998 |      1 |
| 1984 |  99.99989 |  0.13999999 |      1 |
| 1984 |  99.99989 | 11.99998856 |      1 |
| 1984 |  99.99989 | 24.99997330 |      1 |
| 1984 |  99.99989 |  0.69999993 |      1 |
| 1984 |  99.99989 |  2.39999914 |      1 |
| 1984 | 103.99988 | 13.99998474 |      1 |
| 1984 | 104.99980 |  9.99999046 |      1 |
| 1984 | 105.99980 |  0.13999999 |      1 |
| 1984 | 109.99985 |  0.15999997 |      1 |
| 1984 | 112.99986 |  3.19999838 |      1 |
| 1984 | 129.99985 |  8.89999390 |      1 |
| 1984 | 129.99985 | 24.99997330 |      1 |
| 1984 | 134.99982 |  0.72999978 |      1 |
| 1984 | 159.99979 |  0.38999999 |      1 |

*** Analysis

#+BEGIN_SRC jupyter-python :var neb=neb
no3 = pd.DataFrame(data=neb, columns=['year','depth','nitrate','period'])
sns.boxplot(y='nitrate', x='year', data=no3, palette='Set3')
plt.style.use('seaborn-white')
#+END_SRC

#+RESULTS:
[[file:./.ob-jupyter/cf0d22badeec475a2752c7aad372f6b9d8527ad7.png]]

We see 1974 has top quartile larger, 1984 has bottom quartile larger, therefore no transformation will adequately transform them to a normal distribution. Parametric tests won't work.

Use a permutation or nonparametric test.

Nonparametric:
*** Two-sample wilcoxon, AKA Mann Whitney U
#+BEGIN_SRC jupyter-python
x = no3.loc[no3.year==1974, 'nitrate'].to_numpy()
y = no3.loc[no3['year']==1984, 'nitrate'].to_numpy()
mannwhitneyu(x,y, alternative='two-sided')
#+END_SRC

#+RESULTS:
: MannwhitneyuResult(statistic=2288.0, pvalue=0.0026704356086082815)
There is a difference in the medians of the two groups.

If we want to find out by how much they differ,

Want estimate of difference and confidence interval. Best estimate of median difference, and 95% CI around that. Is 0 included?

#+BEGIN_SRC jupyter-python
''' I came across a paper (Calculating confidence intervals for some non-parametric analyses by MICHAEL J CAMPBELL, MARTIN J GARDNER) that gave CI formula.'''
# https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2545906/pdf/bmj00286-0037.pdf
# https://stackoverflow.com/questions/51845606/python-mann-whitney-confidence-interval

from scipy.stats import norm
ct1 = len(x)  #items in dataset 1
ct2 = len(y)  #items in dataset 2
alpha = 0.05       #95% confidence interval
N = norm.ppf(1 - alpha/2) # percent point function - inverse of cdf

# The confidence interval for the difference between the two population
# medians is derived through these nxm differences.
diffs = sorted([i-j for i in x for j in y])

# For an approximate 100(1-a)% confidence interval first calculate K:
k = int(round(ct1*ct2/2 - (N * (ct1*ct2*(ct1+ct2+1)/12)**0.5)))

# The Kth smallest to the Kth largest of the n x m differences
# ct1 and ct2 should be > ~20
CI = (diffs[k], diffs[len(diffs)-k])
CI
#+END_SRC

#+RESULTS:
| -8.999984739999999 | -0.62999982 |

The 95% CI does not include 0.

*** Permutation Test

#+BEGIN_SRC jupyter-python
reps = 10000
x_size = len(x)
y_size = len(y)
diffmeans = []
#mean, sigma = np.mean(value), np.std(value, ddof=1)
for i in np.arange(1,reps+1):
    xsample = np.random.choice(x, x_size, replace=True)
    ysample = np.random.choice(y, y_size, replace=True)
    xstat = np.mean(xsample)
    ystat = np.mean(ysample)
    diffmeans.append(xstat-ystat)
ordered = np.sort(diffmeans)
lower = np.percentile(ordered, 2.5, interpolation='midpoint')
upper = np.percentile(ordered, 97.5, interpolation='midpoint')
plt.style.use('seaborn-white')
ax = sns.distplot(diffmeans, label=f"Bootstrap means,\n n = {reps:,}")
                  #kde_kws={"clip":(-10,10), "bw":1})
ax.axvline(x=lower,
          color='k',
          label=f"95 CI \n ({lower:0.03}, {upper:0.03})")
ax.axvline(x=upper,
          color='k')
ax.axvline(x=mean_diff_obs,
           ls='--',
           color='r',
           label=f"Obs. Mean {mean_diff_obs:0.3}")
ax.set_title("Bootstrap of Difference of Means")
ax.legend()
#+END_SRC

#+RESULTS:
:RESULTS:
: upper= -2.6894825820799038
: <matplotlib.legend.Legend at 0x7fbb3e1aeac0>
[[file:./.ob-jupyter/15aac73ec66fb482d67f7e3791ffb9df93a02027.png]]
:END:
The mean does not include 0.

*** Is difference due to depth not time?
Take lowess curve, get residuals from it
Then we can repeat the mann whitney test with effects of depth subtracted out.

#+BEGIN_SRC jupyter-python
from statsmodels.nonparametric.smoothers_lowess import lowess
# no3 = pd.DataFrame(data=neb, columns=['year','depth','nitrate','period'])
low = lowess(no3.nitrate, no3.depth, frac=0.6)
ax = sns.scatterplot(x='depth',y='nitrate',data=no3)
x,y=[],[]
for row in low:
    x.append(row[0])
    y.append(row[1])
sns.lineplot(x,y, color='r')

#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fbb2b27d9a0>
[[file:./.ob-jupyter/92103db846a883705c3ddecfe29edd73f35bde8b.png]]
:END:

Rank sum test of Lowess residuals vs Time step.

join lowess fitted values to dataframe on year, calculate residuals, run mk test on residuals

#+BEGIN_SRC jupyter-python
#no3 = pd.DataFrame(data=neb, columns=['year','depth','nitrate','period'])
lo = pd.DataFrame(low, columns=['depth','no3_fit'])
no3_a = pd.merge(no3, lo, on='depth', left_index=True, right_index=True)
no3_a['no3_resid'] = no3_a.nitrate - no3_a.no3_fit
x = no3_a.loc[no3_a.year==1974, 'no3_resid'].to_numpy()
y = no3_a.loc[no3_a.year==1984, 'no3_resid'].to_numpy()
mannwhitneyu(x,y, alternative='two-sided')
#+END_SRC

#+RESULTS:
: MannwhitneyuResult(statistic=1735.0, pvalue=9.230749435522947e-07)

It is more significant now by factoring out depth to water and running the test on residuals. The pvalue is much smaller.

*** Parametric approach, ANCOVA, is just multiple regressions.
diagnostic plots. -- keep y units
plot components and residual plots -- use log depth (will have higher r^2)


** Dealing with Seasonal Variation
Continuous Trend Tests with Seasonal Variation

|                 | Simple (only X is time T) | Multiple Xs                          |
|-----------------+---------------------------+--------------------------------------|
| Nonpar Test     | Seasonal kendall test     | s-k test on residuals                |
| parametric test | regression with           | multiple regression with sin and cos |
|                 | sin(2*pi*time)            |                                      |
|                 | cos(2*pi*time)            |                                      |
